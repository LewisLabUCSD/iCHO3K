{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ecee4b",
   "metadata": {},
   "source": [
    "# Final CHO Model\n",
    "This notebook is to asses the validity of our reconstruction and how complete it is.\n",
    "\n",
    "[1. Generation of the dataset and model reconstruction](#generation) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.1 Retrieve information from the Google Sheet datasets reactions and metabolites**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.2 Build a model and feed it the information from the df generated** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.3 Save and validate the model** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.4 Check for unbalanced reactions** <br>\n",
    "\n",
    "[2. Identification of Blocked Reactions and Dead-End Metabolites](#blocked&deadends) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.1 Identification of Blocked Reactions**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.2 Identification of Dead-Ends Metabolites** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.3 Addition of Extracellular Exchange Reanctions** <br>\n",
    "\n",
    "[3. Generation of the Mass Flow Graph](#MFG) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.1 Generation of the \"D-Matrix\"**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.2 Plotting the \"D-Matrix\", Normalized Flow Graph (NFG)** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.3 Generation of the \"FluxOpenValue\" matrix** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.4 Generation of the \"Mass Flow Graph (MFG) Matrix\"** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.5 Plotting the \"MFG Matrix\"** <br>\n",
    "\n",
    "[4. Identification of duplicates through Chemical Formulas](#formulas) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1047d",
   "metadata": {},
   "source": [
    "## 1. Generation of the dataset and model reconstruction <a id='generation'></a>\n",
    "Here we generate the CHO model from the dataset stored in the Google Sheet file. We first use the google_sheet module to extract all the necessary information from the original dataset. Then we use those dataset and the COBRApy library to: (1) Create a new model and add reactions from the **Rxns Sheet**, (2) Add information on each reaction obtained from the **Rxns Sheet** and **Attributes Sheet**, (3) Add boundary reactions from the **BoundaryRxns Sheet**, and (4) Add information for each metabolite from the **Metabolites Sheet**. Finally we save the model as a SBML file and validate it using the cobrapy built-in function \"validate_sbml_model( )\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import cobra\n",
    "from cobra import Model, Reaction, Metabolite\n",
    "from cobra.io import validate_sbml_model, save_json_model, write_sbml_model\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from google_sheet import GoogleSheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ed354",
   "metadata": {},
   "source": [
    "### 1.1 Retrieve information from the Google Sheet datasets reactions and metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719281dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Generate datasets from Google Sheet ----- #####\n",
    "\n",
    "#Credential file\n",
    "KEY_FILE_PATH = 'credentials.json'\n",
    "\n",
    "#CHO Network Reconstruction + Recon3D_v3 Google Sheet ID\n",
    "SPREADSHEET_ID = '1MlBXeHIKw8k8fZyXm-sN__AHTRSunJxar_-bqvukZws'\n",
    "\n",
    "# Initialize the GoogleSheet object\n",
    "sheet = GoogleSheet(SPREADSHEET_ID, KEY_FILE_PATH)\n",
    "\n",
    "# Read data from the Google Sheet\n",
    "sheet_met = 'Metabolites'\n",
    "sheet_rxns = 'Rxns'\n",
    "sheet_attributes = 'Attributes'\n",
    "sheet_boundary = 'BoundaryRxns'\n",
    "\n",
    "metabolites = sheet.read_google_sheet(sheet_met)\n",
    "rxns = sheet.read_google_sheet(sheet_rxns)\n",
    "rxns_attributes = sheet.read_google_sheet(sheet_attributes)\n",
    "boundary_rxns = sheet.read_google_sheet(sheet_boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcd212",
   "metadata": {},
   "source": [
    "### 1.2 Build a model and feed it the information from the df generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927503f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Create a model and add reactions ----- #####\n",
    "model = Model(\"iCHO\")\n",
    "lr = []\n",
    "for _, row in rxns.iterrows():\n",
    "    r = Reaction(row['Reaction'])\n",
    "    lr.append(r)    \n",
    "model.add_reactions(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c619ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Add information to each one of the reactions ----- #####\n",
    "for i,r in enumerate(tqdm(model.reactions)):    \n",
    "    r.build_reaction_from_string(rxns['Reaction Formula'][i])\n",
    "    r.name = rxns['Reaction Name'][i]\n",
    "    r.subsystem = rxns['Subsystem'][i]\n",
    "    r.gene_reaction_rule = str(rxns['GPR_final'][i])\n",
    "    r.lower_bound = float(rxns_attributes['Lower bound'][i])\n",
    "    r.upper_bound = float(rxns_attributes['Upper bound'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4567f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Add Boundary Reactions ----- #####\n",
    "dr = []\n",
    "for _, row in boundary_rxns.iterrows():\n",
    "    r = Reaction(row['Reaction'])\n",
    "    dr.append(r)    \n",
    "model.add_reactions(dr)\n",
    "\n",
    "boundary_rxns_dict = boundary_rxns.set_index('Reaction').to_dict()\n",
    "boundary_rxns_dict\n",
    "\n",
    "for i,r in enumerate(tqdm(model.reactions)):\n",
    "    if r in dr:\n",
    "        r.build_reaction_from_string(boundary_rxns_dict['Reaction Formula'][r.id])\n",
    "        r.name = boundary_rxns_dict['Reaction Name'][r.id]\n",
    "        r.subsystem = boundary_rxns_dict['Subsystem'][r.id]\n",
    "        r.lower_bound = float(boundary_rxns_dict['Lower bound'][r.id])\n",
    "        r.upper_bound = float(boundary_rxns_dict['Upper bound'][r.id]) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Add information for each metabolite ----- #####\n",
    "metabolites_dict = metabolites.set_index('BiGG ID').to_dict('dict')\n",
    "metabolites_dict['Name']\n",
    "for met in model.metabolites:\n",
    "    met.name = metabolites_dict['Name'][f'{met}']\n",
    "    met.formula = metabolites_dict['Formula'][f'{met}']\n",
    "    met.compartment = metabolites_dict['Compartment'][f'{met}'].split(' - ')[0]\n",
    "    try:\n",
    "        met.charge = int(metabolites_dict['Charge'][f'{met}'])\n",
    "    except (ValueError, TypeError):\n",
    "        print(f'{met} doesnt have charge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82092731",
   "metadata": {},
   "source": [
    "### 1.3 Save and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Build the S matrix ----- #####\n",
    "S = cobra.util.create_stoichiometric_matrix(model, array_type='dense')\n",
    "model.S = S\n",
    "\n",
    "model.objective = 'biomass_cho'\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Save the model ----- #####\n",
    "model_name_xml = 'iCHOv3_CHO_'+datetime.now().strftime(\"%d%m%Y\")+'.xml' \n",
    "write_sbml_model(model, model_name_xml)\n",
    "\n",
    "# Write in json because, the sbml doesnt savve the subsystems\n",
    "model_name_json = 'iCHOv3_CHO_'+datetime.now().strftime(\"%d%m%Y\")+'.json' \n",
    "save_json_model(model, model_name_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Test for errors in the recostruction ----- ######\n",
    "\n",
    "# import tempfile\n",
    "# from pprint import pprint\n",
    "# from cobra.io import write_sbml_model, validate_sbml_model\n",
    "# with tempfile.NamedTemporaryFile(suffix='.xml') as f_sbml:\n",
    "#     write_sbml_model(model, filename=f_sbml.name)\n",
    "#     report = validate_sbml_model(filename=f_sbml.name)\n",
    "# pprint(report)\n",
    "\n",
    "from cobra.io import read_sbml_model, validate_sbml_model\n",
    "(_, errors) = validate_sbml_model(model_name_xml)\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5d619",
   "metadata": {},
   "source": [
    "### 1.4 Check for unbalanced reactions\n",
    "Once the model is checked and saved as a xml and json format we then evaluate the amount of mass and charge unbalanced reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unbalanced reactions\n",
    "subsystems = ['BIOMASS SYNTHESIS', 'PROTEIN ASSEMBLY', 'PROTEIN DEGRADATION'] # filter out the reactions from these subsystems\n",
    "\n",
    "# Lists to store the data for each column\n",
    "reaction_ids = []\n",
    "formulas = []\n",
    "metabolites = []\n",
    "unbalances = []\n",
    "\n",
    "counter = 0\n",
    "for rxn in model.reactions:\n",
    "    if not rxn.id.startswith(('EX_','DM_','SK_')) and rxn.subsystem not in subsystems:\n",
    "        mb = rxn.check_mass_balance()\n",
    "        if mb != {}:# and set(mb.keys()) != {'charge'}:  # Check if dictionary has keys other than 'charge'\n",
    "            counter+=1\n",
    "            prod_ids = [{met.id:met.formula} for met in rxn.products]\n",
    "            react_ids = [{met.id:met.formula} for met in rxn.reactants]\n",
    "            # Append values to lists\n",
    "            print(rxn.id)\n",
    "            reaction_ids.append(rxn.id)\n",
    "            print(rxn.reaction)\n",
    "            formulas.append(rxn.reaction)\n",
    "            print(react_ids + prod_ids)\n",
    "            metabolites.append(react_ids + prod_ids)\n",
    "            print(mb)\n",
    "            unbalances.append(mb)\n",
    "            print('...............................')\n",
    "print(counter)\n",
    "\n",
    "# Create DataFrame from lists\n",
    "mass_unbalanced_reactions = pd.DataFrame({\n",
    "    \"Reaction ID\": reaction_ids,\n",
    "    \"Formula\": formulas,\n",
    "    \"Metabolites\": metabolites,\n",
    "    \"Unbalance\": unbalances\n",
    "})\n",
    "\n",
    "mass_unbalanced_reactions.to_excel(\"temp/mass_unbalanced_reactions.xlsx\", engine='openpyxl', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the \"Balance status\" column based on whether the reaction is present in \"reaction_ids\"\n",
    "rxns_copy = rxns.copy()\n",
    "rxns_copy['Balance status'] = rxns_copy['Reaction'].apply(lambda x: 'UNBALANCED' if x in reaction_ids else 'BALANCED')\n",
    "rxns_equals = rxns_copy.equals(rxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#### ------------------------------------ ####\n",
    "#### ---- Update Rxns Google Sheets ----- ####\n",
    "#### ------------------------------------ ####\n",
    "##############################################\n",
    "if not rxns_equals:\n",
    "    sheet.update_google_sheet(sheet_rxns, rxns_copy)\n",
    "    print(\"Google Sheet updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2da40b",
   "metadata": {},
   "source": [
    "## 2. Identification of Blocked Reactions and Dead-End Metabolites <a id='blocked&deadends'></a>\n",
    "In this second part of the notebook we use two different functions from the utils module to: (1) Run a flux variability analysis and identify blocked reactions, and (2) identify dead-end metabolites. Finally we add Extracellular Exchange reactions for the dead-end metabolites that are in the extracellular compartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cobra.io import read_sbml_model\n",
    "from cobra.flux_analysis import find_blocked_reactions\n",
    "from utils import detect_dead_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Read Model ----- #####\n",
    "if 'model' not in locals():\n",
    "    model = read_sbml_model(\"iCHOv3_CHO_11102023.xml\")\n",
    "    print('Model loaded')\n",
    "else:\n",
    "    print('Model already generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919b458",
   "metadata": {},
   "source": [
    "### 2.1 Identification of Blocked Reactions\n",
    "The runMinMax_Single( ) function from the utils module runs an FVA (flux variability analysis) to find the ranges of each reaction's metabolic flux at the optimum. The results are stored in the \"FVA_Results.txt\" file with all the reactions IDs and its respective fluxes. Reactions with no flux (**Blocked Reactions**) are stored in another file called \"Blocked_Reactions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- FVA ----- #####\n",
    "import utils\n",
    "import time\n",
    "start = time.time()\n",
    "model.solver = 'gurobi'\n",
    "\n",
    "\n",
    "for rxn_exchange in model.exchanges:\n",
    "    rxn_exchange.bounds = (-1000, 1000)\n",
    "minmax = utils.runMinMax_Single(model, end_rxn_index=None)\n",
    "\n",
    "\n",
    "##### Create DataFrame and Save to CSV #####\n",
    "# Create a list to store data\n",
    "data = []\n",
    "\n",
    "# Populate the data list\n",
    "for i, j in enumerate(minmax):\n",
    "    data.append({'Reaction': model.reactions[i].id, 'Min': j[0], 'Max': j[1]})\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "fva_results = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "fva_results.to_csv('temp/FVA_Results.csv', index=False)\n",
    "\n",
    "##### Save Blocked Reactions to CSV #####\n",
    "# Filter the DataFrame to only include blocked reactions\n",
    "blocked_reactions = fva_results[(fva_results['Min'] == 0) & (fva_results['Max'] == 0)]\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "blocked_reactions.to_csv('temp/Blocked_Reactions.csv', index=False)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to run FVA with Thanasis' method: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Blocked Reactions ----- #####\n",
    "model.solver = 'gurobi'\n",
    "blocked_reactions_2 = find_blocked_reactions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra.flux_analysis import flux_variability_analysis\n",
    "start = time.time()\n",
    "model.solver = 'gurobi'\n",
    "fva_results_2 = flux_variability_analysis(model)\n",
    "end = time.time()\n",
    "print(f\"Time taken to run FVA with COBRA py method: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rxn in blocked_reactions:\n",
    "    r = model.reactions.get_by_id(rxn)\n",
    "    if r.subsystem == 'BIOMASS SYNTHESIS':\n",
    "        print(r.id)\n",
    "        print('-----------------')\n",
    "        print('-----------------')\n",
    "        for met in r.metabolites:\n",
    "            m = model.metabolites.get_by_id(met.id)\n",
    "            print(m)\n",
    "            print('.................')\n",
    "            for r2 in m.reactions:\n",
    "                if r2.id in blocked_reactions:\n",
    "                    print(f'No Flux -> {r2.id}: {r2.reaction}')\n",
    "                else:\n",
    "                    print(f'With Flux -> {r2.id}: {r2.reaction}')\n",
    "            print('.................')\n",
    "            print(' ')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ab78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Print the amount  and % of blocked reactions ----- #####\n",
    "print('##### ----- Blocked Reactions ----- #####')\n",
    "print(f'The model has {len(model.reactions)} total reactions')\n",
    "print(f'The model has {len(blocked_reactions)} ({round(len(blocked_reactions)/len(model.reactions)*100)}%) blocked reactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69857e39",
   "metadata": {},
   "source": [
    "### 2.2 Identification of Dead-Ends Metabolites\n",
    "The detect_dead_ends( ) function from the utils module returns a list with all the **dead-end** metabolites in our model. A dead-end metabolite refers to a metabolite that is either only consumed but not produced, or only produced but not consumed, in a given metabolic network. The results are stored in the \"Dead-ends.txt\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29332b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Detect Dead-Ends ----- #####\n",
    "model.solver = 'gurobi' #change 'gurobi' for the default cobrapy solver 'glpk' \n",
    "dead_ends = detect_dead_ends(model)\n",
    "\n",
    "\n",
    "with open('Dead_ends.txt', 'w') as f:\n",
    "    for i,j in enumerate(dead_ends):\n",
    "        if j:\n",
    "            print(model.metabolites[i], file=f)\n",
    "            \n",
    "rxn_c = 0\n",
    "met_c = 0\n",
    "for i,j in enumerate(dead_ends):\n",
    "    if j:\n",
    "        met_c += 1\n",
    "        print(\"\\n\", model.metabolites[i],\": \")\n",
    "        for met_rxn in model.metabolites[i].reactions:\n",
    "            rxn_c += 1\n",
    "            print(met_rxn)\n",
    "            print(rxn_c)\n",
    "\n",
    "print(f'Percentage of Dead-End metabolites is {round(met_c/len(model.metabolites)*100)}%' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699099c9",
   "metadata": {},
   "source": [
    "### 2.3 Addition of Extracellular Exchange Reanctions\n",
    "The following cell adds **EXTRACELLULAR EXCHANGE** reactions to the dead-end metabolites in the extracellular compartment from the list generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60831416",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Automatically add EXTRACELLULAR EXCHANGE reactions to the \"BoundaryRxns\" Sheet ----- #####\n",
    "added_exchange = False\n",
    "for i,j in enumerate(dead_ends):\n",
    "    if j:\n",
    "        if str(model.metabolites[i]).endswith('_e'):\n",
    "            new_row_data = {'Curated': '', 'Reaction': 'EX_'+str(model.metabolites[i]), 'Reaction Name': 'Exchange of '+model.metabolites[i].name, 'Reaction Formula': str(model.metabolites[i])+' <=>', 'Subsystem': 'EXTRACELLULAR EXCHANGE',\n",
    "                                    'Reversible': 1, 'Lower bound': -1000, 'Upper bound': 1000, 'Objective': 0}\n",
    "            new_row_df = pd.DataFrame(new_row_data, index=[len(boundary_rxns)])\n",
    "            boundary_rxns = pd.concat([boundary_rxns, new_row_df])\n",
    "            added_exchange = True\n",
    "\n",
    "#Check for duplicated reactions added to the boundary_rxns dataset, IF NOT: update the google sheet file\n",
    "if added_exchange:\n",
    "    if not boundary_rxns['Reaction'].duplicated().any() and not boundary_rxns['Reaction Formula'].duplicated().any():\n",
    "        sheet.update_google_sheet(sheet_boundary, boundary_rxns)\n",
    "        print(\"BoundaryRxns Google Sheet updated.\")\n",
    "    else:\n",
    "        print('Duplicated values found in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be46e6d",
   "metadata": {},
   "source": [
    "### 2.4 Gapfill for blocked reactions\n",
    "Cobrapy has a gap filling implementation that is very similar to that of Reed et al. where we use a mixed-integer linear program to figure out the smallest number of reactions that need to be added for a user-defined collection of reactions, i.e. a universal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef010881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "from cobra.flux_analysis import gapfill\n",
    "\n",
    "#recon_3d = read_sbml_model(\"../Data/GPR_curation/Recon3D.xml\")\n",
    "#iCHO2291 = read_sbml_model(\"../Data/Reconciliation/models/iCHO2291.xml\")\n",
    "#universal = recon_3d.merge(iCHO2291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blocked_reaction in blocked:\n",
    "    model.objective = blocked_reaction\n",
    "    model.optimize().objective_value\n",
    "    try:\n",
    "        solution = gapfill(model, iCHO2291, demand_reactions=True)\n",
    "        print(blocked_reaction)\n",
    "        print(solution)\n",
    "    except Exception as e:\n",
    "        print(f'Gapfill failed for {blocked_reaction}: {str(e)}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25038fb8",
   "metadata": {},
   "source": [
    "### Test CHO - Recon GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16941789",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iCHO_recon3dfrom cobra.io import read_sbml_model\n",
    "# read_sbml_model(\".xml\")\n",
    "\n",
    "model_EX = [i for i, rxn in enumerate(model.reactions) if 'EX_' in rxn.id]\n",
    "model_SK = [i for i, rxn in enumerate(model.reactions) if 'SK_' in rxn.id]\n",
    "model_DM = [i for i, rxn in enumerate(model.reactions) if 'DM_' in rxn.id]\n",
    "for i in model_EX:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_SK:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_DM:\n",
    "    model.reactions[i].bounds = 0, 1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.objective = \"biomass_cho\" # \n",
    "sol1 = model.optimize()\n",
    "print(sol1.objective_value)\n",
    "\n",
    "model.objective = \"biomass_cho_prod\" # \n",
    "sol2 = model.optimize()\n",
    "print(sol2.objective_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Test model KOs ----- #####\n",
    "for reaction in model.reactions:\n",
    "    with model as model:\n",
    "        reaction.knock_out()\n",
    "        model.optimize()\n",
    "        print('%s blocked (bounds: %s), new growth rate %f' %\n",
    "              (reaction.id, str(reaction.bounds), model.objective.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e7b26",
   "metadata": {},
   "source": [
    "## 3. Generation of the Mass Flow Graph <a id='MFG'></a>\n",
    "Based on the publication **_Flux-dependent graphs for metabolic networks_** by _Beguerisse-Diaz et al. (2018)_ (https://www.nature.com/articles/s41540-018-0067-y). Here we use our model to build the **D Matrix** and plot the corresponding graph, then the **M Matrix** and plot the corresponding graph, and finally we generate the **PageRank** file with all the reactions in our reconstruction sorted by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055cf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from skimage import draw\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "import cobra\n",
    "from cobra.io import load_json_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Read Model ----- #####\n",
    "if 'model' not in locals():\n",
    "    model = load_json_model(\"iCHOv3_CHO_21112023.json\")\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712b3cb",
   "metadata": {},
   "source": [
    "### 3.1 Generation of the \"D-Matrix\"\n",
    "The \"D-Matrix\" defines the weight of the edge between reaction nodes Ri and Rj as the probability that any metabolite chosen at random is produced by Ri (reaction i) and consumed by Rj (reaction j). Summing over all metabolites and normalizing, we obtain the edge weights of the adjacency matrix of the NFG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b953700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stoichiometric matrix, m=reactions, n=metabolites\n",
    "start = time.time()\n",
    "S = cobra.util.array.create_stoichiometric_matrix(model)\n",
    "n, m = S.shape\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S, n and m: {end - start} seconds\")\n",
    "\n",
    "# Create S2m matrix\n",
    "start = time.time()\n",
    "r = np.array([reaction.reversibility for reaction in model.reactions]) # m-dimensional reversibility vector with components rj = 1 if reaction Rj is reversible and rj = 0 if it is irreversible.\n",
    "Im = np.eye(m) # m Ã— m identity matrix\n",
    "S2m = np.hstack((S, - S * r)) # unfolded version of the stoichiometric matrix of the 2m forward and reverse reactions.\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S2m: {end - start} seconds\")\n",
    "\n",
    "# Create S2mplus and S2mminus matrices\n",
    "start = time.time()\n",
    "S2mplus = (np.abs(S2m) + S2m) / 2 # production stoichiometric matrix\n",
    "S2mminus = (np.abs(S2m) - S2m) / 2 # consumption stoichiometric matrix\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S2mplus and S2minus: {end - start} seconds\")\n",
    "\n",
    "# Calculate weights\n",
    "start = time.time()\n",
    "Wplus = np.diag(np.nan_to_num(1/ np.sum(S2mplus, axis = 1)))\n",
    "Wminus = np.diag(np.nan_to_num(1 / np.sum(S2mminus, axis = 1)))\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate Wplus and Wminus: {end - start} seconds\")\n",
    "\n",
    "# Calculate D matrix\n",
    "start = time.time()\n",
    "D = 1/n * (Wplus @ S2mplus).T @ (Wminus @ S2mminus)\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate the D-Matrix: {end - start} seconds\")\n",
    "'''\n",
    "# Remove unused reactions\n",
    "start = time.time()\n",
    "IDr = np.nonzero(np.sum(abs(D), axis=0) + np.sum(abs(D), axis=1) == 0)[0]\n",
    "#IDr = ( np.sum(abs(D), axis=0) + np.sum(abs(D), axis=1) ) == 0\n",
    "IDr = IDr[IDr > m]\n",
    "\n",
    "D = np.delete(D, IDr, axis=0)\n",
    "D = np.delete(D, IDr, axis=1)\n",
    "end = time.time()\n",
    "print(f\"Time taken to remove unused reactions from the D-Matrix: {end - start} seconds\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1fa64",
   "metadata": {},
   "source": [
    "### 3.2 Plotting the \"D-Matrix\", Normalized Flow Graph (NFG)\n",
    "The NFG is a weighted, directed graph with reactions as nodes, the edges represent supplier-consumer relationships between reactions, and weights given by the probability that a metabolite chosen at random from all reactions is produced/consumed by the source/target reaction (this discounts naturally the over-representation of pool metabolites). The edge indicates that metabolites are produced by the source reaction and consumed by the target reaction, thus accounting for metabolic directionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43150760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert D matrix into a graph\n",
    "G = nx.from_numpy_array(D, create_using=nx.DiGraph)\n",
    "\n",
    "# Create a subgraph containing only the nodes with a degree greater than min_degree\n",
    "degrees = {node: val for (node, val) in G.degree()} # Calculate the degrees of each node\n",
    "min_degree = 2500 # Define the minimum degree for a node to be kept.\n",
    "nodes_to_keep = [node for node, degree in degrees.items() if degree >= min_degree]\n",
    "G_filtered = G.subgraph(nodes_to_keep)\n",
    "\n",
    "# Normalize weights for the edges of the nodes\n",
    "weights = [G_filtered[u][v]['weight'] for u,v in G_filtered.edges()]\n",
    "normalized_weights = [(w - min(weights)) / (max(weights) - min(weights)) * (7 - 0.001) + 0.001 for w in weights]\n",
    "\n",
    "# Create a dictionary mapping node indices to reaction names for labeling\n",
    "reaction_names = np.concatenate(([reaction.id for reaction in model.reactions], \n",
    "                                 [reaction.id + '_r' for reaction in model.reactions]))\n",
    "node_labels_filtered = {i: reaction_name for i, reaction_name in enumerate(reaction_names) if i in nodes_to_keep}\n",
    "\n",
    "# Normalize the degrees for color mapping\n",
    "degrees_filtered = [val for (node, val) in G_filtered.degree()]\n",
    "normalized_degrees_filtered = [(d - min(degrees_filtered)) / (max(degrees_filtered) - min(degrees_filtered)) for d in degrees_filtered]\n",
    "\n",
    "# Use a colormap to map normalized degrees to colors\n",
    "cmap = plt.get_cmap('OrRd')  # Choose a colormap here\n",
    "node_colors_filtered = [cmap(deg) for deg in normalized_degrees_filtered]\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos_filtered = nx.random_layout(G_filtered)\n",
    "nx.draw(G_filtered, pos_filtered, width=normalized_weights, node_color=node_colors_filtered, edge_color='lightgray', node_size=300, ax=ax, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Network Flux Graph (NFG)', fontsize=25, y=0.95)\n",
    "\n",
    "# Labels\n",
    "label_pos_filtered = {node: (x + 0.012, y + 0.012) for node, (x, y) in pos_filtered.items()}\n",
    "nx.draw_networkx_labels(G_filtered, label_pos_filtered, labels=node_labels_filtered, font_size=10, ax=ax)\n",
    "\n",
    "# Add a colorbar as the legend for node colors\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(normalized_degrees_filtered), vmax=max(normalized_degrees_filtered)))\n",
    "sm.set_array([])\n",
    "cbar_ax = fig.add_axes([0.7, 0.85, 0.07, 0.01])\n",
    "cbar = plt.colorbar(sm, cax=cbar_ax, label='Node Degree', orientation='horizontal')\n",
    "\n",
    "plt.savefig('../Networks/normalized_flow_graph.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769e605",
   "metadata": {},
   "source": [
    "### 3.3 Generation of the \"FluxOpenValue\" matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e70e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the bounds for boundary reactions\n",
    "model_EX = [i for i, rxn in enumerate(model.reactions) if 'EX_' in rxn.id]\n",
    "model_SK = [i for i, rxn in enumerate(model.reactions) if 'SK_' in rxn.id]\n",
    "model_DM = [i for i, rxn in enumerate(model.reactions) if 'DM_' in rxn.id]\n",
    "for i in model_EX:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_SK:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_DM:\n",
    "    model.reactions[i].bounds = 0, 1000\n",
    "\n",
    "# Perform pFBA for Biomass on Non-Producing and Producing Cell Lines\n",
    "model.solver = 'gurobi'\n",
    "objectives = ['biomass_cho', 'biomass_cho_prod']\n",
    "\n",
    "fluxes_list = []\n",
    "for objective in objectives:\n",
    "    model.objective = objective\n",
    "    pfba_solution = cobra.flux_analysis.pfba(model)\n",
    "    fluxes = np.array(pfba_solution.fluxes)\n",
    "    fluxes_list.append(fluxes)\n",
    "    \n",
    "# Stack arrays horizontally\n",
    "FluxOpenValue = np.column_stack(fluxes_list)\n",
    "\n",
    "FluxOpenValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of nonzero values in each row\n",
    "nonzero_counts = np.count_nonzero(FluxOpenValue, axis=1)\n",
    "\n",
    "# Count the number of rows that contain only zeros\n",
    "num_all_zero_rows = np.count_nonzero(nonzero_counts == 0)\n",
    "\n",
    "# Count the number of rows that contain some nonzero value\n",
    "num_some_nonzero_rows = np.count_nonzero(nonzero_counts != 0)\n",
    "\n",
    "print(\"Number of Rxns with no flux:\", num_all_zero_rows)\n",
    "print(\"Number of Rxns with any flux:\", num_some_nonzero_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5e6ee",
   "metadata": {},
   "source": [
    "### 3.4 Generation of the \"Mass Flow Graph (MFG) Matrix\"\n",
    "The MFG is a directed, environment-dependent, graph with weights computed from Flux Balance Analysis (FBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c8e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculation of the MFG for each pFBA analysis\n",
    "M_list = []\n",
    "for i in range(len(objectives)):\n",
    "    start = time.time()\n",
    "    v1 = FluxOpenValue[:, i].T\n",
    "\n",
    "    # unfolding the flux vector\n",
    "    # creation of vplus and vminus\n",
    "    vplus = (np.abs(v1) + v1) / 2\n",
    "    vminus = (np.abs(v1) - v1) / 2\n",
    "\n",
    "    # creation of v2m\n",
    "    v2m = np.concatenate((vplus, vminus))\n",
    "\n",
    "    # creation of J_v\n",
    "    J_v = S2mplus @ v2m.reshape(-1)\n",
    "\n",
    "    # calculation of the MFG\n",
    "    M = (S2mplus * v2m).T @ np.diag(np.nan_to_num(1/J_v)) @ (S2mminus * v2m)\n",
    "    \n",
    "    # Dynamically create a variable named M_<objective>\n",
    "    objective_name = objectives[i]\n",
    "    globals()[f'M_Matrix_{objective_name}'] = M\n",
    "\n",
    "    filename = f'../Networks/M_Matrix_{objectives[i]}.npy'\n",
    "    M_sparse = csr_matrix(M)\n",
    "    np.save(filename, M.astype(np.float32))\n",
    "    end = time.time()\n",
    "    print(f\"Time taken to go through iteration {i}: {end - start} seconds\")\n",
    "    \n",
    "'''\n",
    "# Post-processing of PageRank\n",
    "df = pd.DataFrame(PageRank)\n",
    "PageRank = df.values\n",
    "PageRank = np.array(PageRank).T\n",
    "PageRankRxns = PageRank[:m, :]\n",
    "PageRankRxns_back = PageRank[m:, :]\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(PageRankRxns.shape[1]):\n",
    "        if PageRankRxns_back[i, j] > PageRankRxns[i, j]:\n",
    "            PageRankRxns[i, j] = PageRankRxns_back[i, j]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f87a23",
   "metadata": {},
   "source": [
    "### 3.5 Plotting the \"MFG Matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the .npy file\n",
    "if M_Matrix_biomass_cho not in globals():\n",
    "    M_Matrix_biomass_cho = np.load('../Networks/M_Matrix_biomass_cho.npy')\n",
    "if M_Matrix_biomass_cho_prod not in globals():\n",
    "    M_Matrix_biomass_cho_prod = np.load('../Networks/M_Matrix_biomass_cho_prod.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6217b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which labels to display in the plot\n",
    "reactions_to_keep = [\"LipidSyn\", \"DNAsyn\",\"RNAsyn\", \"PROTsyn\", \"biomass_cho\"]\n",
    "#reactions_to_keep = [\"LipidSyn_prod\", \"DNAsyn_prod\",\"RNAsyn_prod\", \"PROTsyn_prod\", \"biomass_cho_prod\"]\n",
    "\n",
    "# Convert M matrix into a graph\n",
    "G = nx.from_numpy_array(M_Matrix_biomass_cho, create_using=nx.DiGraph)\n",
    "\n",
    "# Create a subgraph containing only the nodes with a degree greater than min_degree\n",
    "degrees = {node: val for (node, val) in G.degree()} # Calculate the degrees of each node\n",
    "min_degree = 1 # Define the minimum degree for a node to be kept.\n",
    "nodes_to_keep = [node for node, degree in degrees.items() if degree >= min_degree]\n",
    "\n",
    "# Create a dictionary mapping node indices to reaction names for labeling\n",
    "reaction_names = np.concatenate(([reaction.id for reaction in model.reactions],\n",
    "                         [reaction.id + '_r' for reaction in model.reactions]))\n",
    "mapping_reactions = {i: reaction_name for i, reaction_name in enumerate(reaction_names)}\n",
    "\n",
    "indices_to_keep = [i for i, reaction_name in enumerate(reaction_names) if reaction_name in reactions_to_keep]\n",
    "nodes_to_keep = list(set(nodes_to_keep).union(set(indices_to_keep)))\n",
    "node_labels_filtered = {i: reaction_name for i, reaction_name in enumerate(reaction_names) if i in nodes_to_keep}\n",
    "\n",
    "# Create a subgraph containing only the nodes with a degree greater than min_degree\n",
    "G_filtered = G.subgraph(node_labels_filtered)\n",
    "\n",
    "# Relabel the nodes in the filtered graph\n",
    "G_filtered = nx.relabel_nodes(G_filtered, node_labels_filtered)\n",
    "\n",
    "# Check if all nodes in node_labels_filtered are in G_filtered\n",
    "missing_nodes = set(node_labels_filtered.values()) - set(G_filtered.nodes())\n",
    "if missing_nodes:\n",
    "    print(f\"Nodes in node_labels_filtered but not in G_filtered: {missing_nodes}\")\n",
    "\n",
    "\n",
    "# Normalize weights for the edges of the nodes\n",
    "weights = [G_filtered[u][v]['weight'] for u,v in G_filtered.edges()]\n",
    "normalized_weights = [(w - min(weights)) / (max(weights) - min(weights)) * (7 - 0.001) + 0.001 for w in weights]\n",
    "\n",
    "# Normalize the degrees for color mapping\n",
    "degrees_filtered = [val for (node, val) in G_filtered.degree()]\n",
    "normalized_degrees_filtered = [(d - min(degrees_filtered)) / (max(degrees_filtered) - min(degrees_filtered)) for d in degrees_filtered]\n",
    "\n",
    "# Use a colormap to map normalized degrees to colors\n",
    "cmap = plt.get_cmap('OrRd')  # Choose a colormap here\n",
    "node_colors_filtered = [cmap(deg) for deg in normalized_degrees_filtered]\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos_filtered = nx.spring_layout(G_filtered, k=4.5, iterations=50)\n",
    "nx.draw(G_filtered, pos_filtered, width=normalized_weights, with_labels=False, node_color=node_colors_filtered, edge_color='lightgray', node_size=300, ax=ax, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Mass Flow Graph (MFG)', fontsize=20, y=0.95)\n",
    "\n",
    "\n",
    "offset = 0.02  # Adjust this value to move the labels up\n",
    "pos_labels = {node: (x, y + offset) for node, (x, y) in pos_filtered.items()}\n",
    "\n",
    "# Draw the graph with adjusted label positions\n",
    "nx.draw_networkx_labels(G_filtered, pos_labels, ax=ax, font_size=10)\n",
    "# Add a colorbar as the legend for node colors\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(normalized_degrees_filtered), vmax=max(normalized_degrees_filtered)))\n",
    "sm.set_array([])\n",
    "cbar_ax = fig.add_axes([0.7, 0.85, 0.07, 0.01])\n",
    "cbar = plt.colorbar(sm, cax=cbar_ax, label='Node Degree', orientation='horizontal')\n",
    "\n",
    "#nx.write_graphml(G, \"../Networks/mass_flow_network.graphml\") #save the object G\n",
    "plt.savefig('../Networks/mass_flow_graph.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd149ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert M matrix into a graph\n",
    "G = nx.from_numpy_array(M_Matrix_LipidSyn, create_using=nx.DiGraph)\n",
    "\n",
    "# Map reaction names to nodes\n",
    "reaction_names = np.concatenate(([reaction.id for reaction in model.reactions],\n",
    "                         [reaction.id + '_r' for reaction in model.reactions]))\n",
    "mapping_reactions = {i: reaction_name for i, reaction_name in enumerate(reaction_names)}\n",
    "G = nx.relabel_nodes(G, mapping_reactions)\n",
    "\n",
    "# Filter nodes based on a degree threshold and specific reactions\n",
    "min_degree = 30\n",
    "reactions_to_keep = [\"LipidSyn\", \"DNAsyn\", \"RNAsyn\", \"PROTsyn\", \"biomass_cho\"]\n",
    "\n",
    "# Identify nodes to keep based on the degree and specific reactions\n",
    "degrees = dict(G.degree())\n",
    "nodes_to_keep = {node for node, degree in degrees.items() if degree >= min_degree or node in reactions_to_keep}\n",
    "\n",
    "# Create a subgraph with the nodes we want to keep\n",
    "G_filtered = G.subgraph(nodes_to_keep)\n",
    "\n",
    "# Calculate weights and degrees for normalization\n",
    "edge_weights = nx.get_edge_attributes(G_filtered, 'weight')\n",
    "max_weight = max(edge_weights.values())\n",
    "min_weight = min(edge_weights.values())\n",
    "\n",
    "node_degrees = dict(G_filtered.degree())\n",
    "max_degree = max(node_degrees.values())\n",
    "min_degree = min(node_degrees.values())\n",
    "\n",
    "# Normalize edge weights and node degrees for visualization purposes\n",
    "normalized_weights = {edge: ((weight - min_weight) / (max_weight - min_weight) * 10 + 0.1) for edge, weight in edge_weights.items()}\n",
    "normalized_degrees = {node: ((degree - min_degree) / (max_degree - min_degree) * 200 + 20) for node, degree in node_degrees.items()}\n",
    "\n",
    "# Map normalized degrees to colors using a colormap\n",
    "cmap = plt.get_cmap('OrRd')\n",
    "node_colors = [cmap((degree - min_degree) / (max_degree - min_degree)) for node, degree in node_degrees.items()]\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos = nx.spring_layout(G_filtered, k=3.50, iterations=50)  # Adjust the layout parameters as needed\n",
    "\n",
    "nx.draw_networkx_edges(G_filtered, pos, width=list(normalized_weights.values()), alpha=0.5, edge_color='lightgray')\n",
    "\n",
    "nx.draw_networkx_nodes(G_filtered, pos, node_size=[normalized_degrees[node] for node in G_filtered.nodes()],\n",
    "                       node_color=node_colors, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "nx.draw_networkx_labels(G_filtered, pos, font_size=8, horizontalalignment='center')\n",
    "\n",
    "# Add a color bar corresponding to node degrees\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min_degree, vmax=max_degree))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, orientation='vertical', shrink=0.1, pad=0.02)\n",
    "cbar.set_label('Node Degree')\n",
    "\n",
    "plt.title('Mass Flow Graph (MFG)', fontsize=20)\n",
    "plt.savefig('../Networks/mass_flow_graph_improved.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baae9cf",
   "metadata": {},
   "source": [
    "### 3.6 Word Cloud Plot for Metabolites Frecuencies in Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store PageRank\n",
    "PageRank = []\n",
    "G = nx.from_numpy_array(M_Matrix, create_using=nx.DiGraph)\n",
    "pr = nx.pagerank(G)\n",
    "PageRank.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97176f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing of PageRank\n",
    "S = cobra.util.array.create_stoichiometric_matrix(model)\n",
    "n, m = S.shape\n",
    "df = pd.DataFrame(PageRank)\n",
    "PageRank = df.values\n",
    "PageRank = np.array(PageRank).T\n",
    "PageRankRxns = PageRank[:m, :]\n",
    "PageRankRxns_back = PageRank[m:, :]\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(PageRankRxns.shape[1]):\n",
    "        if PageRankRxns_back[i, j] > PageRankRxns[i, j]:\n",
    "            PageRankRxns[i, j] = PageRankRxns_back[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_sums = PageRankRxns.sum(axis=1)\n",
    "df = pd.DataFrame()\n",
    "for i,n in enumerate(objectives):\n",
    "    sorted_indices = np.argsort(PageRankRxns[:,i])\n",
    "    rxns_list = []\n",
    "    values_list = []\n",
    "    for s in sorted_indices[::-1]:\n",
    "        rxns_list.append(model.reactions[s].id)\n",
    "        values_list.append(PageRankRxns[s,i])\n",
    "    \n",
    "    df[n] = pd.Series(rxns_list)\n",
    "    df[f'values_{n}'] = pd.Series(values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in df.iterrows():\n",
    "    print(v['biomass_producing'],v['values_biomass_producing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets_list = []\n",
    "for rxn in df['biomass'][df['values_biomass'] > 0.0000412]:\n",
    "    r = model.reactions.get_by_id(rxn)\n",
    "    mets = r.metabolites\n",
    "    for met in mets:\n",
    "        mets_list.append(met.id)\n",
    "        \n",
    "for rxn in df['biomass_producing'][df['values_biomass_producing'] > 0.0000412]:\n",
    "    r = model.reactions.get_by_id(rxn)\n",
    "    mets = r.metabolites\n",
    "    for met in mets:\n",
    "        mets_list.append(met.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequencies of each metabolite\n",
    "mets_freq = Counter(mets_list)\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('h2o_')} #eliminate water\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('h2o2_')} #eliminate peroxide\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('co2_')} #eliminate carbon dioxide\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nh4_')} #eliminate amonium\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('h_')} #eliminate protons\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('atp_')} #eliminate atp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('adp_')} #eliminate adp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('amp_')} #eliminate amp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nad_')} #eliminate nad\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nadh_')} #eliminate nadh\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nadp_')} #eliminate nadp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nadph_')} #eliminate nadph\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('na1_')} #eliminate Sodium\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('coa_')} #eliminate CoA\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('accoa_')} #eliminate Acetyl-CoA\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('pi_')} #eliminate phosphate\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('ppi_')} #eliminate phosphate\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('fadh2_')} #eliminate FADH\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('fad_')} #eliminate FAD\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('o2_')} #eliminate Oxigen\n",
    "\n",
    "# Create a circular mask\n",
    "radius = 500  # you can change to the size you need\n",
    "circle_img = np.zeros((2*radius, 2*radius), np.uint8)\n",
    "rr, cc = draw.disk((radius, radius), radius)\n",
    "circle_img[rr, cc] = 1\n",
    "\n",
    "# Create the word cloud\n",
    "wordcloud = WordCloud(width = 1000, height = 500, mask=circle_img, background_color=\"rgba(255, 255, 255, 0)\", mode=\"RGBA\").generate_from_frequencies(mets_freq)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('wordcloud.png', bbox_inches='tight', transparent=True, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for met in mets_freq:\n",
    "    print(met,mets_freq[met])\n",
    "    counter+=1\n",
    "    \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6990b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the metabolites and their frequencies in a .txt file\n",
    "\n",
    "with open('metabolites.txt', 'w') as f:\n",
    "    for i, j in enumerate(mets_freq):\n",
    "        print(j,'Freq:',mets_freq[j], file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1aa9c0",
   "metadata": {},
   "source": [
    "## 4. Identification of duplicates through Chemical Formulas <a id='formulas'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemical Formula\n",
    "from cobra.io import read_sbml_model\n",
    "model = read_sbml_model(\"iCHOv3_CHO_23052023.xml\")\n",
    "\n",
    "import pandas as pd\n",
    "data = []\n",
    "\n",
    "for m in model.metabolites:\n",
    "    m.name = m.formula\n",
    "for r in model.reactions:\n",
    "    if r not in model.exchanges:\n",
    "        print(r)\n",
    "        reaction_id = r.id\n",
    "        reaction_name = r.name\n",
    "        reaction_formula = r.build_reaction_string(use_metabolite_names=True)\n",
    "        data.append([reaction_id, reaction_name, reaction_formula])\n",
    "df = pd.DataFrame(data, columns=['ID', 'Name', 'Reaction'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b85035",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.metabolites:\n",
    "    m.name = m.formula\n",
    "for r in model.reactions:\n",
    "    if r not in model.exchanges:\n",
    "        reaction_id = r.id\n",
    "        reaction_name = r.name\n",
    "        reaction_formula = r.build_reaction_string(use_metabolite_names=True)\n",
    "        data.append([reaction_id, reaction_name, reaction_formula])\n",
    "df = pd.DataFrame(data, columns=['ID', 'Name', 'Reaction'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a901faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_duplicates = df[df.duplicated(subset='Reaction', keep=False)].reset_index(drop=True)\n",
    "subset_duplicates = subset_duplicates.sort_values(by=['Reaction'])\n",
    "subset_duplicates.to_excel('Test.xlsx')\n",
    "\n",
    "for index, n in enumerate(subset_duplicates['Reaction']):\n",
    "    next_value = subset_duplicates['Reaction'][index + 1]\n",
    "    if n == next_value:\n",
    "        print(subset_duplicates['Name'][index])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee0c57",
   "metadata": {},
   "source": [
    "### Duplicates identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metabolites.get_by_id('uppg1_c').name, model.metabolites.get_by_id('HC01609_c').name)\n",
    "for r in model.metabolites.get_by_id('uppg1_c').reactions:\n",
    "    print(r)\n",
    "print('------')\n",
    "for r in model.metabolites.get_by_id('HC01609_c').reactions:\n",
    "    print(r)\n",
    "# Based on reations HMR_4772 and GapFill-R04972, HC01609_c and uppg1_c  are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df438661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metabolites.get_by_id('HC02187_c').name, model.metabolites.get_by_id('triodthy_c').name)\n",
    "for r in model.metabolites.get_by_id('HC02187_c').reactions:\n",
    "    print(r)\n",
    "print('------')\n",
    "for r in model.metabolites.get_by_id('triodthy_c').reactions:\n",
    "    print(r)\n",
    "# Based on reations HMR_6834 and HMR_6826, triodthy_c and HC02187_c  are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fdbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
