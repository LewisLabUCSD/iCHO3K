{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ecee4b",
   "metadata": {},
   "source": [
    "# Final CHO Model\n",
    "This notebook is to asses the validity of our reconstruction and how complete it is.\n",
    "\n",
    "[1. Generation of the dataset and model reconstruction](#generation) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.1 Retrieve information from the Google Sheet datasets reactions and metabolites**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.2 Build a model and feed it the information from the df generated** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.3 Save and validate the model** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1.4 Check for unbalanced reactions** <br>\n",
    "\n",
    "[2. Identification of Blocked Reactions and Dead-End Metabolites](#blocked&deadends) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.1 Identification of Blocked Reactions**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.2 Identification of Dead-Ends Metabolites** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.3 Gap-fill** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2.4 Addition of Extracellular Exchange Reanctions** <br>\n",
    "\n",
    "[3. Generation of the Mass Flow Graph](#MFG) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.1 Generation of the \"D-Matrix\"**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.2 Plotting the \"D-Matrix\", Normalized Flow Graph (NFG)** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.3 Generation of the \"FluxOpenValue\" matrix** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.4 Generation of the \"Mass Flow Graph (MFG) Matrix\"** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**3.5 Plotting the \"MFG Matrix\"** <br>\n",
    "\n",
    "[4. Biomass Conecting Reactions](#biomass) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1047d",
   "metadata": {},
   "source": [
    "## 1. Generation of the dataset and model reconstruction <a id='generation'></a>\n",
    "Here we generate the CHO model from the dataset stored in the Google Sheet file. We first use the google_sheet module to extract all the necessary information from the original dataset. Then we use those dataset and the COBRApy library to: (1) Create a new model and add reactions from the **Rxns Sheet**, (2) Add information on each reaction obtained from the **Rxns Sheet** and **Attributes Sheet**, (3) Add boundary reactions from the **BoundaryRxns Sheet**, and (4) Add information for each metabolite from the **Metabolites Sheet**. Finally we save the model as a SBML file and validate it using the cobrapy built-in function \"validate_sbml_model( )\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cobra\n",
    "from cobra import Model, Reaction, Metabolite\n",
    "from cobra.io import validate_sbml_model, save_json_model, write_sbml_model, save_matlab_model, load_matlab_model\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from google_sheet import GoogleSheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ed354",
   "metadata": {},
   "source": [
    "### 1.1 Retrieve information from the Google Sheet datasets reactions and metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719281dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Generate datasets from Google Sheet ----- #####\n",
    "\n",
    "#Credential file\n",
    "KEY_FILE_PATH = 'credentials.json'\n",
    "\n",
    "#CHO Network Reconstruction + Recon3D_v3 Google Sheet ID\n",
    "SPREADSHEET_ID = '1MlBXeHIKw8k8fZyXm-sN__AHTRSunJxar_-bqvukZws'\n",
    "\n",
    "# Initialize the GoogleSheet object\n",
    "sheet = GoogleSheet(SPREADSHEET_ID, KEY_FILE_PATH)\n",
    "\n",
    "# Read data from the Google Sheet\n",
    "sheet_met = 'Metabolites'\n",
    "sheet_rxns = 'Rxns'\n",
    "sheet_attributes = 'Attributes'\n",
    "sheet_boundary = 'BoundaryRxns'\n",
    "sheet_genes = 'Genes'\n",
    "\n",
    "metabolites = sheet.read_google_sheet(sheet_met)\n",
    "rxns = sheet.read_google_sheet(sheet_rxns)\n",
    "rxns_attributes = sheet.read_google_sheet(sheet_attributes)\n",
    "boundary_rxns = sheet.read_google_sheet(sheet_boundary)\n",
    "genes_df = sheet.read_google_sheet(sheet_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcd212",
   "metadata": {},
   "source": [
    "### 1.2 Build a model and feed it the information from the df generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927503f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Create a model and add reactions ----- #####\n",
    "model = Model(\"iCHO3595\")\n",
    "lr = []\n",
    "for _, row in rxns.iterrows():\n",
    "    r = Reaction(row['Reaction'])\n",
    "    lr.append(r)    \n",
    "model.add_reactions(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c619ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### ----- Add information to each one of the reactions ----- #####\n",
    "for i,r in enumerate(tqdm(model.reactions)):\n",
    "    r.build_reaction_from_string(rxns['Reaction Formula'][i])\n",
    "    r.name = rxns['Reaction Name'][i]\n",
    "    r.subsystem = rxns['Subsystem'][i]\n",
    "    if not (pd.isna(rxns['GPR_final'][i]) or rxns['GPR_final'][i] == ''):\n",
    "        r.gene_reaction_rule = str(rxns['GPR_final'][i])\n",
    "    r.lower_bound = float(rxns_attributes['Lower bound'][i])\n",
    "    r.upper_bound = float(rxns_attributes['Upper bound'][i])\n",
    "    r.annotation['confidence_score'] = str(rxns['Conf. Score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4567f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Add Boundary Reactions ----- #####\n",
    "dr = []\n",
    "for _, row in boundary_rxns.iterrows():\n",
    "    r = Reaction(row['Reaction'])\n",
    "    dr.append(r)    \n",
    "model.add_reactions(dr)\n",
    "\n",
    "boundary_rxns_dict = boundary_rxns.set_index('Reaction').to_dict()\n",
    "boundary_rxns_dict\n",
    "\n",
    "for i,r in enumerate(tqdm(model.reactions)):\n",
    "    if r in dr:\n",
    "        r.build_reaction_from_string(boundary_rxns_dict['Reaction Formula'][r.id])\n",
    "        r.name = boundary_rxns_dict['Reaction Name'][r.id]\n",
    "        r.subsystem = boundary_rxns_dict['Subsystem'][r.id]\n",
    "        r.lower_bound = float(boundary_rxns_dict['Lower bound'][r.id])\n",
    "        r.upper_bound = float(boundary_rxns_dict['Upper bound'][r.id])\n",
    "        r.annotation['confidence_score'] = str(1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a76a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### ----- Add information for each metabolite ----- #####\n",
    "metabolites_dict = metabolites.set_index('BiGG ID').to_dict('dict')\n",
    "for met in model.metabolites:\n",
    "    try:\n",
    "        met.name = metabolites_dict['Name'][f'{met}']\n",
    "        met.formula = metabolites_dict['Formula'][f'{met}']\n",
    "        met.compartment = metabolites_dict['Compartment'][f'{met}'].split(' - ')[0]\n",
    "        try:\n",
    "            met.charge = int(metabolites_dict['Charge'][f'{met}'])\n",
    "        except (ValueError, TypeError):\n",
    "            print(f'{met} doesnt have charge')\n",
    "    except (KeyError):\n",
    "        print('----------------------------')\n",
    "        print(f'{met} doesnt exist in the df')\n",
    "        print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d2c75-acec-450f-a024-3c9aa7376eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### ----- Add Gene Name information ----- #####\n",
    "genes_dict = genes_df.iloc[:,:2].set_index('Gene Entrez ID').to_dict('dict')\n",
    "for g in model.genes:\n",
    "    if g.id in list(genes_dict['Gene Symbol'].keys()):\n",
    "        g.name = genes_dict['Gene Symbol'][f'{g}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82092731",
   "metadata": {},
   "source": [
    "### 1.3 Save and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Build the S matrix ----- #####\n",
    "S = cobra.util.create_stoichiometric_matrix(model, array_type='dense')\n",
    "model.S = S\n",
    "\n",
    "model.solver = 'gurobi'\n",
    "#model.objective = 'biomass_cho'\n",
    "model.objective = 'biomass_cho_s' # Set specific biomass reaction for ZeLa data cell lines\n",
    "sol = model.optimize()\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Save the entiry reconstruction witouh any preprocessing ----- #####\n",
    "\n",
    "# XML\n",
    "model_name_xml = 'iCHO3595.xml' \n",
    "write_sbml_model(model, model_name_xml)\n",
    "\n",
    "# JSON, because the sbml doesnt save the subsystems\n",
    "model_name_json = 'iCHO3595.json' \n",
    "save_json_model(model, model_name_json)\n",
    "\n",
    "# MATLAB\n",
    "model_name_matlab = 'iCHO3595.mat' \n",
    "save_matlab_model(model, model_name_matlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Test for errors in the recostruction ----- ######\n",
    "\n",
    "# import tempfile\n",
    "# from pprint import pprint\n",
    "# from cobra.io import write_sbml_model, validate_sbml_model\n",
    "# with tempfile.NamedTemporaryFile(suffix='.xml') as f_sbml:\n",
    "#     write_sbml_model(model, filename=f_sbml.name)\n",
    "#     report = validate_sbml_model(filename=f_sbml.name)\n",
    "# pprint(report)\n",
    "\n",
    "from cobra.io import read_sbml_model, validate_sbml_model\n",
    "(_, errors) = validate_sbml_model(model_name_xml)\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5d619",
   "metadata": {},
   "source": [
    "### 1.4 Check for unbalanced reactions\n",
    "Once the model is checked and saved as a xml and json format we then evaluate the amount of mass and charge unbalanced reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127620b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for unbalanced reactions\n",
    "subsystems = ['BIOMASS', 'PROTEIN ASSEMBLY', 'PROTEIN DEGRADATION'] # filter out the reactions from these subsystems\n",
    "\n",
    "# Lists to store the data for each column\n",
    "reaction_ids = []\n",
    "formulas = []\n",
    "metabolites = []\n",
    "unbalances = []\n",
    "\n",
    "counter = 0\n",
    "for rxn in model.reactions:\n",
    "    if not rxn.id.startswith(('EX_','DM_','SK_')) and rxn.subsystem not in subsystems:\n",
    "        mb = rxn.check_mass_balance()\n",
    "        if mb != {}:# and set(mb.keys()) != {'charge'}:  # Check if dictionary has keys other than 'charge'\n",
    "            counter+=1\n",
    "            prod_ids = [{met.id:met.formula} for met in rxn.products]\n",
    "            react_ids = [{met.id:met.formula} for met in rxn.reactants]\n",
    "            # Append values to lists\n",
    "            print(rxn.id)\n",
    "            reaction_ids.append(rxn.id)\n",
    "            print(rxn.reaction)\n",
    "            formulas.append(rxn.reaction)\n",
    "            print(react_ids + prod_ids)\n",
    "            metabolites.append(react_ids + prod_ids)\n",
    "            print(mb)\n",
    "            unbalances.append(mb)\n",
    "            print('...............................')\n",
    "print(counter)\n",
    "\n",
    "# Create DataFrame from lists\n",
    "mass_unbalanced_reactions = pd.DataFrame({\n",
    "    \"Reaction ID\": reaction_ids,\n",
    "    \"Formula\": formulas,\n",
    "    \"Metabolites\": metabolites,\n",
    "    \"Unbalance\": unbalances\n",
    "})\n",
    "\n",
    "mass_unbalanced_reactions.to_excel(\"temp/mass_unbalanced_reactions.xlsx\", engine='openpyxl', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the \"Balance status\" column based on whether the reaction is present in \"reaction_ids\"\n",
    "rxns_copy = rxns.copy()\n",
    "rxns_copy['Balance status'] = rxns_copy['Reaction'].apply(lambda x: 'UNBALANCED' if x in reaction_ids else 'BALANCED')\n",
    "rxns_equals = rxns_copy.equals(rxns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2658c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#### ------------------------------------ ####\n",
    "#### ---- Update Rxns Google Sheets ----- ####\n",
    "#### ------------------------------------ ####\n",
    "##############################################\n",
    "if not rxns_equals:\n",
    "    sheet.update_google_sheet(sheet_rxns, rxns_copy)\n",
    "    print(\"Google Sheet updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2da40b",
   "metadata": {},
   "source": [
    "## 2. Identification of Blocked Reactions and Dead-End Metabolites <a id='blocked&deadends'></a>\n",
    "In this second part of the notebook we use two different functions from the utils module to: (1) Run a flux variability analysis and identify blocked reactions, and (2) identify dead-end metabolites. Finally we add Extracellular Exchange reactions for the dead-end metabolites that are in the extracellular compartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cobra.io import load_json_model, read_sbml_model, load_matlab_model\n",
    "from cobra.flux_analysis import find_blocked_reactions, flux_variability_analysis\n",
    "from utils import detect_dead_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Read Model ----- #####\n",
    "if 'model' not in locals():\n",
    "    model = load_json_model(\"iCHO3595.json\")\n",
    "    print('Model loaded')\n",
    "else:\n",
    "    print('Model already generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919b458",
   "metadata": {},
   "source": [
    "### 2.1 Identification of Blocked Reactions\n",
    "Here we use the COBRApy built-in functions **find_blocked_reactions** and **flux_variability_analysis** to find blocked reactions in our reconstruction and run an FVA analysis respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f590df8-5c5e-4c79-bce6-972beff99edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this group of reactions to avoid Glucose generation loop\n",
    "\n",
    "#glucloop_reactions = [\n",
    "#    'GLPASE1', 'MG2er', 'GapFill-R01206', 'GAUGE-R00557', \n",
    "#    'GAUGE-R00558', 'FNOR', 'GGH', 'r0741', 'r1479', 'XOLESTPOOL'\n",
    "#]\n",
    "\n",
    "glucloop_reactions = [\n",
    "    'GapFill-R01206', 'GAUGE-R00557', \n",
    "    'GAUGE-R00558', 'FNOR', 'GGH', 'r0741', 'r1479', 'XOLESTPOOL'\n",
    "]\n",
    "\n",
    "try:\n",
    "    model.remove_reactions(glucloop_reactions, remove_orphans=True)\n",
    "except KeyError:\n",
    "    print(f'Reaction {reaction_id} not in model {model.id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Blocked Reactions ----- #####\n",
    "for rxn in model.boundary:\n",
    "    if rxn.id.startswith(\"EX_\"):\n",
    "        rxn.bounds = (-1000,1000)\n",
    "    if rxn.id.startswith(\"SK_\"):\n",
    "        rxn.bounds = (-1000,1000)\n",
    "    if rxn.id.startswith(\"DM_\"):\n",
    "        rxn.bounds = (0,1000)\n",
    "\n",
    "model.solver = 'gurobi'\n",
    "blocked_reactions = find_blocked_reactions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a9f68-6896-4e3f-b73d-5b4fadbf23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the bounds for context-specific model generation\n",
    "\n",
    "for rxn in model.boundary:\n",
    "        \n",
    "    # Models that are forced to secrete ethanol are not feasible\n",
    "    if rxn.id == 'EX_etoh_e':\n",
    "        rxn.bounds = (-1,1)\n",
    "        continue\n",
    "    \n",
    "    # Keep boundaries open for essential metabolites\n",
    "    if rxn.id == 'EX_h2o_e':\n",
    "        rxn.bounds = (-1000,1000)\n",
    "        continue\n",
    "    if rxn.id == 'EX_h_e':\n",
    "        rxn.bounds = (-1000,1000)\n",
    "        continue\n",
    "    if rxn.id == 'EX_o2_e':\n",
    "        rxn.bounds = (-1000,1000)\n",
    "        continue\n",
    "    if rxn.id == 'EX_hco3_e':\n",
    "        rxn.bounds = (-1000,1000)\n",
    "        continue\n",
    "    if rxn.id == 'EX_so4_e':\n",
    "        rxn.bounds = (-1000,1000)\n",
    "        continue\n",
    "    if rxn.id == 'EX_pi_e':\n",
    "        rxn.bounds = (-1000,1000)\n",
    "        continue\n",
    "\n",
    "    # Boundaries from Sink reactions on iCHO_v1 (100 times lower)\n",
    "    if rxn.id == 'SK_Asn_X_Ser_Thr_r':\n",
    "        rxn.bounds = (-0.001,1000)\n",
    "        continue\n",
    "    if rxn.id == 'SK_Tyr_ggn_c':\n",
    "        rxn.bounds = (-0.001,1000)\n",
    "        continue\n",
    "    if rxn.id == 'SK_Ser_Thr_g':\n",
    "        rxn.bounds = (-0.001,1000)\n",
    "        continue\n",
    "    if rxn.id == 'SK_pre_prot_r':\n",
    "        rxn.bounds = (-0.001,1000)\n",
    "        continue\n",
    "    \n",
    "    # Close uptake rates for the rest of the boundaries\n",
    "    if rxn.id.startswith(\"EX_\"):\n",
    "        rxn.bounds = (0,1000) \n",
    "    if rxn.id.startswith(\"SK_\"):\n",
    "        rxn.bounds = (0,1000)\n",
    "    if rxn.id.startswith(\"DM_\"):\n",
    "        rxn.bounds = (0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052bade-e3b4-4f85-9f64-dd212edce316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- Remove blocked reactions from the model and save it as a separete model ---- ####\n",
    "\n",
    "model_unblocked = model.copy()\n",
    "\n",
    "# Convert list of reaction IDs to reaction objects\n",
    "blocked_reaction_objects = [model_unblocked.reactions.get_by_id(rxn_id) for rxn_id in blocked_reactions]\n",
    "\n",
    "# Remove blocked reactions\n",
    "model_unblocked.remove_reactions(blocked_reaction_objects, remove_orphans=True)\n",
    "print(f\"Removed {len(blocked_reaction_objects)} blocked reactions from the model.\")\n",
    "\n",
    "# XML\n",
    "model_name_xml = 'iCHO3595_unblocked.xml' \n",
    "write_sbml_model(model_unblocked, model_name_xml)\n",
    "\n",
    "# JSON, because the sbml doesnt save the subsystems\n",
    "model_name_json = 'iCHO3595_unblocked.json' \n",
    "save_json_model(model_unblocked, model_name_json)\n",
    "\n",
    "# MATLAB\n",
    "model_name_matlab = 'iCHO3595_unblocked.mat' \n",
    "save_matlab_model(model_unblocked, model_name_matlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97f270-d590-441b-b1de-fc193a26a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confidence score for Context-Specific model generation\n",
    "\n",
    "conf_scores = []\n",
    "for r in model_unblocked.reactions:\n",
    "    conf_scores.append(r.annotation['confidence_score'])\n",
    "conf_scores_array = np.array(conf_scores)\n",
    "\n",
    "np.savetxt(\"../Data/Context_specific_models/confidence_scores.csv\", conf_scores_array, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- FVA ---- ####\n",
    "model.solver = 'gurobi'\n",
    "fva_results = flux_variability_analysis(model)\n",
    "fva_results.to_excel('temp/fva_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if any reaction in the BIOMASS subsystem is blocked\n",
    "for rxn in blocked_reactions:\n",
    "    r = model.reactions.get_by_id(rxn)\n",
    "    if r.subsystem == 'BIOMASS':\n",
    "        print(r.id)\n",
    "        print('-----------------')\n",
    "        print('-----------------')\n",
    "        for met in r.metabolites:\n",
    "            m = model.metabolites.get_by_id(met.id)\n",
    "            print(m)\n",
    "            print('.................')\n",
    "            for r2 in m.reactions:\n",
    "                if r2.id in blocked_reactions:\n",
    "                    print(f'No Flux -> {r2.id}: {r2.reaction}')\n",
    "                else:\n",
    "                    print(f'With Flux -> {r2.id}: {r2.reaction}')\n",
    "            print('.................')\n",
    "            print(' ')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ab78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Print the amount  and % of blocked reactions ----- #####\n",
    "print('##### ----- Blocked Reactions ----- #####')\n",
    "print(f'The model has {len(model.reactions)} total reactions')\n",
    "print(f'The model has {len(blocked_reactions)} ({round(len(blocked_reactions)/len(model.reactions)*100)}%) blocked reactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69857e39",
   "metadata": {},
   "source": [
    "### 2.2 Identification of Dead-Ends Metabolites\n",
    "The detect_dead_ends( ) function from the utils module returns a list with all the **dead-end** metabolites in our model. A dead-end metabolite refers to a metabolite that is either only consumed but not produced, or only produced but not consumed, in a given metabolic network. The results are stored in the \"Dead-ends.txt\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29332b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Detect Dead-Ends ----- #####\n",
    "model.solver = 'gurobi' #change 'gurobi' for the default cobrapy solver 'glpk' \n",
    "dead_ends = detect_dead_ends(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b86e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counter=0\n",
    "for i, is_dead_end in enumerate(dead_ends):\n",
    "    if is_dead_end:\n",
    "        metabolite = model.metabolites[i]\n",
    "        counter+=1\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e75eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dead ends and their associated reaction in a pandas df\n",
    "data = []\n",
    "\n",
    "for i, is_dead_end in enumerate(dead_ends):\n",
    "    if is_dead_end:\n",
    "        metabolite = model.metabolites[i]\n",
    "        reactions = [str(met_rxn) for met_rxn in metabolite.reactions]  # Convert reactions to strings\n",
    "        data.append([metabolite.id] + reactions)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "dead_ends_df = pd.DataFrame(data, columns=['Metabolite', 'Reaction1', 'Reaction2', 'Reaction3', 'Reaction4'])  # 'etc.' is a placeholder\n",
    "\n",
    "# Adjusting the DataFrame to handle variable number of reactions\n",
    "dead_ends_df = dead_ends_df.apply(lambda x: pd.Series(x.dropna().values), axis=1).fillna('')\n",
    "\n",
    "# Renaming the columns appropriately\n",
    "new_columns = ['Metabolite'] + [f'Reaction{i}' for i in range(1, len(dead_ends_df.columns))]\n",
    "dead_ends_df.columns = new_columns\n",
    "\n",
    "dead_ends_df.to_excel('temp/dead_ends_reactions.xlsx', index=False)\n",
    "\n",
    "print(f'Total amount of dead-end metabolites: {len(dead_ends_df)}')  # To display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec404fe-9b79-4e6e-8cd7-1cf7c3f25e8b",
   "metadata": {},
   "source": [
    "### 2.3 Gap-fill\n",
    "Here we try different gap-filling approaches to fix dead-end metabolites in our reconstruction. First, we create artificial transport reactions if we detect that a dead-end metabolites is at oposite sides in different reactions. Then we extract boundary reactions from other reconstructions associated with our dead-end metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### --- Create transport reactions to fill the gaps --- ######\n",
    "\n",
    "def check_metabolite_sides(reactions, metabolite_base):\n",
    "    sides = []  # List to store the side ('left' or 'right') of each reaction\n",
    "    for reaction in reactions:\n",
    "        # Splitting the reaction string into reactants and products\n",
    "        if '-->' in reaction:\n",
    "            reactants, products = reaction.split('-->')\n",
    "        elif '<=>' in reaction:\n",
    "            reactants, products = reaction.split('<=>')\n",
    "\n",
    "        # Splitting reactants and products into individual metabolites and trimming whitespace\n",
    "        reactant_ids = [r.strip() for r in reactants.split('+')]\n",
    "        product_ids = [p.strip() for p in products.split('+')]\n",
    "\n",
    "        # Constructing specific identifiers for comparison\n",
    "        metabolite_id_with_compartment = f\"{metabolite_base}_\"\n",
    "\n",
    "        # Checking if the specific identifier is in reactants or products\n",
    "        if any(metabolite_id_with_compartment in r for r in reactant_ids):\n",
    "            sides.append('left')\n",
    "        if any(metabolite_id_with_compartment in p for p in product_ids):\n",
    "            sides.append('right')\n",
    "\n",
    "    return 'left' in sides and 'right' in sides\n",
    "\n",
    "\n",
    "# Use the modified metabolites_compartments dictionary creation logic from before\n",
    "\n",
    "# Initialize a dictionary to keep track of metabolites, their compartments, and reactions\n",
    "metabolites_compartments = {}\n",
    "\n",
    "for i, is_dead_end in enumerate(dead_ends):\n",
    "    if is_dead_end:\n",
    "        met = model.metabolites[i]\n",
    "        base_id = met.id[:-2]  # Extract the base ID of the metabolite\n",
    "        compartment = met.id[-1]  # Extract the compartment\n",
    "        reactions = {str(met_rxn) for met_rxn in met.reactions}  # Use a set for unique reactions\n",
    "\n",
    "        # Check if the base ID is already in the dictionary\n",
    "        if base_id in metabolites_compartments:\n",
    "            # Add the compartment if not already present and update the reactions set\n",
    "            metabolites_compartments[base_id]['compartments'].add(compartment)\n",
    "            metabolites_compartments[base_id]['reactions'].update(reactions)\n",
    "        else:\n",
    "            # If the base ID is not in the dictionary, add it with the current compartment and reactions\n",
    "            metabolites_compartments[base_id] = {'compartments': {compartment}, 'reactions': reactions}\n",
    "\n",
    "# Filtering metabolites present on opposite sides of reaction formulas\n",
    "for metabolite, info in list(metabolites_compartments.items()):\n",
    "    if len(info['compartments']) > 1:\n",
    "        # Only keep metabolites that appear on opposite sides of the reaction equations\n",
    "        if not check_metabolite_sides(info['reactions'], metabolite):\n",
    "            del metabolites_compartments[metabolite]  # Remove metabolites not meeting the criteria\n",
    "\n",
    "# Displaying the filtered results\n",
    "counter=0\n",
    "transport_reactions = []\n",
    "for metabolite, info in metabolites_compartments.items():\n",
    "    if len(info['compartments']) > 1:\n",
    "        compartments = list(info['compartments'])\n",
    "        for i in range(len(compartments)):\n",
    "            for j in range(i+1, len(compartments)):\n",
    "                # Construct the reaction string\n",
    "                treaction = f\"{metabolite}_{compartments[i]} <=> {metabolite}_{compartments[j]}\"\n",
    "                transport_reactions.append(treaction)\n",
    "        print(f\"{metabolite} is present in compartments: {', '.join(info['compartments'])}\")\n",
    "        print(\"Associated reactions:\")\n",
    "        for reaction in info['reactions']:\n",
    "            print(reaction)\n",
    "        print('------------------------------')\n",
    "        print(f'Reaction created: {treaction}')\n",
    "        counter+=1\n",
    "        print()\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50bded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models to extract boundary reactions from\n",
    "iCHO1766 = read_sbml_model('../Data/Reconciliation/models/iCHOv1_final.xml')\n",
    "iCHO2291 = read_sbml_model('../Data/Reconciliation/models/iCHO2291.xml')\n",
    "recon3d = load_matlab_model('../Data/Reconciliation/models/Recon3D_301.mat')\n",
    "\n",
    "models = [iCHO1766, iCHO2291, recon3d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbac295-9dfc-4800-9a3a-45aa32c695f8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### --- Exctracting boundary reaction from other recosntructions --- ######\n",
    "\n",
    "# Initialize the DataFrame with the desired columns\n",
    "columns = ['ID','Name','Reaction', 'GPR', 'Subsystem', 'Lower Bound', 'Upper Bound']\n",
    "reactions_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Initialize a set to track unique reaction IDs\n",
    "seen_ids = set()\n",
    "\n",
    "c=0\n",
    "for mdl in models:\n",
    "    for rxn in mdl.demands:\n",
    "        # Standarize r.id according to our reconstruction\n",
    "        rxn_id = rxn.id.replace('[', '_').replace(']', '').replace('_hs_', '_cho_').rstrip('_')\n",
    "        if rxn_id not in seen_ids:\n",
    "            seen_ids.add(rxn_id)\n",
    "            # Standarize r.reaction according to our reconstruction\n",
    "            rxn_reaction = rxn.reaction.replace('[', '_').replace(']', '').replace('_hs_', '_cho_')\n",
    "            # Standarize met ids according to our reconstruction\n",
    "            r_m = [m.id for m in rxn.metabolites][0].replace('[', '_').replace(']', '').replace('_hs_', '_cho_')\n",
    "            for i, is_dead_end in enumerate(dead_ends):\n",
    "                if is_dead_end:\n",
    "                    met = model.metabolites[i]\n",
    "                    if met.id == r_m:\n",
    "                        # Create a temporary DataFrame for the new entry\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'ID': [rxn_id],\n",
    "                            'Name': [rxn.name],\n",
    "                            'Reaction': [rxn_reaction],\n",
    "                            'GPR': [rxn.gpr],\n",
    "                            'Subsystem': [rxn.subsystem],\n",
    "                            'Lower Bound': [rxn.lower_bound],\n",
    "                            'Upper Bound': [rxn.upper_bound]\n",
    "                        })\n",
    "                        # Concatenate the new row to the main DataFrame\n",
    "                        reactions_df = pd.concat([reactions_df, new_row], ignore_index=True)\n",
    "                        c+=1\n",
    "\n",
    "reactions_df.to_excel('temp/gap_fill_boundaries_output.xlsx', index=False)  # 'index=False' avoids writing row indices to the file.\n",
    "print(f\"Total reactions processed: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47752f3-909b-4059-a520-594c4cd9dc70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### --- Exctracting actual reactions from other reconstructions --- ######\n",
    "\n",
    "# Initialize the DataFrame with the desired columns\n",
    "columns = ['ID','Name','Reaction', 'GPR', 'Subsystem', 'Lower Bound', 'Upper Bound']\n",
    "reactions_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Initialize a set to track unique reaction IDs\n",
    "iCHO3000_rxn_ids = set([r.id for r in model.reactions])\n",
    "\n",
    "c=0\n",
    "for mdl in models:\n",
    "    for rxn in mdl.reactions:\n",
    "        # Standarize r.id according to our reconstruction\n",
    "        rxn_id = rxn.id.replace('[', '_').replace(']', '').replace('_hs_', '_cho_').rstrip('_')\n",
    "        if rxn_id not in iCHO3000_rxn_ids:\n",
    "            iCHO3000_rxn_ids.add(rxn_id)\n",
    "            # Standarize r.reaction according to our reconstruction\n",
    "            rxn_reaction = rxn.reaction.replace('[', '_').replace(']', '').replace('_hs_', '_cho_')\n",
    "            # Standarize met ids according to our reconstruction\n",
    "            r_m = [m.id.replace('[', '_').replace(']', '').replace('_hs_', '_cho_') for m in rxn.metabolites]\n",
    "            for i, is_dead_end in enumerate(dead_ends):\n",
    "                if is_dead_end:\n",
    "                    met = model.metabolites[i]\n",
    "                    if met.id in r_m:\n",
    "                        # Create a temporary DataFrame for the new entry\n",
    "                        new_row = pd.DataFrame({\n",
    "                            'ID': [rxn_id],\n",
    "                            'Name': [rxn.name],\n",
    "                            'Reaction': [rxn_reaction],\n",
    "                            'GPR': [rxn.gpr],\n",
    "                            'Subsystem': [rxn.subsystem],\n",
    "                            'Lower Bound': [rxn.lower_bound],\n",
    "                            'Upper Bound': [rxn.upper_bound],\n",
    "                            'Dead_end': [met.id],\n",
    "                            'Model': [mdl.id],\n",
    "                        })\n",
    "                        # Concatenate the new row to the main DataFrame\n",
    "                        reactions_df = pd.concat([reactions_df, new_row], ignore_index=True)\n",
    "                        c+=1\n",
    "\n",
    "reactions_df.to_excel('temp/gap_fill_boundaries_output.xlsx', index=False)  # 'index=False' avoids writing row indices to the file.\n",
    "print(f\"Total reactions processed: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699099c9",
   "metadata": {},
   "source": [
    "### 2.3 Addition of Extracellular Exchange Reanctions\n",
    "The following cell adds **EXTRACELLULAR EXCHANGE** reactions to the dead-end metabolites in the extracellular compartment from the list generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60831416",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Automatically add EXTRACELLULAR EXCHANGE reactions to the \"BoundaryRxns\" Sheet ----- #####\n",
    "added_exchange = False\n",
    "for i,j in enumerate(dead_ends):\n",
    "    if j:\n",
    "        if str(model.metabolites[i]).endswith('_e'):\n",
    "            new_row_data = {'Curated': '', 'Reaction': 'EX_'+str(model.metabolites[i]), 'Reaction Name': 'Exchange of '+model.metabolites[i].name, 'Reaction Formula': str(model.metabolites[i])+' <=>', 'Subsystem': 'EXTRACELLULAR EXCHANGE',\n",
    "                                    'Reversible': 1, 'Lower bound': -1000, 'Upper bound': 1000, 'Objective': 0}\n",
    "            new_row_df = pd.DataFrame(new_row_data, index=[len(boundary_rxns)])\n",
    "            boundary_rxns = pd.concat([boundary_rxns, new_row_df])\n",
    "            added_exchange = True\n",
    "\n",
    "#Check for duplicated reactions added to the boundary_rxns dataset, IF NOT: update the google sheet file\n",
    "if added_exchange:\n",
    "    if not boundary_rxns['Reaction'].duplicated().any() and not boundary_rxns['Reaction Formula'].duplicated().any():\n",
    "        sheet.update_google_sheet(sheet_boundary, boundary_rxns)\n",
    "        print(\"BoundaryRxns Google Sheet updated.\")\n",
    "    else:\n",
    "        print('Duplicated values found in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be46e6d",
   "metadata": {},
   "source": [
    "### 2.4 Gapfill for blocked reactions\n",
    "Cobrapy has a gap filling implementation that is very similar to that of Reed et al. where we use a mixed-integer linear program to figure out the smallest number of reactions that need to be added for a user-defined collection of reactions, i.e. a universal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef010881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "from cobra.flux_analysis import gapfill\n",
    "\n",
    "#recon_3d = read_sbml_model(\"../Data/GPR_curation/Recon3D.xml\")\n",
    "#iCHO2291 = read_sbml_model(\"../Data/Reconciliation/models/iCHO2291.xml\")\n",
    "#universal = recon_3d.merge(iCHO2291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blocked_reaction in blocked:\n",
    "    model.objective = blocked_reaction\n",
    "    model.optimize().objective_value\n",
    "    try:\n",
    "        solution = gapfill(model, iCHO2291, demand_reactions=True)\n",
    "        print(blocked_reaction)\n",
    "        print(solution)\n",
    "    except Exception as e:\n",
    "        print(f'Gapfill failed for {blocked_reaction}: {str(e)}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25038fb8",
   "metadata": {},
   "source": [
    "### Test CHO - Recon GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16941789",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iCHO_recon3dfrom cobra.io import read_sbml_model\n",
    "# read_sbml_model(\".xml\")\n",
    "\n",
    "model_EX = [i for i, rxn in enumerate(model.reactions) if 'EX_' in rxn.id]\n",
    "model_SK = [i for i, rxn in enumerate(model.reactions) if 'SK_' in rxn.id]\n",
    "model_DM = [i for i, rxn in enumerate(model.reactions) if 'DM_' in rxn.id]\n",
    "for i in model_EX:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_SK:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_DM:\n",
    "    model.reactions[i].bounds = 0, 1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.objective = \"biomass_cho\" # \n",
    "sol1 = model.optimize()\n",
    "print(sol1.objective_value)\n",
    "\n",
    "model.objective = \"biomass_cho_prod\" # \n",
    "sol2 = model.optimize()\n",
    "print(sol2.objective_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Test model KOs ----- #####\n",
    "for reaction in model.reactions:\n",
    "    with model as model:\n",
    "        reaction.knock_out()\n",
    "        model.optimize()\n",
    "        print('%s blocked (bounds: %s), new growth rate %f' %\n",
    "              (reaction.id, str(reaction.bounds), model.objective.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e7b26",
   "metadata": {},
   "source": [
    "## 3. Generation of the Mass Flow Graph <a id='MFG'></a>\n",
    "Based on the publication **_Flux-dependent graphs for metabolic networks_** by _Beguerisse-Diaz et al. (2018)_ (https://www.nature.com/articles/s41540-018-0067-y). Here we use our model to build the **D Matrix** and plot the corresponding graph, then the **M Matrix** and plot the corresponding graph, and finally we generate the **PageRank** file with all the reactions in our reconstruction sorted by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055cf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.linalg import pinv\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from dash import Dash, html\n",
    "import dash_cytoscape as cyto\n",
    "from skimage import draw\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "import cobra\n",
    "from cobra.io import load_json_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Read Model ----- #####\n",
    "if 'model' not in locals():\n",
    "    model = load_json_model(\"iCHOv3_CHO_15032024.json\")\n",
    "    print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712b3cb",
   "metadata": {},
   "source": [
    "### 3.1 Generation of the \"D-Matrix\"\n",
    "The \"D-Matrix\" defines the weight of the edge between reaction nodes Ri and Rj as the probability that any metabolite chosen at random is produced by Ri (reaction i) and consumed by Rj (reaction j). Summing over all metabolites and normalizing, we obtain the edge weights of the adjacency matrix of the NFG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b953700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stoichiometric matrix, m=reactions, n=metabolites\n",
    "start = time.time()\n",
    "S = cobra.util.array.create_stoichiometric_matrix(model)\n",
    "n, m = S.shape\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S, n and m: {end - start} seconds\")\n",
    "\n",
    "# Create S2m matrix\n",
    "start = time.time()\n",
    "r = np.array([reaction.reversibility for reaction in model.reactions]) # m-dimensional reversibility vector with components rj = 1 if reaction Rj is reversible and rj = 0 if it is irreversible.\n",
    "Im = np.eye(m) # m Ã— m identity matrix\n",
    "S2m = np.hstack((S, - S * r)) # unfolded version of the stoichiometric matrix of the 2m forward and reverse reactions.\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S2m: {end - start} seconds\")\n",
    "\n",
    "# Create S2mplus and S2mminus matrices\n",
    "start = time.time()\n",
    "S2mplus = (np.abs(S2m) + S2m) / 2 # production stoichiometric matrix\n",
    "S2mminus = (np.abs(S2m) - S2m) / 2 # consumption stoichiometric matrix\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S2mplus and S2minus: {end - start} seconds\")\n",
    "\n",
    "# Calculate weights\n",
    "start = time.time()\n",
    "Wplus = np.diag(np.nan_to_num(1/ np.sum(S2mplus, axis = 1)))\n",
    "Wminus = np.diag(np.nan_to_num(1 / np.sum(S2mminus, axis = 1)))\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate Wplus and Wminus: {end - start} seconds\")\n",
    "\n",
    "# Calculate D matrix\n",
    "start = time.time()\n",
    "D = 1/n * (Wplus @ S2mplus).T @ (Wminus @ S2mminus)\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate the D-Matrix: {end - start} seconds\")\n",
    "'''\n",
    "# Remove unused reactions\n",
    "start = time.time()\n",
    "IDr = np.nonzero(np.sum(abs(D), axis=0) + np.sum(abs(D), axis=1) == 0)[0]\n",
    "#IDr = ( np.sum(abs(D), axis=0) + np.sum(abs(D), axis=1) ) == 0\n",
    "IDr = IDr[IDr > m]\n",
    "\n",
    "D = np.delete(D, IDr, axis=0)\n",
    "D = np.delete(D, IDr, axis=1)\n",
    "end = time.time()\n",
    "print(f\"Time taken to remove unused reactions from the D-Matrix: {end - start} seconds\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1fa64",
   "metadata": {},
   "source": [
    "### 3.2 Plotting the \"D-Matrix\", Normalized Flow Graph (NFG)\n",
    "The NFG is a weighted, directed graph with reactions as nodes, the edges represent supplier-consumer relationships between reactions, and weights given by the probability that a metabolite chosen at random from all reactions is produced/consumed by the source/target reaction (this discounts naturally the over-representation of pool metabolites). The edge indicates that metabolites are produced by the source reaction and consumed by the target reaction, thus accounting for metabolic directionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bbd8c-bc5e-44d0-93a1-cdd1c1efd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# NetworkX Processing\n",
    "# -------------------\n",
    "\n",
    "# Convert D matrix into a graph\n",
    "G = nx.from_numpy_array(D, create_using=nx.DiGraph)\n",
    "\n",
    "# Filter nodes based on degree\n",
    "min_degree = 2500\n",
    "degrees = dict(G.degree())\n",
    "nodes_to_keep = [node for node, degree in degrees.items() if degree >= min_degree]\n",
    "G_filtered = G.subgraph(nodes_to_keep)\n",
    "\n",
    "# Normalize weights for visualization\n",
    "weights = [G_filtered[u][v]['weight'] for u, v in G_filtered.edges()]\n",
    "min_weight, max_weight = min(weights), max(weights)\n",
    "normalized_weights = [(w - min_weight) / (max_weight - min_weight) * (7 - 0.001) + 0.001 for w in weights]\n",
    "\n",
    "# Mapping node indices to reaction names\n",
    "reaction_names = np.concatenate(([r.id for r in model.reactions], [r.id + '_r' for r in model.reactions]))\n",
    "node_labels_filtered = {i: reaction_name for i, reaction_name in enumerate(reaction_names) if i in nodes_to_keep}\n",
    "\n",
    "# Color mapping based on degree\n",
    "degrees_filtered = [val for (node, val) in G_filtered.degree()]\n",
    "min_deg, max_deg = min(degrees_filtered), max(degrees_filtered)\n",
    "normalized_degrees_filtered = [(d - min_deg) / (max_deg - min_deg) for d in degrees_filtered]\n",
    "cmap = plt.get_cmap('OrRd')\n",
    "node_colors_filtered = [cmap(deg) for deg in normalized_degrees_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbad548-2c74-49f5-8f58-bc39c3758821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------\n",
    "# Matplotlib Plot\n",
    "# -------------\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos_filtered = nx.random_layout(G_filtered)\n",
    "nx.draw(G_filtered, pos_filtered, width=normalized_weights, node_color=node_colors_filtered, edge_color='lightgray', node_size=300, ax=ax, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Network Flux Graph (NFG)', fontsize=25, y=0.95)\n",
    "\n",
    "# Adding labels and colorbar\n",
    "label_pos_filtered = {node: (pos[0] + 0.012, pos[1] + 0.012) for node, pos in pos_filtered.items()}\n",
    "nx.draw_networkx_labels(G_filtered, label_pos_filtered, labels=node_labels_filtered, font_size=10, ax=ax)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(normalized_degrees_filtered), vmax=max(normalized_degrees_filtered)))\n",
    "plt.colorbar(sm, ax=ax, orientation='horizontal', label='Node Degree')\n",
    "\n",
    "plt.savefig('../Networks/normalized_flow_graph.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e56d1-9319-4f5e-9c54-3d2b2a3c1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Dash Cytoscape Setup\n",
    "# ----------------\n",
    "\n",
    "# Preparing elements for Dash Cytoscape\n",
    "elements = [\n",
    "    {\n",
    "        'data': {'id': str(node), 'label': node_labels_filtered[node]},\n",
    "        'position': {'x': pos[0] * 1000, 'y': pos[1] * 1000}\n",
    "    }\n",
    "    for node, pos in pos_filtered.items()\n",
    "] + [\n",
    "    {'data': {'id': f'{u}-{v}', 'source': str(u), 'target': str(v)}}\n",
    "    for u, v in G_filtered.edges()\n",
    "]\n",
    "\n",
    "# Convert RGBA to hexadecimal for Dash\n",
    "hex_colors = [mcolors.to_hex(color) for color in node_colors_filtered]\n",
    "\n",
    "stylesheet = []\n",
    "\n",
    "# Node style with individual colors\n",
    "for i, node in enumerate(G_filtered.nodes()):\n",
    "    hex_color = hex_colors[i]  # Get the hexadecimal color for the node\n",
    "    node_style = {\n",
    "        'selector': f'[id = \"{str(node)}\"]',\n",
    "        'style': {\n",
    "            'background-color': hex_color,\n",
    "            'label': 'data(label)',\n",
    "            'color': 'black',  # Adjust label color as needed\n",
    "            'border-color': 'black',\n",
    "            'border-width': 1,\n",
    "        }\n",
    "    }\n",
    "    stylesheet.append(node_style)\n",
    "\n",
    "# Edge styles remain the same\n",
    "stylesheet += [\n",
    "    {\n",
    "        'selector': 'edge',\n",
    "        'style': {\n",
    "            'width': 1,\n",
    "            'line-color': '#888'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Dash app initialization and layout setup\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.P(\"Interactive Metabolic Network:\"),\n",
    "    cyto.Cytoscape(id='network', elements=elements, style={'width': '1000px', 'height': '1000px'}, layout={'name': 'preset'}, stylesheet=stylesheet)\n",
    "])\n",
    "\n",
    "# Running the Dash app\n",
    "if __name__ == '__main__':\n",
    "    port = 8051  # Ensure this port is free or adjust as needed\n",
    "    url = f'http://127.0.0.1:{port}/'\n",
    "    webbrowser.open_new_tab(url)\n",
    "    app.run_server(debug=True, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769e605",
   "metadata": {},
   "source": [
    "### 3.3 Generation of the \"FluxOpenValue\" matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e70e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the bounds for boundary reactions\n",
    "model_EX = [i for i, rxn in enumerate(model.reactions) if 'EX_' in rxn.id]\n",
    "model_SK = [i for i, rxn in enumerate(model.reactions) if 'SK_' in rxn.id]\n",
    "model_DM = [i for i, rxn in enumerate(model.reactions) if 'DM_' in rxn.id]\n",
    "for i in model_EX:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_SK:\n",
    "    model.reactions[i].bounds = -1000, 1000\n",
    "\n",
    "for i in model_DM:\n",
    "    model.reactions[i].bounds = 0, 1000\n",
    "\n",
    "# Perform pFBA for Biomass on Non-Producing and Producing Cell Lines\n",
    "model.solver = 'gurobi'\n",
    "objectives = ['biomass_cho', 'biomass_cho_prod']\n",
    "\n",
    "fluxes_list = []\n",
    "for objective in objectives:\n",
    "    model.objective = objective\n",
    "    pfba_solution = cobra.flux_analysis.pfba(model)\n",
    "    fluxes = np.array(pfba_solution.fluxes)\n",
    "    fluxes_list.append(fluxes)\n",
    "    \n",
    "# Stack arrays horizontally\n",
    "FluxOpenValue = np.column_stack(fluxes_list)\n",
    "\n",
    "FluxOpenValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of nonzero values in each row\n",
    "nonzero_counts = np.count_nonzero(FluxOpenValue, axis=1)\n",
    "\n",
    "# Count the number of rows that contain only zeros\n",
    "num_all_zero_rows = np.count_nonzero(nonzero_counts == 0)\n",
    "\n",
    "# Count the number of rows that contain some nonzero value\n",
    "num_some_nonzero_rows = np.count_nonzero(nonzero_counts != 0)\n",
    "\n",
    "print(\"Number of Rxns with no flux:\", num_all_zero_rows)\n",
    "print(\"Number of Rxns with any flux:\", num_some_nonzero_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5e6ee",
   "metadata": {},
   "source": [
    "### 3.4 Generation of the \"Mass Flow Graph (MFG) Matrix\"\n",
    "The MFG is a directed, environment-dependent, graph with weights computed from Flux Balance Analysis (FBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the MFG for each pFBA analysis\n",
    "M_list = []\n",
    "for i in range(len(objectives)):\n",
    "    start = time.time()\n",
    "    v1 = FluxOpenValue[:, i].T\n",
    "\n",
    "    # unfolding the flux vector\n",
    "    # creation of vplus and vminus\n",
    "    vplus = (np.abs(v1) + v1) / 2\n",
    "    vminus = (np.abs(v1) - v1) / 2\n",
    "\n",
    "    # creation of v2m\n",
    "    v2m = np.concatenate((vplus, vminus))\n",
    "\n",
    "    # creation of J_v\n",
    "    J_v = S2mplus @ v2m.reshape(-1)\n",
    "\n",
    "    # calculation of the MFG\n",
    "    M = (S2mplus * v2m).T @ np.diag(np.nan_to_num(1/J_v)) @ (S2mminus * v2m)\n",
    "    \n",
    "    # Dynamically create a variable named M_<objective>\n",
    "    objective_name = objectives[i]\n",
    "    globals()[f'M_Matrix_{objective_name}'] = M\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Time taken to go through iteration {i+1}: {end - start} seconds\")\n",
    "    \n",
    "'''\n",
    "# Post-processing of PageRank\n",
    "df = pd.DataFrame(PageRank)\n",
    "PageRank = df.values\n",
    "PageRank = np.array(PageRank).T\n",
    "PageRankRxns = PageRank[:m, :]\n",
    "PageRankRxns_back = PageRank[m:, :]\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(PageRankRxns.shape[1]):\n",
    "        if PageRankRxns_back[i, j] > PageRankRxns[i, j]:\n",
    "            PageRankRxns[i, j] = PageRankRxns_back[i, j]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f87a23",
   "metadata": {},
   "source": [
    "### 3.5 Plotting the \"MFG Matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5e310-994f-4f4d-8403-34644c84465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# NetworkX Processing\n",
    "# -------------------\n",
    "\n",
    "# Convert M matrix into a graph\n",
    "G = nx.from_numpy_array(M_Matrix_biomass_cho, create_using=nx.DiGraph)\n",
    "\n",
    "# Filter nodes based on degree and specific reactions to keep\n",
    "min_degree = 1\n",
    "degrees = {node: val for (node, val) in G.degree()}\n",
    "nodes_to_keep = [node for node, degree in degrees.items() if degree >= min_degree]\n",
    "\n",
    "# Additional logic to ensure specific reactions are always included\n",
    "reactions_to_keep = [\"LipidSyn\", \"DNAsyn\", \"RNAsyn\", \"PROTsyn\", \"biomass_cho\"]\n",
    "reaction_names = np.concatenate(([reaction.id for reaction in model.reactions], \n",
    "                                 [reaction.id + '_r' for reaction in model.reactions]))\n",
    "indices_to_keep = [i for i, reaction_name in enumerate(reaction_names) if reaction_name in reactions_to_keep]\n",
    "nodes_to_keep = list(set(nodes_to_keep).union(set(indices_to_keep)))\n",
    "\n",
    "# Mapping node indices to reaction names for labeling\n",
    "node_labels_filtered = {i: reaction_name for i, reaction_name in enumerate(reaction_names) if i in nodes_to_keep}\n",
    "\n",
    "# Creating a subgraph and relabeling nodes\n",
    "G_filtered = G.subgraph(nodes_to_keep)\n",
    "G_filtered = nx.relabel_nodes(G_filtered, node_labels_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37c908-5d06-42d7-9799-1529df083f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Matplotlib Plot\n",
    "# -----------------\n",
    "\n",
    "# Normalize weights and degrees for edge width and node color\n",
    "weights = [G_filtered[u][v]['weight'] for u, v in G_filtered.edges()]\n",
    "normalized_weights = [(w - min(weights)) / (max(weights) - min(weights)) * (7 - 0.001) + 0.001 for w in weights]\n",
    "degrees_filtered = [val for (node, val) in G_filtered.degree()]\n",
    "normalized_degrees_filtered = [(d - min(degrees_filtered)) / (max(degrees_filtered) - min(degrees_filtered)) for d in degrees_filtered]\n",
    "cmap = plt.get_cmap('OrRd')\n",
    "node_colors_filtered = [cmap(deg) for deg in normalized_degrees_filtered]\n",
    "\n",
    "# Plotting with Matplotlib\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos_filtered = nx.random_layout(G_filtered)\n",
    "nx.draw(G_filtered, pos_filtered, width=normalized_weights, node_color=node_colors_filtered, edge_color='lightgray', \n",
    "        node_size=300, ax=ax, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Mass Flow Graph (MFG)', fontsize=20, y=0.95)\n",
    "\n",
    "# Adjusting labels and adding colorbar\n",
    "offset = 0.02  # Adjust this value to move the labels up\n",
    "pos_labels = {node: (x, y + offset) for node, (x, y) in pos_filtered.items()}\n",
    "nx.draw_networkx_labels(G_filtered, pos_labels, ax=ax, font_size=10)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(normalized_degrees_filtered), vmax=max(normalized_degrees_filtered)))\n",
    "plt.colorbar(sm, ax=ax, orientation='horizontal', label='Node Degree')\n",
    "plt.savefig('../Networks/mass_flow_graph.png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b182c-2f99-4ded-86bc-c9f7e25dbb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Dash Cytoscape Setup\n",
    "# ----------------\n",
    "\n",
    "# Function to map node degrees to color values\n",
    "def degree_to_color(degree, cmap=plt.get_cmap('OrRd')):\n",
    "    max_degree = max(dict(G_filtered.degree()).values())\n",
    "    min_degree = min(dict(G_filtered.degree()).values())\n",
    "    return cmap((degree - min_degree) / (max_degree - min_degree))\n",
    "\n",
    "# Function to adjust edge width based on its weight\n",
    "def adjust_weight(weight):\n",
    "    min_weight = min([G_filtered[u][v]['weight'] for u, v in G_filtered.edges()])\n",
    "    max_weight = max([G_filtered[u][v]['weight'] for u, v in G_filtered.edges()])\n",
    "    min_edge_width = 0.5  # Adjusted for clearer visibility\n",
    "    max_edge_width = 10\n",
    "    normalized_weight = (weight - min_weight) / (max_weight - min_weight)\n",
    "    return normalized_weight * (max_edge_width - min_edge_width) + min_edge_width\n",
    "\n",
    "# Generate elements for nodes and edges\n",
    "elements = []\n",
    "stylesheet = [\n",
    "    {'selector': 'edge', 'style': {'line-color': '#CCCCCC'}},\n",
    "]\n",
    "\n",
    "for node, data in G_filtered.nodes(data=True):\n",
    "    degree = G_filtered.degree(node)\n",
    "    color_hex = matplotlib.colors.to_hex(degree_to_color(degree))\n",
    "    elements.append({'data': {'id': str(node), 'label': str(node)}})\n",
    "    stylesheet.append({\n",
    "        'selector': f'#{node}',\n",
    "        'style': {\n",
    "            'background-color': color_hex,\n",
    "            'label': 'data(label)',\n",
    "            'color': 'black',\n",
    "            'border-color': 'black',\n",
    "            'border-width': 1\n",
    "        }\n",
    "    })\n",
    "\n",
    "for source, target, data in G_filtered.edges(data=True):\n",
    "    adjusted_weight = adjust_weight(data['weight'])\n",
    "    edge_id = f'{source}-{target}'\n",
    "    elements.append({\n",
    "        'data': {'id': edge_id, 'source': str(source), 'target': str(target)}\n",
    "    })\n",
    "    stylesheet.append({\n",
    "        'selector': f'#{edge_id}',\n",
    "        'style': {'width': adjusted_weight}\n",
    "    })\n",
    "\n",
    "# Setup Dash app\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.P(\"Interactive Network Visualization:\"),\n",
    "    cyto.Cytoscape(\n",
    "        id='network',\n",
    "        elements=elements,\n",
    "        layout={'name': 'random'},\n",
    "        style={'width': '800px', 'height': '800px'},\n",
    "        stylesheet=stylesheet\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baae9cf",
   "metadata": {},
   "source": [
    "### 3.6 Word Cloud Plot for Metabolites Frecuencies in Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store PageRank\n",
    "PageRank = []\n",
    "G = nx.from_numpy_array(M_Matrix, create_using=nx.DiGraph)\n",
    "pr = nx.pagerank(G)\n",
    "PageRank.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97176f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing of PageRank\n",
    "S = cobra.util.array.create_stoichiometric_matrix(model)\n",
    "n, m = S.shape\n",
    "df = pd.DataFrame(PageRank)\n",
    "PageRank = df.values\n",
    "PageRank = np.array(PageRank).T\n",
    "PageRankRxns = PageRank[:m, :]\n",
    "PageRankRxns_back = PageRank[m:, :]\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(PageRankRxns.shape[1]):\n",
    "        if PageRankRxns_back[i, j] > PageRankRxns[i, j]:\n",
    "            PageRankRxns[i, j] = PageRankRxns_back[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_sums = PageRankRxns.sum(axis=1)\n",
    "df = pd.DataFrame()\n",
    "for i,n in enumerate(objectives):\n",
    "    sorted_indices = np.argsort(PageRankRxns[:,i])\n",
    "    rxns_list = []\n",
    "    values_list = []\n",
    "    for s in sorted_indices[::-1]:\n",
    "        rxns_list.append(model.reactions[s].id)\n",
    "        values_list.append(PageRankRxns[s,i])\n",
    "    \n",
    "    df[n] = pd.Series(rxns_list)\n",
    "    df[f'values_{n}'] = pd.Series(values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in df.iterrows():\n",
    "    print(v['biomass_producing'],v['values_biomass_producing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets_list = []\n",
    "for rxn in df['biomass'][df['values_biomass'] > 0.0000412]:\n",
    "    r = model.reactions.get_by_id(rxn)\n",
    "    mets = r.metabolites\n",
    "    for met in mets:\n",
    "        mets_list.append(met.id)\n",
    "        \n",
    "for rxn in df['biomass_producing'][df['values_biomass_producing'] > 0.0000412]:\n",
    "    r = model.reactions.get_by_id(rxn)\n",
    "    mets = r.metabolites\n",
    "    for met in mets:\n",
    "        mets_list.append(met.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequencies of each metabolite\n",
    "mets_freq = Counter(mets_list)\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('h2o_')} #eliminate water\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('h2o2_')} #eliminate peroxide\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('co2_')} #eliminate carbon dioxide\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nh4_')} #eliminate amonium\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('h_')} #eliminate protons\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('atp_')} #eliminate atp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('adp_')} #eliminate adp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('amp_')} #eliminate amp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nad_')} #eliminate nad\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nadh_')} #eliminate nadh\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nadp_')} #eliminate nadp\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('nadph_')} #eliminate nadph\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('na1_')} #eliminate Sodium\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('coa_')} #eliminate CoA\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('accoa_')} #eliminate Acetyl-CoA\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('pi_')} #eliminate phosphate\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('ppi_')} #eliminate diphosphate\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('fadh2_')} #eliminate FADH\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('fad_')} #eliminate FAD\n",
    "mets_freq = {k: v for k, v in mets_freq.items() if not k.startswith('o2_')} #eliminate Oxigen\n",
    "\n",
    "# Create a circular mask\n",
    "radius = 500  # you can change to the size you need\n",
    "circle_img = np.zeros((2*radius, 2*radius), np.uint8)\n",
    "rr, cc = draw.disk((radius, radius), radius)\n",
    "circle_img[rr, cc] = 1\n",
    "\n",
    "# Create the word cloud\n",
    "wordcloud = WordCloud(width = 1000, height = 500, mask=circle_img, background_color=\"rgba(255, 255, 255, 0)\", mode=\"RGBA\").generate_from_frequencies(mets_freq)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.savefig('wordcloud.png', bbox_inches='tight', transparent=True, pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6cbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for met in mets_freq:\n",
    "    print(met,mets_freq[met])\n",
    "    counter+=1\n",
    "    \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6990b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the metabolites and their frequencies in a .txt file\n",
    "\n",
    "with open('metabolites.txt', 'w') as f:\n",
    "    for i, j in enumerate(mets_freq):\n",
    "        print(j,'Freq:',mets_freq[j], file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1aa9c0",
   "metadata": {},
   "source": [
    "## 4. Biomass Conecting Reactions <a id='biomass'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41456fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "from cobra.flux_analysis import pfba\n",
    "from cobra.io import load_json_model\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Read Model ----- #####\n",
    "if 'model' not in locals():\n",
    "    model = load_json_model(\"iCHOv3_CHO_20022024.json\")\n",
    "    print('Model loaded')\n",
    "else:\n",
    "    print('Model already generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.solver = 'gurobi'\n",
    "sol = pfba(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10613258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogpfba = sol.fluxes\n",
    "duplicate_series = ogpfba.copy()\n",
    "duplicate_series.index = [f\"{idx}_r\" for idx in duplicate_series.index]\n",
    "finalpfba = pd.concat([ogpfba, duplicate_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01064b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_with_flux = []\n",
    "rxn_with_flux = []\n",
    "counter=0\n",
    "for i,(r,f) in enumerate(ogpfba.items()):\n",
    "    if f != 0:\n",
    "        print(i,counter,r,f)\n",
    "        indexes_with_flux.append(i)\n",
    "        rxn_with_flux.append(r)\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for i,flux in enumerate(sol.fluxes):\n",
    "    if flux != 0:\n",
    "        r = sol.fluxes.index[i]\n",
    "        #print(i,r,flux)\n",
    "        counter+=1\n",
    "    \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_reactions = [reaction_id for reaction_id, flux in sol.fluxes.items() if flux != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46027ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a628c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = cobra.util.array.create_stoichiometric_matrix(model, array_type='DataFrame')\n",
    "S_filtered = S[active_reactions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Extract stoichiometric matrix, m=reactions, n=metabolites\n",
    "start = time.time()\n",
    "n, m = S_filtered.shape\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S, n and m: {end - start} seconds\")\n",
    "\n",
    "# Create S2m matrix\n",
    "start = time.time()\n",
    "r = np.array([reaction.reversibility for reaction in model.reactions if reaction.id in S_filtered.columns]) # m-dimensional reversibility vector with components rj = 1 if reaction Rj is reversible and rj = 0 if it is irreversible.\n",
    "Im = np.eye(m) # m Ã— m identity matrix\n",
    "S2m = np.hstack((S_filtered, - S_filtered * r)) # unfolded version of the stoichiometric matrix of the 2m forward and reverse reactions.\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S2m: {end - start} seconds\")\n",
    "\n",
    "# Create S2mplus and S2mminus matrices\n",
    "start = time.time()\n",
    "S2mplus = (np.abs(S2m) + S2m) / 2 # production stoichiometric matrix\n",
    "S2mminus = (np.abs(S2m) - S2m) / 2 # consumption stoichiometric matrix\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate S2mplus and S2minus: {end - start} seconds\")\n",
    "\n",
    "# Calculate weights\n",
    "start = time.time()\n",
    "Wplus = np.diag(np.nan_to_num(1/ np.sum(S2mplus, axis = 1)))\n",
    "Wminus = np.diag(np.nan_to_num(1 / np.sum(S2mminus, axis = 1)))\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate Wplus and Wminus: {end - start} seconds\")\n",
    "\n",
    "# Calculate D matrix\n",
    "start = time.time()\n",
    "D = 1/n * (Wplus @ S2mplus).T @ (Wminus @ S2mminus)\n",
    "end = time.time()\n",
    "print(f\"Time taken to generate the D-Matrix: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725aff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert D matrix into a graph\n",
    "G = nx.from_numpy_array(D, create_using=nx.DiGraph)\n",
    "\n",
    "# Create a subgraph containing only the nodes with a degree greater than min_degree\n",
    "degrees = {node: val for (node, val) in G.degree()} # Calculate the degrees of each node\n",
    "min_degree = 10 # Define the minimum degree for a node to be kept.\n",
    "nodes_to_keep = [node for node, degree in degrees.items() if degree >= min_degree]\n",
    "G_filtered = G.subgraph(nodes_to_keep)\n",
    "\n",
    "# Normalize weights for the edges of the nodes\n",
    "weights = [G_filtered[u][v]['weight'] for u,v in G_filtered.edges()]\n",
    "normalized_weights = [(w - min(weights)) / (max(weights) - min(weights)) * (7 - 0.001) + 0.001 for w in weights]\n",
    "\n",
    "# Create a dictionary mapping node indices to reaction names for labeling\n",
    "reaction_names = np.concatenate(([reaction for reaction in S_filtered.columns], \n",
    "                                 [reaction + '_r' for reaction in S_filtered.columns]))\n",
    "node_labels_filtered = {i: reaction_name for i, reaction_name in enumerate(reaction_names) if i in nodes_to_keep}\n",
    "\n",
    "# Normalize the degrees for color mapping\n",
    "degrees_filtered = [val for (node, val) in G_filtered.degree()]\n",
    "normalized_degrees_filtered = [(d - min(degrees_filtered)) / (max(degrees_filtered) - min(degrees_filtered)) for d in degrees_filtered]\n",
    "\n",
    "# Use a colormap to map normalized degrees to colors\n",
    "cmap = plt.get_cmap('OrRd')  # Choose a colormap here\n",
    "node_colors_filtered = [cmap(deg) for deg in normalized_degrees_filtered]\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "pos_filtered = nx.random_layout(G_filtered)\n",
    "\n",
    "# Find the node number for \"biomass_cho\"\n",
    "biomass_cho_node_number = {v: k for k, v in node_labels_filtered.items()}.get('biomass_cho', None)\n",
    "\n",
    "if biomass_cho_node_number is not None:\n",
    "    # Position \"biomass_cho\" at the lower right corner\n",
    "    pos_filtered[biomass_cho_node_number] = (1, 0)\n",
    "    \n",
    "    # Identify predecessors and filter based on edge weight\n",
    "    connected_nodes = G_filtered.predecessors(biomass_cho_node_number)\n",
    "    weight_threshold = 0.0001  # Define your weight threshold here\n",
    "    filtered_predecessors = [node for node in connected_nodes if G_filtered[node][biomass_cho_node_number]['weight'] >= weight_threshold]\n",
    "    \n",
    "    # Calculate positions for filtered_predecessors\n",
    "    radius = 0.05  # Distance from \"biomass_cho\" node, adjust as needed\n",
    "    angle_increment = math.pi / len(filtered_predecessors)  # Adjust for semi-circle or full circle\n",
    "\n",
    "    # Position filtered_predecessors in a semi-circle starting from upper left\n",
    "    for i, node in enumerate(filtered_predecessors):\n",
    "        angle = math.pi + (i * angle_increment)  # Adjust starting angle for upper left\n",
    "        # Convert polar to Cartesian coordinates\n",
    "        x = pos_filtered[biomass_cho_node_number][0] + radius * math.cos(angle)\n",
    "        y = pos_filtered[biomass_cho_node_number][1] + radius * math.sin(angle)\n",
    "        pos_filtered[node] = (x, y)\n",
    "        \n",
    "    # Now handle second-level predecessors for each node\n",
    "    second_level_predecessors = list(G_filtered.predecessors(node))\n",
    "    angle_increment = math.pi / max(len(second_level_predecessors), 1)  # Avoid division by zero\n",
    "\n",
    "    for j, second_node in enumerate(second_level_predecessors):\n",
    "        # Calculate angle for second-level predecessors\n",
    "        angle = math.pi + (j * angle_increment)  # Adjust starting angle for upper left\n",
    "        # Convert polar to Cartesian coordinates for second-level predecessors\n",
    "        x = pos_filtered[node][0] + second_level_radius * math.cos(angle)\n",
    "        y = pos_filtered[node][1] + second_level_radius * math.sin(angle)\n",
    "        pos_filtered[second_node] = (x, y)\n",
    "\n",
    "\n",
    "nx.draw(G_filtered, pos_filtered, width=normalized_weights, node_color=node_colors_filtered, edge_color='lightgray', node_size=300, ax=ax, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Network Flux Graph (NFG)', fontsize=25, y=0.95)\n",
    "\n",
    "# Labels\n",
    "label_pos_filtered = {node: (x + 0.012, y + 0.012) for node, (x, y) in pos_filtered.items()}\n",
    "nx.draw_networkx_labels(G_filtered, label_pos_filtered, labels=node_labels_filtered, font_size=10, ax=ax)\n",
    "\n",
    "plt.savefig('../Networks/normalized_flow_graph.png', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_predecessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming G_filtered is your directed graph and \"biomass_cho\" is the node of interest\n",
    "biomass_cho_index = {label: index for index, label in node_labels_filtered.items()}.get('biomass_cho')\n",
    "\n",
    "# Set a threshold for filtering based on weight\n",
    "weight_threshold = 0.0001  # Example threshold, adjust as needed\n",
    "\n",
    "# Initialize a list to hold labels of filtered predecessors\n",
    "filtered_predecessor_labels = []\n",
    "\n",
    "if biomass_cho_index is not None:\n",
    "    # Get all predecessors of \"biomass_cho\"\n",
    "    predecessors = list(G_filtered.predecessors(biomass_cho_index))\n",
    "    \n",
    "    # Filter predecessors based on edge weight\n",
    "    for pred in predecessors:\n",
    "        weight = G_filtered[pred][biomass_cho_index]['weight']\n",
    "        if weight >= weight_threshold:\n",
    "            # If the weight meets the threshold, add the predecessor's label to the list\n",
    "            if pred in node_labels_filtered:\n",
    "                filtered_predecessor_labels.append(node_labels_filtered[pred])\n",
    "\n",
    "# Now, filtered_predecessor_labels contains the labels of the filtered predecessors\n",
    "print(\"Filtered predecessor labels:\", filtered_predecessor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_predecessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e553d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
