{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Tests\n",
    "\n",
    "This notebook is intended for running different kinds of analyses that would validate our reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "[1. Subsystem Overview and Analysis](#subsystems) <br>\n",
    "[2. Context-specific Model Generation](#bound_sim) <br>\n",
    "[3. ATP and Gluc Generation loops Identification and Removal](#loops) <br>\n",
    "[4. tSNE Comparison of Models](#tSNE) <br>\n",
    "[5. pFBA](#pFBA) <br>\n",
    "[6. Flux Sampling Analysis](#fsa) <br>\n",
    "[7. Pathway Enrichment Analysis](#pea) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subsystem Overview and Analysis <a id='subsystems'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- Read iCHO3K reaction files ----- #####\n",
    "\n",
    "#Path to iCHO3K Excell\n",
    "FILE_PATH = '../iCHO3K/Dataset/iCHO3K.xlsx'\n",
    "\n",
    "\n",
    "# Sheets\n",
    "sheet_subsystems = 'Summary Systems'\n",
    "sheet_rxns = 'Rxns'\n",
    "\n",
    "# Read into DataFrames\n",
    "subsystems      = pd.read_excel(FILE_PATH, sheet_name=sheet_subsystems)\n",
    "reactions_rec             = pd.read_excel(FILE_PATH, sheet_name=sheet_rxns)\n",
    "\n",
    "# Remove the total count\n",
    "subsystems = subsystems.iloc[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Sunburst Plot\n",
    "\n",
    "fig = px.sunburst(subsystems, path=['System', 'Subsystems', 'Kegg Pathway'],\n",
    "                  color='System')\n",
    "\n",
    "# Convert to a Graph Objects figure\n",
    "fig_go = go.Figure(fig)\n",
    "\n",
    "# Update layout\n",
    "fig_go.update_layout(width=1200, height=1000)\n",
    "\n",
    "# Define font sizes\n",
    "font_size_last_layer = 10\n",
    "font_size_previous_layer = 15\n",
    "default_font_size = 12\n",
    "\n",
    "# Create a list to store font sizes\n",
    "font_sizes = []\n",
    "\n",
    "# Update font size for each level\n",
    "for trace in fig_go.data:\n",
    "    if isinstance(trace, go.Sunburst):\n",
    "        for id in trace.ids:\n",
    "            level = id.count(\"/\")  # Determine level by the number of slashes in the id\n",
    "            if level == 2:  # Last layer (Kegg Pathway)\n",
    "                font_sizes.append(font_size_last_layer)\n",
    "            elif level == 1:  # Previous layer (Subsystems)\n",
    "                font_sizes.append(font_size_previous_layer)\n",
    "            else:\n",
    "                font_sizes.append(default_font_size)  # Default size for other layers\n",
    "\n",
    "# Apply the font sizes to the figure\n",
    "fig_go.update_traces(insidetextfont=dict(size=font_sizes))\n",
    "\n",
    "# Save the figure\n",
    "fig_go.write_html(\"../Analyses/subsystem_overview/sunburst_subsystems.html\")  # Save as interactive HTML file\n",
    "fig_go.write_image(\"../Analyses/subsystem_overview/sunburst_subsystems.png\", width=1200, height=1000, scale=2)  # Increase resolution by setting scale parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a susbsystem dictionary\n",
    "\n",
    "subsystems = subsystems[['System', 'Subsystems']]\n",
    "subsystems_dict = subsystems.groupby('System')['Subsystems'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxotrophies\n",
    "Check for auxotrophies in iCHO3K model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-03-06\n"
     ]
    }
   ],
   "source": [
    "from cobra.io import load_json_model\n",
    "\n",
    "iCHO_path = \"../iCHO3K/Model/iCHO3K_unblocked.json\"\n",
    "iCHO = load_json_model(iCHO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arginine 0.0\n",
      "asparagine 0.0\n",
      "cysteine 0.0\n",
      "histidine 0.0\n",
      "isoleucine 0.0\n",
      "leucine 0.0\n",
      "lysine 0.0\n",
      "methionine 0.0\n",
      "phenylalanine 0.0\n",
      "proline 0.0\n",
      "threonine 0.0\n",
      "tryptophan 0.0\n",
      "valine 0.0\n"
     ]
    }
   ],
   "source": [
    "amino_acids = {\n",
    "    \"arginine\": [\"EX_arg_L_e\", \"EX_arg_D_e\"],\n",
    "    \"asparagine\": [\"EX_asn_L_e\", \"EX_asn_D_e\"],\n",
    "    \"cysteine\": [\"EX_cys_L_e\", \"EX_cys_D_e\", \"EX_Lcystin_e\"],\n",
    "    \"histidine\": [\"EX_his_L_e\", \"EX_his_D_e\"],\n",
    "    \"isoleucine\": [\"EX_ile_L_e\", \"EX_ile_D_e\"],\n",
    "    \"leucine\": [\"EX_leu_L_e\", \"EX_leu_D_e\"],\n",
    "    \"lysine\": [\"EX_lys_L_e\", \"EX_lys_D_e\"],\n",
    "    \"methionine\": [\"EX_met_L_e\", \"EX_met_D_e\"],\n",
    "    \"phenylalanine\": [\"EX_phe_L_e\", \"EX_phe_D_e\"],\n",
    "    \"proline\": [\"EX_pro_L_e\", \"EX_pro_D_e\"],\n",
    "    \"threonine\": [\"EX_thr_L_e\", \"EX_thr_D_e\"],\n",
    "    \"tryptophan\": [\"EX_trp_L_e\", \"EX_trp_D_e\"],\n",
    "    \"valine\": [\"EX_val_L_e\", \"EX_val_D_e\"]\n",
    "}\n",
    "for amino_acid in amino_acids:\n",
    "    # ----- Setup initial bounds -----\n",
    "    for exchange_reaction in iCHO.exchanges:\n",
    "        exchange_reaction.bounds = -10, 10\n",
    "\n",
    "    # Arginine\n",
    "    iCHO.reactions.get_by_id('GAUGE-R10107').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('GLYAMDTRc').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('GAUGE-R10107').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('EX_valarggly_e').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('ARGSL').bounds = 0, 0\n",
    "\n",
    "    # Asparigine\n",
    "    iCHO.reactions.get_by_id('ASNS1').bounds = 0, 0\n",
    "\n",
    "    # Cysteine\n",
    "    iCHO.reactions.get_by_id('r0129').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('EX_cgly_e').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('AMPTASECG').bounds = -10, 0\n",
    "    iCHO.reactions.get_by_id('AMPTASECGe').bounds = -10, 0\n",
    "    iCHO.reactions.get_by_id('CYSTGL').bounds = -10, 0\n",
    "    iCHO.reactions.get_by_id('EX_HC00250_e').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('EX_sfcys_e').bounds = 0, 0\n",
    "\n",
    "    # Histidine\n",
    "    iCHO.reactions.get_by_id('VALTRPVALr').bounds = 0,0 \n",
    "\n",
    "    # Isoleucine - DONE\n",
    "    iCHO.reactions.get_by_id('EX_CE2916_e').bounds = 0,0 \n",
    "    iCHO.reactions.get_by_id('EX_CE2915_e').bounds = 0,0 \n",
    "    iCHO.reactions.get_by_id('ILETA').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('ILETAm').bounds = 0, 10\n",
    "\n",
    "    # Leucine\n",
    "    iCHO.reactions.get_by_id('LEULEULAPc').bounds = 0,0 \n",
    "    iCHO.reactions.get_by_id('EX_leugly_e').bounds = 0,0 \n",
    "    iCHO.reactions.get_by_id('EX_glyleu_e').bounds = 0,0 \n",
    "    iCHO.reactions.get_by_id('LEUTA').bounds = 0, 10 \n",
    "    iCHO.reactions.get_by_id('LEUTAm').bounds = 0, 10 \n",
    "    iCHO.reactions.get_by_id('EX_CE5797_e').bounds = 0, 0\n",
    "\n",
    "    # Lysine\n",
    "    iCHO.reactions.get_by_id('EX_biocyt_e').bounds = 0,0 \n",
    "\n",
    "    # Methionine\n",
    "    iCHO.reactions.get_by_id('METS').bounds = -10, 0 \n",
    "    iCHO.reactions.get_by_id('BHMT').bounds = -10, 0  \n",
    "    iCHO.reactions.get_by_id('UNK2').bounds = -10, 0 \n",
    "    iCHO.reactions.get_by_id('UNK3').bounds = -10, 0 \n",
    "    #iCHO.reactions.get_by_id('GAUGE-R06895').bounds = 0, 0 # Curated by MR with 1 score / Erased from the reconstruction\n",
    "\n",
    "    # Phenylalanine\n",
    "    iCHO.reactions.get_by_id('EX_CE5786_e').bounds = 0, 0 \n",
    "    iCHO.reactions.get_by_id('EX_pheleu_e').bounds = 0, 0 \n",
    "    iCHO.reactions.get_by_id('EX_glyphe_e').bounds = 0, 0 \n",
    "    iCHO.reactions.get_by_id('EX_CE2917_e').bounds = 0, 0 \n",
    "    iCHO.reactions.get_by_id('EX_CE5786_e').bounds = 0, 0 \n",
    "    iCHO.reactions.get_by_id('EX_CE5789_e').bounds = 0, 0 \n",
    "    iCHO.reactions.get_by_id('EX_phpyr_e').bounds = 0, 0 \n",
    "\n",
    "    # Proline\n",
    "    iCHO.reactions.get_by_id('EX_glypro_e').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('EX_progly_e').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('P5CR').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('P5CRxm').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('P5CRx').bounds = 0, 0\n",
    "    iCHO.reactions.get_by_id('P5CRm').bounds = 0, 0\n",
    "\n",
    "    # Threonine\n",
    "    #iCHO.reactions.get_by_id('THRS').bounds = 0,0  # Erased from the reconstruction\n",
    "\n",
    "\n",
    "    # Tryptophan\n",
    "    iCHO.reactions.get_by_id('NBAHH_ir').bounds = 0,0  # Histidine hydrolase\n",
    "\n",
    "    # Valine\n",
    "    iCHO.reactions.get_by_id('EX_valarggly_e').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('EX_vallystyr_e').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('VALTA').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('VALTAm').bounds = 0, 10\n",
    "    iCHO.reactions.get_by_id('EX_valval_e').bounds = 0, 10\n",
    "    # -----------------------------------------\n",
    "\n",
    "    for exchange_reaction in iCHO.exchanges:\n",
    "        if exchange_reaction.id in amino_acids[amino_acid]:\n",
    "            exchange_reaction.bounds = 0, 10\n",
    "            # print(amino_acid, exchange_reaction.id, amino_acids[amino_acid])\n",
    "    sol = iCHO.optimize()\n",
    "    print(amino_acid, sol.objective_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulation of reaction bounds for low confidence timepoint models <a id='bound_sim'></a>\n",
    "\n",
    "Here we use a matrix generated with rmf_CADRE to generate each context_specific model for each one of the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from time import process_time\n",
    "\n",
    "import pymCADRE\n",
    "from pymCADRE.rank import rank_reactions\n",
    "from pymCADRE.check import check_model_function\n",
    "from pymCADRE.prune import prune_model\n",
    "\n",
    "import cobra\n",
    "from cobra.io import load_json_model\n",
    "from cobra.exceptions import Infeasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generic model\n",
    "model = load_json_model('iCHO3K_unblocked.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uptake and secretion rate \"Intervals dict\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_wt_dict.pkl', 'rb') as file:\n",
    "    uptsec_wt = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrain the model by the growth rate and low variability reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = \"P0 to P2\"\n",
    "\n",
    "# Exclude 'exp_growth_rate' from reactions\n",
    "reaction_fluxes = {rxn: fluxes for rxn, fluxes in uptsec_wt.items() if rxn != 'exp_growth_rate'}\n",
    "\n",
    "# Compute variability for each reaction at the timepoint \"P0 to P2\"\n",
    "reaction_variability = {}\n",
    "for rxn_id, flux_dict in reaction_fluxes.items():\n",
    "    # Collect all flux values for the specific timepoint \"P0 to P2\" across replicates\n",
    "    flux_values = [v for (replicate, timepoint), v in flux_dict.items() if timepoint == time]\n",
    "    # Compute standard deviation\n",
    "    std_dev = np.std(flux_values)\n",
    "    reaction_variability[rxn_id] = std_dev\n",
    "\n",
    "# Now we have a dictionary of reaction IDs and their standard deviations at \"P0 to P2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the threshold for low variability (e.g., reactions with std_dev less than 25th percentile)\n",
    "variability_values = list(reaction_variability.values())\n",
    "threshold = np.percentile(variability_values, 25)\n",
    "\n",
    "# Select reactions with standard deviation below the threshold\n",
    "low_variability_reactions = [rxn_id for rxn_id, std_dev in reaction_variability.items() if std_dev <= threshold]\n",
    "\n",
    "# Reactions with high variability\n",
    "high_variability_reactions = [rxn_id for rxn_id in reaction_fluxes if rxn_id not in low_variability_reactions]\n",
    "\n",
    "print(f\"Selected {len(low_variability_reactions)} reactions with low variability.\")\n",
    "print(f\"Selected {len(high_variability_reactions)} reactions with high variability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the lower bound values according to the experimental growth rates\n",
    "intervals = {'P0 to P2': 'P2'}\n",
    "replicates = {'U1': 'Bio141', 'U2': 'Bio142', 'U3': 'Bio143'}\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# For storing simulated fluxes\n",
    "simulated_fluxes = {}\n",
    "\n",
    "# For storing modified reaction bounds\n",
    "uptake_secretion_wt_sim_p0p2 = {}\n",
    "\n",
    "for interval_key, interval_model_time in intervals.items():\n",
    "    for rep_key, rep_suffix in replicates.items():\n",
    "        with model as modified_model:\n",
    "            \n",
    "            # Keep boundaries open for essential metabolites\n",
    "            for rxn in modified_model.boundary:\n",
    "                if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                    rxn.bounds = (-1000, 1000)\n",
    "                elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                    rxn.bounds = (-0.001, 1000)\n",
    "                elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                    rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "            \n",
    "            print(f\"\\nCalculating for WT Condition: {rep_key}, {interval_key}\")\n",
    "            exp_gr = uptsec_wt['exp_growth_rate'][(rep_key, interval_key)]\n",
    "            print(f\"Experimental growth rate: {exp_gr}\")\n",
    "            \n",
    "            # Set constraints for reactions with low variability\n",
    "            constrained_reactions = []\n",
    "            unconstrained_reactions = []\n",
    "            for reaction in modified_model.reactions:\n",
    "                if reaction.id == 'EX_etoh_e':  # Model creates infeasible solutions when secreting etoh\n",
    "                    continue\n",
    "                if reaction.id == 'EX_bhb_e':  # Included manually based on exp data\n",
    "                    reaction.bounds = (0.009, 0.012)\n",
    "                elif reaction.id == 'EX_nh4_e':\n",
    "                    reaction.bounds = (0.38, 0.41)\n",
    "                elif reaction.id == 'EX_ac_e':\n",
    "                    reaction.bounds = (0.014, 0.023)\n",
    "                elif reaction.id == 'EX_ala_L_e':\n",
    "                    reaction.bounds = (0.165, 0.174)\n",
    "                elif reaction.id == 'EX_arg_L_e':\n",
    "                    reaction.bounds = (-0.03, -0.02)\n",
    "                elif reaction.id == 'EX_asn_L_e':\n",
    "                    reaction.bounds = (-0.288, -0.188)\n",
    "                elif reaction.id == 'EX_asp_L_e':\n",
    "                    reaction.bounds = (0.04, 0.05)\n",
    "                elif reaction.id == 'EX_2hb_e':\n",
    "                    reaction.bounds = (0.012, 0.0135)\n",
    "                elif reaction.id == 'EX_cys_L_e':\n",
    "                    reaction.bounds = (-0.016, -0.010)\n",
    "                elif reaction.id == 'EX_for_e':\n",
    "                    reaction.bounds = (0.099, 0.105)\n",
    "                elif reaction.id == 'EX_glc_e':\n",
    "                    reaction.bounds = (-0.98, -0.78)\n",
    "                elif reaction.id == 'EX_glu_L_e':\n",
    "                    reaction.bounds = (0.035, 0.042)\n",
    "                elif reaction.id == 'EX_gln_L_e':\n",
    "                    reaction.bounds = (-0.71, -0.059)\n",
    "                elif reaction.id == 'EX_gly_e':\n",
    "                    reaction.bounds = (0.030, 0.035)\n",
    "                elif reaction.id == 'EX_his_L_e':\n",
    "                    reaction.bounds = (-0.0124, -0.007)\n",
    "                elif reaction.id == 'EX_4hpro_e':\n",
    "                    reaction.bounds = (-0.012, -0.005)\n",
    "                elif reaction.id == 'EX_ile_L_e':\n",
    "                    reaction.bounds = (-0.025, -0.015)\n",
    "                elif reaction.id == 'EX_lac_L_e':\n",
    "                    reaction.bounds = (1.8, 1.93)\n",
    "                elif reaction.id == 'EX_leu_L_e':\n",
    "                    reaction.bounds = (-0.039, -0.025)\n",
    "                elif reaction.id == 'EX_lys_L_e':\n",
    "                    reaction.bounds = (-0.047, -0.019)\n",
    "                elif reaction.id == 'EX_met_L_e':\n",
    "                    reaction.bounds = (-0.015, -0.011)\n",
    "                elif reaction.id == 'EX_phe_L_e':\n",
    "                    reaction.bounds = (-0.025, -0.019)\n",
    "                elif reaction.id == 'EX_pro_L_e':\n",
    "                    reaction.bounds = (0.01, 0.14)\n",
    "                elif reaction.id == 'EX_5oxpro_e':\n",
    "                    reaction.bounds = (0.28, 0.33)\n",
    "                elif reaction.id == 'EX_pyr_e':\n",
    "                    reaction.bounds = (-0.088, -0.07)\n",
    "                elif reaction.id == 'EX_ser_L_e':\n",
    "                    reaction.bounds = (-0.14, -0.11)\n",
    "                elif reaction.id == 'EX_thr_L_e':\n",
    "                    reaction.bounds = (-0.026, -0.007)\n",
    "                elif reaction.id == 'EX_trp_L_e':\n",
    "                    reaction.bounds = (-0.012, -0.006)\n",
    "                elif reaction.id == 'EX_tyr_L_e':\n",
    "                    reaction.bounds = (0.008, 0.015)\n",
    "                elif reaction.id == 'EX_val_L_e':\n",
    "                    reaction.bounds = (-0.035, -0.02)\n",
    "                elif reaction.id == 'ATPM':  # Add ATP Maintenance Cost\n",
    "                    reaction.lower_bound = 6\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            \n",
    "            # Constrain biomass reaction to experimental growth rate\n",
    "            # Allowing a small deviation (e.g., ±10%) to account for model limitations\n",
    "            growth_lb = exp_gr * 0.9\n",
    "            growth_ub = exp_gr * 1.1\n",
    "            modified_model.reactions.get_by_id(objective).bounds = (growth_lb, growth_ub)\n",
    "            print(f\"Biomass reaction bounds set to: {growth_lb}, {growth_ub}\")\n",
    "\n",
    "            \n",
    "            \n",
    "            # Run pFBA\n",
    "            try:\n",
    "                pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                pfba_obj = pfba_solution.fluxes[objective]\n",
    "                print(f\"Solution status: {pfba_solution.status}\")\n",
    "                print(f\"Objective value (growth rate): {pfba_obj}\")\n",
    "\n",
    "                # Capture simulated uptake and secretion rates\n",
    "                simulated_flux = {}\n",
    "                for rxn in modified_model.reactions:\n",
    "                    if rxn.id in uptsec_wt.keys():\n",
    "                        uptake_secretion_wt_sim_p0p2[rxn.id] = {}\n",
    "                        uptake_secretion_wt_sim_p0p2[rxn.id][interval_key] = rxn.bounds\n",
    "                        \n",
    "                        flux = pfba_solution.fluxes.get(rxn.id, 0)\n",
    "                        simulated_flux[rxn.id] = flux\n",
    "                simulated_fluxes[(rep_key, interval_key)] = simulated_flux\n",
    "                uptake_secretion_wt_sim_p0p2['exp_growth_rate'] = {}\n",
    "                uptake_secretion_wt_sim_p0p2['exp_growth_rate'][interval_key] = (0.034,0.037)\n",
    "                print(\"Simulated uptake and secretion rates:\")\n",
    "                for rxn_id, flux in simulated_flux.items():\n",
    "                    print(f\"{rxn_id}: {flux}\")\n",
    "\n",
    "                uptake_secretion_wt_sim_p0p2\n",
    "\n",
    "            except Infeasible:\n",
    "                print(f\"Infeasible solution for replicate {rep_key} interval {interval_key}\")\n",
    "                # Optionally, you can implement a procedure to relax constraints or identify problematic reactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_wt_sim_p0p2.pkl', 'wb') as file:\n",
    "    pickle.dump(uptake_secretion_wt_sim_p0p2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ATP and Gluc Generation loops Identification and Removal <a id='loops'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pickle\n",
    "import numpy as np\n",
    "from optlang.symbolics import Zero\n",
    "import pandas as pd\n",
    "\n",
    "import cobra\n",
    "from cobra.util import create_stoichiometric_matrix\n",
    "from cobra.io import load_json_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model and Uptake/Secretion rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_json_model('iCHO3K_unblocked.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and combining dictionaries\n",
    "\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_wt_dict.pkl', 'rb') as file:\n",
    "    uptsec_wt = pickle.load(file)\n",
    "\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_zela_dict.pkl', 'rb') as file:\n",
    "    uptsec_zela = pickle.load(file)\n",
    "\n",
    "# Define the dictionaries uptsec_wt and uptsec_zela\n",
    "combined_dict = {}\n",
    "\n",
    "# Loop through keys in uptsec_wt\n",
    "for key in uptsec_wt.keys():\n",
    "    combined_dict[key] = {}\n",
    "    # Add subkeys from uptsec_wt to combined_dict\n",
    "    for subkey, value in uptsec_wt[key].items():\n",
    "        combined_dict[key][subkey] = value\n",
    "\n",
    "# Loop through keys in uptsec_zela\n",
    "for key in uptsec_zela.keys():\n",
    "    if key not in combined_dict:\n",
    "        combined_dict[key] = {}\n",
    "    # Add subkeys from uptsec_zela to combined_dict\n",
    "    for subkey, value in uptsec_zela[key].items():\n",
    "        combined_dict[key][subkey] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the boundary conditions as per your code\n",
    "def set_boundary_conditions(model, exp_boundaries, batch, timepoint):\n",
    "    for rxn in model.boundary:\n",
    "        if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "            rxn.bounds = (-1000, 1000)\n",
    "        elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "            rxn.bounds = (-0.001, 1000)\n",
    "        elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "            rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "    \n",
    "    for reaction in model.reactions:\n",
    "        if reaction.id == 'EX_etoh_e': #Model creates infeasible solutions when secreting etoh\n",
    "            continue\n",
    "        for r,v in combined_dict.items():\n",
    "            if reaction.id == r:\n",
    "                reaction.upper_bound = 1000\n",
    "                reaction.lower_bound = v[(batch, timepoint)]\n",
    "\n",
    "# Function to remove ATP loop reactions by setting their flux to zero\n",
    "def remove_atp_loops(model, atp_loop_reactions):\n",
    "    for reaction_id in atp_loop_reactions:\n",
    "        if reaction_id in model.reactions:\n",
    "            model.reactions.get_by_id(reaction_id).bounds = (0, 0)  # Knock out the reaction\n",
    "\n",
    "\n",
    "# Function to run optimization for ATPM\n",
    "def optimize_atpm(model):\n",
    "    # Set the objective to maximize ATPM\n",
    "    if 'ATPM' in model.reactions:\n",
    "        model.objective = model.reactions.get_by_id('ATPM')\n",
    "        model.objective_direction = 'max'\n",
    "    else:\n",
    "        raise ValueError(\"ATPM reaction not found in the model.\")\n",
    "    \n",
    "    # Optimize the model\n",
    "    solution = model.optimize()\n",
    "    if solution.status == 'optimal':\n",
    "        atpm_flux = solution.fluxes['ATPM']\n",
    "        flux_distribution = solution.fluxes\n",
    "    else:\n",
    "        atpm_flux = None  # Infeasible solution\n",
    "        flux_distribution = None\n",
    "    return atpm_flux, flux_distribution\n",
    "\n",
    "# Function to run optimization for Glucose generation from dextrin\n",
    "def optimize_glc(model):\n",
    "    # Set the objective to generate Glucose\n",
    "    if 'GLDBRAN' in model.reactions:\n",
    "        model.objective = model.reactions.get_by_id('GLDBRAN')\n",
    "        model.objective_direction = 'max'\n",
    "    else:\n",
    "        raise ValueError(\"GLDBRAN reaction not found in the model.\")\n",
    "    \n",
    "    # Optimize the model\n",
    "    solution = model.optimize()\n",
    "    if solution.status == 'optimal':\n",
    "        glc_flux = solution.fluxes['GLDBRAN']\n",
    "        flux_distribution = solution.fluxes\n",
    "    else:\n",
    "        glc_flux = None  # Infeasible solution\n",
    "        flux_distribution = None\n",
    "    return glc_flux, flux_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_reactions = [\n",
    "    'SCP22x','TMNDNCCOAtx','OCCOAtx','r0391','BiGGRxn67','r2247','r2280',\n",
    "    'r2246','r2279','r2245','r2305','r2317','r2335','HMR_0293','HMR_7741',\n",
    "    'r0509','r1453','HMR_4343','ACONTm','PDHm','r0426','r0383','r0555',\n",
    "    'r1393','NICRNS','GAUGE-R00648','GAUGE-R03326','GapFill-R08726','RE2915M',\n",
    "    'HMR_3288','HMR_1325','HMR_7599','r1431','r1433','RE2439C','r0082','r0791',\n",
    "    'r1450','GAUGE-R00270','GAUGE-R02285','GAUGE-R04283','GAUGE-R06127','GAUGE-R06128',\n",
    "    'GAUGE-R06238','GAUGE-R00524','RE3477C','AAPSAS','RE3347C','HMR_0960','HMR_0980',\n",
    "    'RE3476C','r0708','r0777','r0084','r0424','r0698','3HDH260p','HMR_3272','ACOAD183n3m',\n",
    "    'HMR_1996','GapFill-R01463','GapFill-R04807','r1468','r2435','r0655','r0603','r0541',\n",
    "    'RE0383C','HMR_1329','TYRA','NRPPHRt_2H','GAUGE-R07364','GapFill-R03599','ARD',\n",
    "    'RE3095C','RE3104C','RE3104R','ACONT','ICDHxm','ICDHy','r0083',\n",
    "    'r0425','r0556','NH4t4r','PROPAT4te','r0085','r0156','r0464','ABUTDm',\n",
    "    'OIVD1m','OIVD2m','OIVD3m','r2194','r2202','HMR_9617','r2197','r2195',\n",
    "    '2OXOADOXm','r2328','r0386','r0451','FAS100COA','FAS120COA','FAS140COA',\n",
    "    'FAS80COA_L','r0604','r0670','r2334','r0193','r0595','r0795','GLYCLm',\n",
    "    'MACACI','r2193','r0779','r0669','UDCHOLt','r2146','r2139'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  # to store results of all (batch, timepoint) combos\n",
    "\n",
    "timepoints = ['P0 to P2', 'P2 to P4', 'P4 to P6', 'P6 to P8']\n",
    "batches = ['U1', 'U2', 'U3', 'U4', 'U5', 'U6', 'U7', 'U8']\n",
    "\n",
    "for timepoint in timepoints:\n",
    "    for batch in batches:\n",
    "        print(f'Analyzing batch {batch} at timepoint {timepoint}')\n",
    "        try:\n",
    "            with model as test_model:\n",
    "                # 1. Set boundary conditions for the current batch and timepoint\n",
    "                set_boundary_conditions(test_model, combined_dict, batch, timepoint)\n",
    "\n",
    "                # 2. Remove loop-generating reactions before optimization\n",
    "                #remove_atp_loops(test_model, loop_reactions)\n",
    "                \n",
    "                # 3. (Optional) Check feasibility & Growth Rate (GR)\n",
    "                #    We’ll use slim_optimize() to quickly get the biomass objective, \n",
    "                #    assuming it’s already set in the model\n",
    "                gr = test_model.slim_optimize()\n",
    "                \n",
    "                if gr is None:\n",
    "                    print('yay')\n",
    "                    # If infeasible or something off, skip or store infeasible\n",
    "                    print(f\"No feasible growth solution for {batch}, {timepoint}\")\n",
    "                    results.append({\n",
    "                        'Batch': batch,\n",
    "                        'Timepoint': timepoint,\n",
    "                        'Growth Rate': None,\n",
    "                        'Max ATP': None,\n",
    "                        'Max Glc': None,\n",
    "                        'High Flux Reactions (ATP)': None,\n",
    "                        'High Flux Reactions (Glc)': None,\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                # 4. Optimize for ATPM\n",
    "                max_atp, flux_distribution_atp = optimize_atpm(test_model)\n",
    "                \n",
    "                # Identify high-flux reactions from the ATP optimization\n",
    "                if flux_distribution_atp is not None:\n",
    "                    high_flux_threshold = 2\n",
    "                    high_flux_reactions_atp = flux_distribution_atp[abs(flux_distribution_atp) >= high_flux_threshold]\n",
    "                    high_flux_reactions_atp_dict = high_flux_reactions_atp.to_dict()\n",
    "                else:\n",
    "                    high_flux_reactions_atp_dict = None\n",
    "\n",
    "                # 5. Optimize for Glucose generation (dextrin -> GLC)\n",
    "                max_glc, flux_distribution_glc = optimize_glc(test_model)\n",
    "                \n",
    "                # Identify high-flux reactions from the Glc optimization\n",
    "                if flux_distribution_glc is not None:\n",
    "                    high_flux_threshold = 2\n",
    "                    high_flux_reactions_glc = flux_distribution_glc[abs(flux_distribution_glc) >= high_flux_threshold]\n",
    "                    high_flux_reactions_glc_dict = high_flux_reactions_glc.to_dict()\n",
    "                else:\n",
    "                    high_flux_reactions_glc_dict = None\n",
    "                \n",
    "                # 6. Store all results\n",
    "                results.append({\n",
    "                    'Batch': batch,\n",
    "                    'Timepoint': timepoint,\n",
    "                    'Growth Rate': gr,\n",
    "                    'Max ATP': max_atp,\n",
    "                    'High Flux Reactions (ATP)': high_flux_reactions_atp_dict,\n",
    "                    'Max Glc': max_glc,\n",
    "                    'High Flux Reactions (Glc)': high_flux_reactions_glc_dict\n",
    "                })\n",
    "                \n",
    "                # Print summary to console\n",
    "                print(f'GR for batch {batch}, timepoint {timepoint}: {gr}')\n",
    "                print(f'Max ATP for batch {batch}, timepoint {timepoint}: {max_atp}')\n",
    "                print(f'Max Glc for batch {batch}, timepoint {timepoint}: {max_glc}')\n",
    "                \n",
    "                if high_flux_reactions_atp_dict is not None:\n",
    "                    print(f'ATP high-flux reactions: {len(high_flux_reactions_atp_dict)}')\n",
    "                if high_flux_reactions_glc_dict is not None:\n",
    "                    print(f'Glucose high-flux reactions: {len(high_flux_reactions_glc_dict)}')\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Catch any errors and store them\n",
    "            print(f\"Error with batch {batch} and timepoint {timepoint}: {e}\")\n",
    "            results.append({\n",
    "                'Batch': batch,\n",
    "                'Timepoint': timepoint,\n",
    "                'Growth Rate': None,\n",
    "                'Max ATP': None,\n",
    "                'Max Glc': None,\n",
    "                'High Flux Reactions (ATP)': None,\n",
    "                'High Flux Reactions (Glc)': None,\n",
    "                'Error': str(e)\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save loop generating reactions as a txt file\n",
    "\n",
    "# Option 1: Write one item per line\n",
    "with open('loop_reactions.txt', 'w') as f:\n",
    "    for item in loop_reactions:\n",
    "        f.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. tSNE and Heatmaps Comparison of Models <a id='tSNE'></a>\n",
    "Here we visualize the relationships across the reaction structures of individual cell lines to reveal of similar cell types group together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import mstats\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from collections import defaultdict\n",
    "from cobra.io import load_matlab_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_cobra_models(models, print_results=False, plot_results=False, group_vector=None, func_compare=False, task_file=None, subsystem_mapping=None,\n",
    "                                  subsystem_filter=None, condition_filter=None, save_path=None, cluster_rows=False, cluster_cols=False):\n",
    "    \"\"\"\n",
    "    Compares two or more condition-specific COBRA models in terms of reactions and subsystems.\n",
    "    \n",
    "    Parameters:\n",
    "        models (list): List of `cobra.Model` objects to compare.\n",
    "        print_results (bool): If True, prints the results. Default is False.\n",
    "        plot_results (bool): If True, generates plots. Default is False.\n",
    "        group_vector (list): List or array that groups similar models. Default is None.\n",
    "        func_compare (bool): If True, performs functional comparison. Default is False.\n",
    "        task_file (str): Name of the task file for functional comparison. Default is None.\n",
    "        subsystem_mapping (dict): Mapping from subsystem names to grouped system names.\n",
    "    \n",
    "    Returns:\n",
    "        comp_struct (dict): A dictionary containing the comparison results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if there are at least two models\n",
    "    if len(models) <= 1:\n",
    "        raise ValueError(\"Cannot compare only one model. Provide at least two models.\")\n",
    "\n",
    "    # Initialize comp_struct to store the comparison results\n",
    "    comp_struct = {\n",
    "        'modelIDs': [],\n",
    "        'reactions': {},\n",
    "        'subsystems': {},\n",
    "        'structComp': None,\n",
    "        'structCompMap': None,\n",
    "        'funcComp': None\n",
    "    }\n",
    "\n",
    "    # Get model IDs\n",
    "    comp_struct['modelIDs'] = [model.id for model in models]\n",
    "\n",
    "    # Assign labels based on the model names\n",
    "    labels = assign_labels_based_on_name(comp_struct['modelIDs'])\n",
    "\n",
    "    # Group models by WT and ZeLa\n",
    "    wt_models = [model for model in models if 'WT' in model.id]\n",
    "    zela_models = [model for model in models if 'ZeLa' in model.id]\n",
    "\n",
    "    # Compare subsystems with grouping\n",
    "    print(\"\\nComparing subsystem utilization\")\n",
    "    subsystems, subsystem_matrix = compare_cobra_field(models, 'subsystem', normalize=True, subsystem_mapping=subsystem_mapping)\n",
    "    comp_struct['subsystems'] = {'ID': subsystems, 'matrix': subsystem_matrix}\n",
    "\n",
    "    if print_results:\n",
    "        print(\"\\nComparison of grouped subsystem populations:\")\n",
    "        df = pd.DataFrame(subsystem_matrix, index=subsystems, columns=comp_struct['modelIDs'])\n",
    "        print(df.head(15))\n",
    "\n",
    "    if plot_results:\n",
    "        # Compute the average usage of subsystems for WT and ZeLa groups\n",
    "        wt_indices = [i for i, model_id in enumerate(comp_struct['modelIDs']) if 'WT' in model_id]\n",
    "        zela_indices = [i for i, model_id in enumerate(comp_struct['modelIDs']) if 'ZeLa' in model_id]\n",
    "\n",
    "        wt_avg = np.mean(subsystem_matrix[:, wt_indices], axis=1)\n",
    "        zela_avg = np.mean(subsystem_matrix[:, zela_indices], axis=1)\n",
    "\n",
    "        # Calculate the absolute differences between WT and ZeLa\n",
    "        differences = np.abs(wt_avg - zela_avg)\n",
    "\n",
    "        # Sort subsystems by the biggest differences\n",
    "        sorted_indices = np.argsort(differences)[::-1]  # Sort in descending order of differences\n",
    "        sorted_subsystems = [subsystems[i] for i in sorted_indices]\n",
    "        sorted_matrix = subsystem_matrix[sorted_indices, :]\n",
    "\n",
    "        # Plot heatmap for the sorted matrix\n",
    "        plot_heatmap(sorted_matrix, sorted_subsystems, comp_struct['modelIDs'], \"Systems Differences: WT vs ZeLa\", save_path=save_path, subsystem_filter=subsystem_filter, condition_filter=condition_filter,\n",
    "                     cluster_rows=cluster_rows, cluster_cols=cluster_cols)\n",
    "\n",
    "    # Compare model structures\n",
    "    print(\"\\nComparing model structures\")\n",
    "    reactions, reaction_matrix = compare_cobra_field(models, 'reaction')\n",
    "    comp_struct['reactions'] = {'IDs': reactions, 'matrix': reaction_matrix}\n",
    "\n",
    "    # t-SNE projection for structure comparison\n",
    "    n_models = reaction_matrix.T.shape[0]  # Get the number of models\n",
    "\n",
    "    # Set perplexity to be less than the number of models\n",
    "    perplexity = min(n_models - 1, 2)  # Ensure that perplexity is valid\n",
    "\n",
    "    tsne = TSNE(metric='hamming', n_components=2, perplexity=perplexity, random_state=42)\n",
    "    tsne_projection = tsne.fit_transform(reaction_matrix.T)\n",
    "    comp_struct['structCompMap'] = tsne_projection\n",
    "\n",
    "    if plot_results:\n",
    "        plot_tsne(tsne_projection, labels=labels, model_ids=comp_struct['modelIDs'], title=\"Network Topology t-SNE Projection\", save_path=save_path)\n",
    "\n",
    "    # Functional comparison (if required)\n",
    "    if func_compare and task_file:\n",
    "        print(\"\\nChecking model performance on specified tasks.\")\n",
    "        task_report = perform_functional_comparison(models, task_file)\n",
    "        comp_struct['funcComp'] = task_report\n",
    "        if plot_results:\n",
    "            task_matrix = np.array([report['ok'] for report in task_report])\n",
    "            plot_heatmap(task_matrix, comp_struct['modelIDs'], [task['description'] for task in task_report[0]['tasks']], \"Functional Comparison\")\n",
    "\n",
    "    return comp_struct\n",
    "\n",
    "\n",
    "def compare_cobra_field(models, field, normalize=False, subsystem_mapping=None):\n",
    "    \"\"\"\n",
    "    Compares COBRA models by a specified field (e.g., reactions, subsystems) and returns\n",
    "    a matrix indicating the number of reactions per field (e.g., subsystems) for each model.\n",
    "\n",
    "    Parameters:\n",
    "        models (list): List of `cobra.Model` objects.\n",
    "        field (str): The field to compare ('reaction', 'subsystem').\n",
    "        normalize (bool): If True and field is 'subsystem', applies Z-score normalization.\n",
    "        subsystem_mapping (dict): Mapping from subsystem names to grouped system names.\n",
    "\n",
    "    Returns:\n",
    "        id_list (list): List of unique field entries (reactions or subsystems).\n",
    "        comp_matrix (numpy array): Comparison matrix for the field.\n",
    "    \"\"\"\n",
    "    if field == 'reaction':\n",
    "        # For reactions, collect all unique reaction IDs\n",
    "        field_entries = [set(rxn.id for rxn in model.reactions) for model in models]\n",
    "    elif field == 'subsystem':\n",
    "        # For subsystems, collect all unique subsystems\n",
    "        if subsystem_mapping is not None:\n",
    "            # Reverse the mapping to get a mapping from subsystem names to group names\n",
    "            reverse_mapping = {}\n",
    "            for group, subsystems in subsystem_mapping.items():\n",
    "                for subsystem in subsystems:\n",
    "                    reverse_mapping[subsystem.upper()] = group  # Uppercase for case-insensitive matching\n",
    "            field_entries = []\n",
    "            for model in models:\n",
    "                mapped_subsystems = set()\n",
    "                for rxn in model.reactions:\n",
    "                    if rxn.subsystem:\n",
    "                        subsystem_name = rxn.subsystem.upper()\n",
    "                        group = reverse_mapping.get(subsystem_name)\n",
    "                        if group:\n",
    "                            mapped_subsystems.add(group)\n",
    "                field_entries.append(mapped_subsystems)\n",
    "        else:\n",
    "            field_entries = [set(rxn.subsystem for rxn in model.reactions if rxn.subsystem) for model in models]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid field. Supported fields are 'reaction' and 'subsystem'.\")\n",
    "\n",
    "    # Create a sorted list of all unique field entries\n",
    "    id_list = sorted(set.union(*field_entries))\n",
    "\n",
    "    # Initialize a matrix to store the number of reactions per field (subsystem or reaction)\n",
    "    comp_matrix = np.zeros((len(id_list), len(models)), dtype=int)\n",
    "\n",
    "    # Fill the comparison matrix\n",
    "    for i, model in enumerate(models):\n",
    "        if field == 'reaction':\n",
    "            # Create a set of reaction IDs in the model\n",
    "            reaction_ids = set(rxn.id for rxn in model.reactions)\n",
    "            for j, entry in enumerate(id_list):\n",
    "                if entry in reaction_ids:\n",
    "                    comp_matrix[j, i] = 1  # Presence of the reaction\n",
    "        elif field == 'subsystem':\n",
    "            # Create a dictionary to count reactions per subsystem\n",
    "            subsystem_counts = {subsystem: 0 for subsystem in id_list}\n",
    "            for rxn in model.reactions:\n",
    "                if rxn.subsystem:\n",
    "                    if subsystem_mapping is not None:\n",
    "                        subsystem_name = rxn.subsystem.upper()\n",
    "                        group = reverse_mapping.get(subsystem_name)\n",
    "                        if group:\n",
    "                            subsystem_counts[group] += 1\n",
    "                    else:\n",
    "                        subsystem_counts[rxn.subsystem] += 1\n",
    "            # Update the matrix with subsystem counts\n",
    "            for j, entry in enumerate(id_list):\n",
    "                comp_matrix[j, i] = subsystem_counts.get(entry, 0)\n",
    "\n",
    "    # Apply Z-score normalization only for subsystems if requested\n",
    "    if field == 'subsystem' and normalize:\n",
    "        comp_matrix = z_score_normalize(comp_matrix)\n",
    "\n",
    "    return id_list, comp_matrix\n",
    "\n",
    "\n",
    "def z_score_normalize(matrix):\n",
    "    \"\"\"\n",
    "    Applies Z-score normalization to each row of the matrix (i.e., for each subsystem).\n",
    "\n",
    "    Parameters:\n",
    "        matrix (numpy array): The comparison matrix to normalize.\n",
    "\n",
    "    Returns:\n",
    "        normalized_matrix (numpy array): Z-score normalized matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute Z-scores along the rows (axis=1)\n",
    "    mean_values = np.mean(matrix, axis=1, keepdims=True)\n",
    "    std_values = np.std(matrix, axis=1, keepdims=True)\n",
    "\n",
    "    # Avoid division by zero in case any row has zero standard deviation\n",
    "    std_values[std_values == 0] = 1\n",
    "\n",
    "    normalized_matrix = (matrix - mean_values) / std_values\n",
    "    return normalized_matrix\n",
    "\n",
    "\n",
    "def plot_heatmap(data, row_labels, col_labels, title, vmin=-2, vmax=2, save_path=None, \n",
    "                           subsystem_filter=None, condition_filter=None, method='average', metric='euclidean',\n",
    "                           standardize=False, cluster_rows=True, cluster_cols=False):\n",
    "    \"\"\"\n",
    "    Generates a clustered heatmap for the given data, optionally filtering by subsystems and conditions.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy array): Data matrix to plot.\n",
    "        row_labels (list): Labels for rows (subsystems).\n",
    "        col_labels (list): Labels for columns (conditions).\n",
    "        title (str): Title of the plot.\n",
    "        vmin (float): Minimum value for the color scale. Default is -2.\n",
    "        vmax (float): Maximum value for the color scale. Default is 2.\n",
    "        save_path (str): Path to save the plot. Default is None (no saving).\n",
    "        subsystem_filter (list): List of subsystem names to include in the plot. Default is None (all subsystems).\n",
    "        condition_filter (list): List of condition names (column labels) to include. Default is None (all conditions).\n",
    "        method (str): Linkage method for clustering ('single', 'complete', 'average', 'ward'). Default is 'average'.\n",
    "        metric (str): Distance metric for clustering ('euclidean', 'cityblock', 'cosine', etc.). Default is 'euclidean'.\n",
    "        standardize (bool): Whether to standardize the data before clustering. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter rows by specified subsystems if a filter is provided\n",
    "    if subsystem_filter:\n",
    "        subsystem_indices = [i for i, label in enumerate(row_labels) if label in subsystem_filter]\n",
    "        data = data[subsystem_indices, :]\n",
    "        row_labels = [row_labels[i] for i in subsystem_indices]\n",
    "\n",
    "    # Filter columns by specified conditions if a filter is provided\n",
    "    if condition_filter:\n",
    "        condition_indices = [i for i, label in enumerate(col_labels) if label in condition_filter]\n",
    "        data = data[:, condition_indices]\n",
    "        col_labels = [col_labels[i] for i in condition_indices]\n",
    "\n",
    "    # Check if the filtered data is valid for plotting\n",
    "    if data.shape[0] < 2 or data.shape[1] < 2:\n",
    "        print(f\"Insufficient data for clustered heatmap in '{title}'. Need at least 2 subsystems and 2 conditions.\")\n",
    "        return  # Skip plotting if data is too small\n",
    "\n",
    "    # Optional standardization\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "    # Create a pandas DataFrame for easier handling in seaborn\n",
    "    df = pd.DataFrame(data, index=row_labels, columns=col_labels)\n",
    "\n",
    "    # **Remove rows where all values are zero, and update row_labels\n",
    "    df = df.loc[(df != 0).any(axis=1)]\n",
    "    row_labels = df.index.tolist()\n",
    "    \n",
    "    # Plotting the clustered heatmap\n",
    "    g = sns.clustermap(df, cmap=\"coolwarm\", vmin=vmin, vmax=vmax, method=method, metric=metric,\n",
    "                       figsize=(12, 10), row_cluster=cluster_rows, col_cluster=cluster_cols, dendrogram_ratio=(0.2, 0.2), cbar_kws={'label': 'Value'})\n",
    "\n",
    "    # Adjust the title\n",
    "    plt.title(title, y=1.05)\n",
    "\n",
    "    # Save the plot if save_path is provided\n",
    "    if save_path:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Replace spaces and special characters in the title to create a valid filename\n",
    "        sanitized_title = title.replace(' ', '_').replace('-', '_').replace(':', '_')\n",
    "\n",
    "        # Construct the full file path with the .svg extension\n",
    "        filename = f\"{sanitized_title}.svg\"\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "\n",
    "        plt.savefig(full_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_tsne(data_matrix, labels=None, model_ids=None, title=\"t-SNE Projection\", save_path=None):\n",
    "    \"\"\"\n",
    "    Generates a 2D t-SNE projection of the input data with a specific color for each condition.\n",
    "\n",
    "    Parameters:\n",
    "        data_matrix (numpy array): High-dimensional data to project.\n",
    "        labels (list): Labels to use for coloring the plot (e.g., 'WT_P2', 'ZeLa_P4', etc.).\n",
    "        model_ids (list): Specific model IDs to annotate each point.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    if data_matrix.shape[1] < 2:\n",
    "        print(f\"Insufficient data for t-SNE in '{title}'. Only one model or not enough models found.\")\n",
    "        return  # Skip t-SNE if there are not enough models\n",
    "\n",
    "    # Define a color palette to map each condition to a specific color\n",
    "    unique_conditions = sorted(set(labels))  # Get unique conditions\n",
    "    color_palette = sns.color_palette(\"hsv\", len(unique_conditions))  # Generate distinct colors\n",
    "    condition_to_color = {condition: color_palette[i] for i, condition in enumerate(unique_conditions)}\n",
    "\n",
    "    # Assign colors based on the condition of each label\n",
    "    colors = [condition_to_color[label] for label in labels]\n",
    "\n",
    "    # Plotting the t-SNE results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(data_matrix[:, 0], data_matrix[:, 1], c=colors, s=100)\n",
    "\n",
    "    # Annotating the points with the model IDs\n",
    "    if model_ids is None:\n",
    "        model_ids = labels  # Default to labels if model_ids not provided\n",
    "    for i, model_id in enumerate(model_ids):\n",
    "        plt.annotate(model_id, (data_matrix[i, 0], data_matrix[i, 1]), fontsize=14)\n",
    "\n",
    "    # Create a legend mapping each condition to its color\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color=color, linestyle='', markersize=10)\n",
    "               for condition, color in condition_to_color.items()]\n",
    "    plt.legend(handles, unique_conditions, title=\"Conditions\", title_fontsize=16, loc=\"best\", prop={'size': 14})\n",
    "\n",
    "    # Increase title and axis label font sizes\n",
    "    plt.title(title, fontsize=22)\n",
    "    plt.xlabel('t-SNE 1', fontsize=18)\n",
    "    plt.ylabel('t-SNE 2', fontsize=18)\n",
    "    \n",
    "    # Increase tick label sizes\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    # Save the plot if save_path is provided\n",
    "    if save_path:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Replace spaces and special characters in the title to create a valid filename\n",
    "        sanitized_title = title.replace(' ', '_').replace('-', '_').replace(':', '_')\n",
    "\n",
    "        # Construct the full file path with the .svg extension\n",
    "        filename = f\"{sanitized_title}.svg\"\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "\n",
    "        plt.savefig(full_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_functional_comparison(models, task_file):\n",
    "    \"\"\"\n",
    "    Performs functional comparison for the models using a task file.\n",
    "\n",
    "    Parameters:\n",
    "        models (list): List of models.\n",
    "        task_file (str): Path to the task file.\n",
    "\n",
    "    Returns:\n",
    "        task_report (list): A list of task performance reports.\n",
    "    \"\"\"\n",
    "    task_report = []\n",
    "    for model in models:\n",
    "        # Placeholder for actual task-checking logic\n",
    "        report = {\n",
    "            'ok': np.random.randint(0, 2, size=10),  # Random binary result (pass/fail)\n",
    "            'description': [f'Task {i}' for i in range(10)]  # Example task descriptions\n",
    "        }\n",
    "        task_report.append(report)\n",
    "    return task_report\n",
    "\n",
    "def assign_labels_based_on_name(model_ids):\n",
    "    \"\"\"\n",
    "    Assigns labels to models based on their names (e.g., 'WT_P2', 'WT_P4', 'ZeLa_P2', etc.).\n",
    "\n",
    "    Parameters:\n",
    "        model_ids (list): List of model IDs.\n",
    "\n",
    "    Returns:\n",
    "        labels (list): List of labels corresponding to each model based on its ID.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for model_id in model_ids:\n",
    "        if 'WT_P2' in model_id:\n",
    "            labels.append('WT_P2')\n",
    "        elif 'WT_P4' in model_id:\n",
    "            labels.append('WT_P4')\n",
    "        elif 'WT_P6' in model_id:\n",
    "            labels.append('WT_P6')\n",
    "        elif 'WT_P8' in model_id:\n",
    "            labels.append('WT_P8')\n",
    "        elif 'ZeLa_P2' in model_id:\n",
    "            labels.append('ZeLa_P2')\n",
    "        elif 'ZeLa_P4' in model_id:\n",
    "            labels.append('ZeLa_P4')\n",
    "        elif 'ZeLa_P6' in model_id:\n",
    "            labels.append('ZeLa_P6')\n",
    "        elif 'ZeLa_P8' in model_id:\n",
    "            labels.append('ZeLa_P8')\n",
    "        else:\n",
    "            labels.append('Unknown')  # Fallback for unexpected names\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing context-specific models\n",
    "model_directory = '../Data/Context_specific_models/'\n",
    "\n",
    "# Model Names\n",
    "wt_model_names = ['WT_P2_Bio141', 'WT_P2_Bio142', 'WT_P2_Bio143',\n",
    "                  'WT_P4_Bio141', 'WT_P4_Bio142', 'WT_P4_Bio143',\n",
    "                  'WT_P6_Bio141', 'WT_P6_Bio142', 'WT_P6_Bio143']\n",
    "\n",
    "zela_model_names = ['ZeLa_P2_Bio145', 'ZeLa_P2_Bio146', 'ZeLa_P2_Bio147', 'ZeLa_P2_Bio148',\n",
    "                    'ZeLa_P4_Bio144', 'ZeLa_P4_Bio145', 'ZeLa_P4_Bio146', 'ZeLa_P4_Bio147', 'ZeLa_P4_Bio148',\n",
    "                    'ZeLa_P6_Bio144', 'ZeLa_P6_Bio146', 'ZeLa_P6_Bio147', 'ZeLa_P6_Bio148',\n",
    "                    'ZeLa_P8_Bio144', 'ZeLa_P8_Bio145', 'ZeLa_P8_Bio146', 'ZeLa_P8_Bio147', 'ZeLa_P8_Bio148']\n",
    "\n",
    "# List to store the loaded models\n",
    "models_list = []\n",
    "\n",
    "# List all .mat files in the directory\n",
    "model_files = [f for f in os.listdir(model_directory) if f.endswith('.mat')]\n",
    "\n",
    "# Function to parse and extract category, time point, and batch for sorting\n",
    "def parse_model_name(model_name):\n",
    "    parts = model_name.split('_')\n",
    "    if len(parts) == 3:\n",
    "        category = parts[0]  # WT or ZeLa\n",
    "        time_point = parts[1]  # P2, P4, P6, P8\n",
    "        batch = parts[2][3:]  # Extract the number from 'Bio141', 'Bio142', etc.\n",
    "        return category, time_point, batch\n",
    "    return None, None, None\n",
    "\n",
    "# Iterate through the .mat files and load only the models that match predefined strings\n",
    "for model_file in model_files:\n",
    "    # Full path to the model file\n",
    "    model_path = os.path.join(model_directory, model_file)\n",
    "    \n",
    "    # Check if the file corresponds to a WT or ZeLa model\n",
    "    for model_name in wt_model_names + zela_model_names:\n",
    "        if model_name in model_file:\n",
    "            # Load the model\n",
    "            model = load_matlab_model(model_path)\n",
    "            model.id = model_name\n",
    "\n",
    "            # Append the model to the list\n",
    "            models_list.append(model)\n",
    "            print(f\"Loaded model: {model_name}\")\n",
    "            break  # Stop checking other names since this file is already processed\n",
    "\n",
    "# Function to generate a sorting key for models\n",
    "def model_sort_key(model):\n",
    "    category, time_point, batch = parse_model_name(model.id)\n",
    "    return (category, time_point, int(batch))\n",
    "\n",
    "# Sort the list of models by category (WT, ZeLa), time point (P2, P4, P6), and batch (numeric)\n",
    "models_list.sort(key=model_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to select subsystems and conditions\n",
    "\n",
    "#amino_acid_subsystems = subsystems_dict['AMINO ACID METABOLISM']\n",
    "#selected_conditions = ['WT_P4_Bio141', 'WT_P4_Bio142', 'WT_P4_Bio143', 'ZeLa_P4_Bio144', 'ZeLa_P4_Bio145', 'ZeLa_P4_Bio146', 'ZeLa_P4_Bio147', 'ZeLa_P4_Bio148']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison function\n",
    "save_path='../Analyses/tSNE/'\n",
    "\n",
    "comp_struct = compare_multiple_cobra_models(\n",
    "    models_list,\n",
    "    print_results=False,\n",
    "    plot_results=True,\n",
    "    #subsystem_mapping=subsystems_dict, #Uncomment this if using subsystem_dict (generated in section 1 --above--)\n",
    "    #subsystem_filter=amino_acid_subsystems,\n",
    "    #condition_filter=selected_conditions,\n",
    "    save_path=save_path,\n",
    "    cluster_rows=True,\n",
    "    cluster_cols=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of transcriptomics datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rna_seq = pd.read_csv('../Data/ZeLa Data/20200307_Bio141-148_merged.tpm.tsv', sep='\\t')\n",
    "icho3k_genes = pd.read_excel('../Data/iCHO3K_final/iCHO3K.xlsx', sheet_name='Genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_seq_subset = rna_seq[rna_seq['Gene'].isin(icho3k_genes['Gene Symbol'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping from 'S' numbers to sample conditions\n",
    "sample_conditions = {\n",
    "    'S1':  'WT_P2_Bio141', 'S7':  'WT_P2_Bio142', 'S13': 'WT_P2_Bio143', 'S2':  'WT_P4_Bio141', 'S8':  'WT_P4_Bio142', 'S14': 'WT_P4_Bio143', 'S3':  'WT_P6_Bio141',\n",
    "    'S9':  'WT_P6_Bio142', 'S15': 'WT_P6_Bio143', 'S4':  'WT_P8_Bio141', 'S10': 'WT_P8_Bio142', 'S16': 'WT_P8_Bio143', 'S5':  'WT_P12_Bio141', 'S11': 'WT_P12_Bio142',\n",
    "    'S6':  'WT_P14_Bio141', 'S12': 'WT_P14_Bio142', 'S17': 'WT_P14_Bio143', 'S18': 'ZeLa_P4_Bio144', 'S23': 'ZeLa_P4_Bio145', 'S28': 'ZeLa_P4_Bio146',\n",
    "    'S34': 'ZeLa_P4_Bio147', 'S39': 'ZeLa_P4_Bio148', 'S19': 'ZeLa_P6_Bio144', 'S29': 'ZeLa_P6_Bio146', 'S35': 'ZeLa_P6_Bio147', 'S40': 'ZeLa_P6_Bio148',\n",
    "    'S20': 'ZeLa_P8_Bio144', 'S24': 'ZeLa_P8_Bio145', 'S30': 'ZeLa_P8_Bio146', 'S36': 'ZeLa_P8_Bio147', 'S41': 'ZeLa_P8_Bio148', 'S25': 'ZeLa_P12_Bio145',\n",
    "    'S31': 'ZeLa_P12_Bio146', 'S42': 'ZeLa_P12_Bio148', 'S21': 'ZeLa_P14_Bio144', 'S26': 'ZeLa_P14_Bio145', 'S32': 'ZeLa_P14_Bio146', 'S37': 'ZeLa_P14_Bio147',\n",
    "    'S43': 'ZeLa_P14_Bio148', 'S22': 'ZeLa_P2_Bio145',  'S27': 'ZeLa_P2_Bio146', 'S33': 'ZeLa_P2_Bio147', 'S38': 'ZeLa_P2_Bio148'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the columns in 'rna_seq' to sample conditions\n",
    "sample_columns = rna_seq_subset.columns.tolist()[1:]  # Exclude 'Gene' column\n",
    "column_to_condition = {col: sample_conditions.get(col, 'Unknown') for col in sample_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names\n",
    "wt_model_names = ['WT_P2_Bio141', 'WT_P2_Bio142', 'WT_P2_Bio143',\n",
    "                  'WT_P4_Bio141', 'WT_P4_Bio142', 'WT_P4_Bio143',\n",
    "                  'WT_P6_Bio141', 'WT_P6_Bio142', 'WT_P6_Bio143']\n",
    "\n",
    "zela_model_names = ['ZeLa_P2_Bio145', 'ZeLa_P2_Bio146', 'ZeLa_P2_Bio147', 'ZeLa_P2_Bio148',\n",
    "                    'ZeLa_P4_Bio144', 'ZeLa_P4_Bio145', 'ZeLa_P4_Bio146', 'ZeLa_P4_Bio147', 'ZeLa_P4_Bio148',\n",
    "                    'ZeLa_P6_Bio144', 'ZeLa_P6_Bio146', 'ZeLa_P6_Bio147', 'ZeLa_P6_Bio148',\n",
    "                    'ZeLa_P8_Bio144', 'ZeLa_P8_Bio145', 'ZeLa_P8_Bio146', 'ZeLa_P8_Bio147', 'ZeLa_P8_Bio148']\n",
    "\n",
    "\n",
    "# Define the conditions to include\n",
    "include_conditions = set(wt_model_names + zela_model_names)\n",
    "\n",
    "# Create a list of columns to include\n",
    "columns_to_include = ['Gene'] + [col for col in sample_columns if column_to_condition[col] in include_conditions]\n",
    "\n",
    "# Subset the data\n",
    "rna_seq_subset_filtered = rna_seq_subset[columns_to_include]\n",
    "rna_seq_subset_filtered.set_index('Gene', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample conditions for the subset columns\n",
    "subset_columns = rna_seq_subset_filtered.columns.tolist()\n",
    "subset_sample_conditions = [column_to_condition[col] for col in subset_columns]\n",
    "\n",
    "# Define a function to assign labels\n",
    "def assign_sample_labels(sample_conditions):\n",
    "    labels = []\n",
    "    for condition in sample_conditions:\n",
    "        if 'WT_P2' in condition:\n",
    "            labels.append('WT_P2')\n",
    "        elif 'WT_P4' in condition:\n",
    "            labels.append('WT_P4')\n",
    "        elif 'WT_P6' in condition:\n",
    "            labels.append('WT_P6')\n",
    "        elif 'WT_P8' in condition:\n",
    "            labels.append('WT_P8')\n",
    "        elif 'ZeLa_P2' in condition:\n",
    "            labels.append('ZeLa_P2')\n",
    "        elif 'ZeLa_P4' in condition:\n",
    "            labels.append('ZeLa_P4')\n",
    "        elif 'ZeLa_P6' in condition:\n",
    "            labels.append('ZeLa_P6')\n",
    "        elif 'ZeLa_P8' in condition:\n",
    "            labels.append('ZeLa_P8')\n",
    "        else:\n",
    "            labels.append('Unknown')\n",
    "    return labels\n",
    "\n",
    "# Assign labels\n",
    "labels = assign_sample_labels(subset_sample_conditions)\n",
    "\n",
    "# Verify the labels\n",
    "print(\"Sample Labels:\")\n",
    "for sample, label in zip(subset_sample_conditions, labels):\n",
    "    print(f\"{sample}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Set 'Gene' as the index\n",
    "\n",
    "\n",
    "# # Log2 transformation\n",
    "data_log = np.log2(rna_seq_subset_filtered + 1)\n",
    "\n",
    "# # Log10 transformation\n",
    "data_log10 = np.log10(rna_seq_subset_filtered + 1)\n",
    "\n",
    "# # Square root transformation\n",
    "data_sqrt = np.sqrt(rna_seq_subset_filtered)\n",
    "\n",
    "# # Z-score normalization\n",
    "data_zscore = (rna_seq_subset_filtered - rna_seq_subset_filtered.mean()) / rna_seq_subset_filtered.std()\n",
    "\n",
    "# # Min-max normalization\n",
    "data_minmax = (rna_seq_subset_filtered - rna_seq_subset_filtered.min()) / (rna_seq_subset_filtered.max() - rna_seq_subset_filtered.min())\n",
    "\n",
    "# # Natural log transformation\n",
    "data_ln = np.log(rna_seq_subset_filtered + 1)\n",
    "\n",
    "# # Robust scaling\n",
    "data_robust = (rna_seq_subset_filtered - rna_seq_subset_filtered.median()) / (rna_seq_subset_filtered.quantile(0.75) - rna_seq_subset_filtered.quantile(0.25))\n",
    "\n",
    "# Pareto scaling\n",
    "data_pareto = (rna_seq_subset_filtered - rna_seq_subset_filtered.mean()) / np.sqrt(rna_seq_subset_filtered.std())\n",
    "\n",
    "# Transpose the data to have samples as rows\n",
    "data_matrix = data_log.T.values  # Shape: (samples, genes)\n",
    "\n",
    "# Check for NaNs or infinite values\n",
    "if np.isnan(data_matrix).any():\n",
    "    print(\"Data contains NaNs, handling missing values.\")\n",
    "    # Fill NaNs with the mean expression of each gene\n",
    "    data_matrix = np.nan_to_num(data_matrix, nan=np.nanmean(data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP PCA\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA\n",
    "#pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "#data_pca = pca.fit_transform(data_matrix)\n",
    "\n",
    "# pca_init = PCA(n_components=None,svd_solver='full')\n",
    "# score = pca_init.fit_transform(data_matrix)\n",
    "# Var=pca_init.explained_variance_ratio_.cumsum()\n",
    "# nscores = np.argmax(Var > 0.95)+1\n",
    "\n",
    "# # second iteration with number compnent to explain >90% of variance\n",
    "# pca = PCA(n_components=nscores,svd_solver='full')\n",
    "# data_pca = pca.fit_transform(data_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Determine perplexity\n",
    "n_samples = data_matrix.shape[0]\n",
    "perplexity = min(n_samples - 1, 30)\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(metric='euclidean', n_components=2, perplexity=perplexity, random_state=42)\n",
    "tsne_results = tsne.fit_transform(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_tsne_transcriptomics(tsne_results, labels, sample_names, title=\"Transcriptomics t-SNE Projection\", save_path=None):\n",
    "    unique_conditions = sorted(set(labels))\n",
    "    color_palette = sns.color_palette(\"hsv\", len(unique_conditions))\n",
    "    condition_to_color = {condition: color_palette[i] for i, condition in enumerate(unique_conditions)}\n",
    "    colors = [condition_to_color[label] for label in labels]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=colors, s=100)\n",
    "    \n",
    "    # Increase annotation font size\n",
    "    for i, sample_name in enumerate(sample_names):\n",
    "        plt.annotate(sample_name, (tsne_results[i, 0], tsne_results[i, 1]), fontsize=14)\n",
    "    \n",
    "    # Customize legend with larger font\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color=color, linestyle='', markersize=10)\n",
    "               for condition, color in condition_to_color.items()]\n",
    "    plt.legend(handles, unique_conditions, title=\"Conditions\", title_fontsize=16, loc=\"best\", prop={'size': 14})\n",
    "    \n",
    "    # Increase title and axis label font sizes\n",
    "    plt.title(title, fontsize=22)\n",
    "    plt.xlabel('t-SNE 1', fontsize=18)\n",
    "    plt.ylabel('t-SNE 2', fontsize=18)\n",
    "    \n",
    "    # Increase tick label sizes\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        sanitized_title = title.replace(' ', '_').replace('-', '_').replace(':', '_')\n",
    "        filename = f\"{sanitized_title}.svg\"\n",
    "        full_path = os.path.join(save_path, filename)\n",
    "        plt.savefig(full_path, format='svg', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the plotting function\n",
    "save_path='../Analyses/tSNE/'\n",
    "sample_names = subset_sample_conditions  # Sample conditions corresponding to the samples\n",
    "plot_tsne_transcriptomics(tsne_results, labels, sample_names, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. pFBA <a id='pFBA'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import cobra\n",
    "from cobra import Reaction\n",
    "from cobra.io import load_json_model, read_sbml_model, load_matlab_model\n",
    "from cobra.exceptions import Infeasible\n",
    "from cobra.sampling import ACHRSampler\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing context-specific models models\n",
    "model_directory = '../Data/Context_specific_models/'\n",
    "\n",
    "# Model Names\n",
    "wt_model_names = ['WT_P2_Bio141', 'WT_P2_Bio142', 'WT_P2_Bio143',\n",
    "                  'WT_P4_Bio141', 'WT_P4_Bio142', 'WT_P4_Bio143',\n",
    "                  'WT_P6_Bio141', 'WT_P6_Bio142', 'WT_P6_Bio143']\n",
    "\n",
    "zela_model_names = ['ZeLa_P2_Bio145', 'ZeLa_P2_Bio146', 'ZeLa_P2_Bio147', 'ZeLa_P2_Bio148',\n",
    "                    'ZeLa_P4_Bio144', 'ZeLa_P4_Bio145', 'ZeLa_P4_Bio146', 'ZeLa_P4_Bio147', 'ZeLa_P4_Bio148',\n",
    "                    'ZeLa_P6_Bio144', 'ZeLa_P6_Bio146', 'ZeLa_P6_Bio147', 'ZeLa_P6_Bio148',\n",
    "                    'ZeLa_P8_Bio144', 'ZeLa_P8_Bio145', 'ZeLa_P8_Bio146', 'ZeLa_P8_Bio147', 'ZeLa_P8_Bio148']\n",
    "\n",
    "# Dictionaries to store the loaded models\n",
    "wt_models = {}\n",
    "zela_models = {}\n",
    "\n",
    "# List all .mat files in the directory\n",
    "model_files = [f for f in os.listdir(model_directory) if f.endswith('.mat')]\n",
    "\n",
    "# Iterate through the .mat files and load only the models that match predefined strings\n",
    "for model_file in model_files:\n",
    "    # Full path to the model file\n",
    "    model_path = os.path.join(model_directory, model_file)\n",
    "    \n",
    "    # Check if the file corresponds to a WT model\n",
    "    for model_name in wt_model_names:\n",
    "        if model_name in model_file:\n",
    "            wt_models[model_name] = load_matlab_model(model_path)\n",
    "            wt_models[model_name].id = model_name\n",
    "            print(f\"Loaded WT model: {model_name}\")\n",
    "            break  # Stop checking other names since this file is already processed\n",
    "    \n",
    "    # Check if the file corresponds to a ZeLa model\n",
    "    for model_name in zela_model_names:\n",
    "        if model_name in model_file:\n",
    "            zela_models[model_name] = load_matlab_model(model_path)\n",
    "            zela_models[model_name].id = model_name\n",
    "            print(f\"Loaded ZeLa model: {model_name}\")\n",
    "            break  # Stop checking other names since this file is already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uptake and secretion rate dict\n",
    "\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_wt_dict.pkl', 'rb') as file:\n",
    "    uptsec_wt = pickle.load(file)\n",
    "\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_zela_dict.pkl', 'rb') as file:\n",
    "    uptsec_zela = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATP generating reactions loop\n",
    "\n",
    "atp_loop_reactions = [\n",
    "    'SCP22x','TMNDNCCOAtx','OCCOAtx','r0391','BiGGRxn67','r2247','r2280',\n",
    "    'r2246','r2279','r2245','r2305','r2317','r2335','HMR_0293','HMR_7741',\n",
    "    'r0509','r1453','HMR_4343','ACONTm','PDHm','r0426','r0383','r0555',\n",
    "    'r1393','NICRNS','GAUGE-R00648','GAUGE-R03326','GapFill-R08726','RE2915M',\n",
    "    'HMR_3288','HMR_1325','HMR_7599','r1431','r1433','RE2439C','r0791',\n",
    "    'r1450','GAUGE-R00270','GAUGE-R02285','GAUGE-R04283','GAUGE-R06127','GAUGE-R06128',\n",
    "    'GAUGE-R06238','GAUGE-R00524','RE3477C','AAPSAS','RE3347C','HMR_0960','HMR_0980',\n",
    "    'RE3476C','r0708','r0777','r0424','r0698','3HDH260p','HMR_3272','ACOAD183n3m',\n",
    "    'HMR_1996','GapFill-R01463','GapFill-R04807','r1468','r2435','r0655','r0603','r0541',\n",
    "    'RE0383C','HMR_1329','TYRA','NRPPHRt_2H','GAUGE-R07364','GapFill-R03599','ARD',\n",
    "    'RE3095C','RE3104C','RE3104R','ACONT','ICDHxm','ICDHy',\n",
    "    'r0425','r0556','NH4t4r','PROPAT4te','r0085','r0156','r0464','ABUTDm',\n",
    "    'OIVD1m','OIVD2m','OIVD3m','r2194','r2202','HMR_9617','r2197','r2195',\n",
    "    '2OXOADOXm','r2328','r0386','r0451','FAS100COA','FAS120COA','FAS140COA',\n",
    "    'FAS80COA_L','r0604','r0670','r2334','r0193','r0595','r0795','GLYCLm',\n",
    "    'MACACI','r2193','r0779','r0669','UDCHOLt','r2146','r2139'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check. Make sure all the exchange reactions in the experimental data are present in the model and that the models ar growing\n",
    "\n",
    "for name,model in zela_models.items():\n",
    "    print('---------------------------------------')\n",
    "    print(f'------------{model.id}------------')\n",
    "    print('')\n",
    "    print(f'Objective:{model.objective}')\n",
    "    print('')\n",
    "    print(f'GR1: {model.slim_optimize()}')\n",
    "    exchanges = list(uptsec_wt.keys())\n",
    "    \n",
    "    model_ex = [r.id for r in model.boundary]\n",
    "    model_reactions = [r.id for r in model.reactions]\n",
    "\n",
    "    # Remove ATP loop reactions\n",
    "    for atp_reaction in atp_loop_reactions:\n",
    "        if atp_reaction in model_reactions:\n",
    "            # Knock out the reaction temporarily\n",
    "            reaction = model.reactions.get_by_id(atp_reaction)\n",
    "            original_bounds = reaction.bounds  # Store the original bounds to revert later\n",
    "            reaction.knock_out()\n",
    "            \n",
    "            # Test if the model remains feasible after knocking out the reaction\n",
    "            new_gr = model.slim_optimize()\n",
    "            print(f'GR2 (After knocking out {atp_reaction}): {new_gr}')\n",
    "            \n",
    "            # Check if the model's growth rate is infeasible or close to zero\n",
    "            if math.isnan(new_gr) or new_gr < 1e-6:\n",
    "                print(f'{atp_reaction} causes infeasibility. Reverting knockout.')\n",
    "                reaction.bounds = (0,1000)  # Set reaction to irreversible\n",
    "\n",
    "                new_gr_3 = model.slim_optimize() # Optimize model with irreversible reaction\n",
    "                print(f'GR3 (After making reaction irreversible {atp_reaction}): {new_gr_3}')\n",
    "\n",
    "                # Check if the model's growth rate 3 is infeasible or close to zero\n",
    "                if math.isnan(new_gr_3) or new_gr_3 < 1e-6:\n",
    "                    print(f'{atp_reaction} in its irreversible format causes infeasibility.')\n",
    "                    reaction.bounds = original_bounds\n",
    "                else:\n",
    "                    print(f'{atp_reaction} successfully transformed to reversible without infeasibility.')\n",
    "                    \n",
    "            else:\n",
    "                print(f'{atp_reaction} successfully knocked out without infeasibility.')\n",
    "\n",
    "                \n",
    "    print('')\n",
    "    print('Missing reactions from exp data:')\n",
    "    for ex in exchanges:\n",
    "        if ex not in model_ex:\n",
    "            print(ex)\n",
    "    print('---------------------------------------')\n",
    "    print('---------------------------------------')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "\n",
    "for name,model in wt_models.items():\n",
    "    print('---------------------------------------')\n",
    "    print(f'------------{model.id}------------')\n",
    "    print('')\n",
    "    print(f'Objective:{model.objective}')\n",
    "    print('---------------------------------------')\n",
    "    print(f'GR: {model.slim_optimize()}')\n",
    "    exchanges = list(uptsec_wt.keys())\n",
    "    model_ex = [r.id for r in model.boundary]\n",
    "    model_reactions = [r.id for r in model.reactions]\n",
    "\n",
    "    # Remove ATP loop reactions\n",
    "    for atp_reaction in atp_loop_reactions:\n",
    "        if atp_reaction in model_reactions:\n",
    "            # Knock out the reaction temporarily\n",
    "            reaction = model.reactions.get_by_id(atp_reaction)\n",
    "            original_bounds = reaction.bounds  # Store the original bounds to revert later\n",
    "            reaction.knock_out()\n",
    "            \n",
    "            # Test if the model remains feasible after knocking out the reaction\n",
    "            new_gr = model.slim_optimize()\n",
    "            print(f'GR2 (After knocking out {atp_reaction}): {new_gr}')\n",
    "            \n",
    "            # Check if the model's growth rate is infeasible or close to zero\n",
    "            if math.isnan(new_gr) or new_gr < 1e-6:\n",
    "                print(f'{atp_reaction} causes infeasibility. Reverting knockout.')\n",
    "                reaction.bounds = (0,1000)  # Set reaction to irreversible\n",
    "\n",
    "                new_gr_3 = model.slim_optimize() # Optimize model with irreversible reaction\n",
    "                print(f'GR3 (After making reaction irreversible {atp_reaction}): {new_gr_3}')\n",
    "\n",
    "                # Check if the model's growth rate 3 is infeasible or close to zero\n",
    "                if math.isnan(new_gr_3) or new_gr_3 < 1e-6:\n",
    "                    print(f'{atp_reaction} in its irreversible format causes infeasibility.')\n",
    "                    reaction.bounds = original_bounds\n",
    "                else:\n",
    "                    print(f'{atp_reaction} successfully transformed to reversible without infeasibility.')\n",
    "                    \n",
    "            else:\n",
    "                print(f'{atp_reaction} successfully knocked out without infeasibility.')\n",
    "    \n",
    "    for ex in exchanges:\n",
    "        if ex not in model_ex:\n",
    "            print(ex)\n",
    "    print('---------------------------------------')\n",
    "    print('---------------------------------------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth Rate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WT\n",
    "\n",
    "results = []\n",
    "\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# Adjust the lower bound values according to the experimental growth rates in order to indetify bottlenecks\n",
    "intervals = {'P0 to P2':'P2', 'P2 to P4':'P4', 'P4 to P6':'P6', 'P6 to P8':'P8', 'P8 to P12':'P12', 'P12 to P14':'P14'}\n",
    "replicates = {'U1':'Bio141', 'U2':'Bio142', 'U3':'Bio143'}\n",
    "\n",
    "for name,model in wt_models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    # Set lower bounds of the reactions according to the experimental data\n",
    "    for interval_key, interval_model_time in intervals.items():\n",
    "        if interval_model_time in name:  # Match model name with interval\n",
    "            for rep_key, rep_suffix in replicates.items():\n",
    "                if rep_suffix in name:  # Match replicate with model name\n",
    "\n",
    "                     with model as modified_model:\n",
    "\n",
    "                        # Open the bounds for the biomass reaction \n",
    "                        modified_model.reactions.biomass_cho_s.bounds = (0,1000)\n",
    "                         \n",
    "                        # Create a copy of the modified_model before making changes\n",
    "                        pre_modification_model = modified_model.copy()\n",
    "                         \n",
    "                        for rxn in modified_model.boundary:\n",
    "                            # Keep boundaries open for essential metabolites\n",
    "                            if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                rxn.bounds = (-1000, 1000)\n",
    "                            elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                rxn.bounds = (-0.001, 1000)\n",
    "                            elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                                rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "                        \n",
    "                        print(f'Calculating Growth Rate for WT Condition:{rep_key,interval_key}')\n",
    "                        exp_gr = uptsec_wt['exp_growth_rate'][(rep_key,interval_key)]\n",
    "                        print(f'Experimental growth rate is: {exp_gr}')\n",
    "                        for reaction in modified_model.reactions:\n",
    "                            if reaction.id == 'EX_etoh_e': #Model creates infeasible solutions when secreting etoh\n",
    "                                continue\n",
    "                            elif reaction.id == 'ATPM': # Add ATP Maintenance Cost\n",
    "                                reaction.lower_bound = 8.5\n",
    "                            for r,v in uptsec_wt.items():\n",
    "                                if reaction.id == r:\n",
    "                                    reaction.upper_bound = 1000\n",
    "                                    reaction.lower_bound = v[(rep_key,interval_key)]\n",
    "                                    \n",
    "                        try:                            \n",
    "                            pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            pfba_atp = pfba_solution.fluxes['ATPM']\n",
    "                            print(f'Simulated ATP Maintencance is: {pfba_atp}')\n",
    "                            print(f'Simulated growth rate is: {pfba_obj}')\n",
    "            \n",
    "                        except Infeasible:\n",
    "                            print(f'Infeasible solution for replicate {rep_key} interval {interval_key}')\n",
    "                            print('Reverting to the original model and retrying pFBA...')\n",
    "                            \n",
    "                            for rxn in pre_modification_model.boundary:\n",
    "                            # Keep boundaries open for essential metabolites\n",
    "                                if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                    rxn.bounds = (-1000, 1000)\n",
    "                                elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                    rxn.bounds = (-0.01, 1000)\n",
    "                            \n",
    "                            pfba_solution = cobra.flux_analysis.pfba(pre_modification_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            pfba_atp = pfba_solution.fluxes['ATPM']\n",
    "                            print(f'Simulated ATP Maintencance is: {pfba_atp}')\n",
    "                            print(f'Simulated growth rate with original model is: {pfba_obj}')\n",
    "\n",
    "\n",
    "                        results.append([rep_suffix, interval_key, exp_gr, pfba_obj])\n",
    "\n",
    "                        \n",
    "# Creating a DataFrame\n",
    "df_wt = pd.DataFrame(results, columns=['Hue', 'Category', 'X Axis', 'Y Axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define markers for categories\n",
    "markers = {\n",
    "    'P0 to P2': 'p',  # Pentagons\n",
    "    'P2 to P4': 'o',  # Circles\n",
    "    'P4 to P6': 's',  # Squares\n",
    "}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each group with different colors for hue and different shapes for categories\n",
    "hues = df_wt['Hue'].unique()\n",
    "colors = plt.get_cmap('Set2')  # Improved color palette\n",
    "\n",
    "for hue_idx, hue in enumerate(hues):\n",
    "    hue_subset = df_wt[df_wt['Hue'] == hue]\n",
    "    for category, marker in markers.items():\n",
    "        subset = hue_subset[hue_subset['Category'] == category]\n",
    "        plt.scatter(subset['X Axis'], subset['Y Axis'], \n",
    "                    label=f'{hue} - {category}', \n",
    "                    marker=marker, \n",
    "                    s=120,  # Slightly larger size for better visibility\n",
    "                    color=colors(hue_idx),\n",
    "                    edgecolor='black',  # Adding edge color for better separation\n",
    "                    alpha=0.7)  # Adding transparency to reduce clutter\n",
    "\n",
    "# Add the identity line y = x\n",
    "plt.plot([0.00, 0.055], [0.00, 0.055], color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "\n",
    "x_data = df_wt['X Axis']\n",
    "y_data = df_wt['Y Axis']\n",
    "# Calculate R²\n",
    "r2 = r2_score(y_data, x_data)\n",
    "# Calculate Pearson's r\n",
    "r, p_value = pearsonr(x_data, y_data)\n",
    "\n",
    "# Annotate R² and Pearson's r\n",
    "plt.text(0.005, max(df_wt['Y Axis'].max(), df_wt['X Axis'].max()) * 0.95, \n",
    "         f'R² = {r2:.3f}\\nPearson\\'s r = {r:.3f}', \n",
    "         fontsize=14, \n",
    "         bbox=dict(facecolor='white', alpha=0.6, edgecolor='gray'))\n",
    "\n",
    "# Set evenly distributed ticks from 0.01 to 0.08\n",
    "ticks = [i / 100.0 for i in range(1, 11)]\n",
    "plt.xticks(ticks=ticks, labels=[f'{i/100.0:.2f}' for i in range(1, 11)], fontsize=12)\n",
    "plt.yticks(ticks=ticks, labels=[f'{i/100.0:.2f}' for i in range(1, 11)], fontsize=12)\n",
    "\n",
    "# Set limits for both axes\n",
    "plt.xlim(0.0, 0.055)\n",
    "plt.ylim(0.0, 0.055)\n",
    "\n",
    "# Setting the same scale for both axes\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Adding labels and title with improved font size\n",
    "plt.xlabel('Experimental Growth Rate', fontsize=16)\n",
    "plt.ylabel('Simulated Growth Rate', fontsize=16)\n",
    "plt.title('pFBA Simulations CHO-S WT', fontsize=20)\n",
    "\n",
    "# Adding a light grid for better readability\n",
    "plt.grid(visible=True, linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "\n",
    "# Adjusting legend for better placement and readability\n",
    "plt.legend(title='Batch - Timepoint', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=14, title_fontsize=18)\n",
    "\n",
    "# Show plot with improved layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Analyses/growth_rate_pred/bar_plot_growth_rate_wt.png', dpi=300)  # Save with higher resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeLa\n",
    "\n",
    "results = []\n",
    "\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# Adjust the lower bound values according to the experimental growth rates in order to indetify bottlenecks\n",
    "intervals = {'P0 to P2':'P2', 'P2 to P4':'P4', 'P4 to P6':'P6', 'P6 to P8':'P8', 'P8 to P12':'P12', 'P12 to P14':'P14'}\n",
    "replicates = {'U4':'Bio144', 'U5':'Bio145', 'U6':'Bio146', 'U7':'Bio147', 'U8':'Bio148'}\n",
    "\n",
    "for name,model in zela_models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    # Set lower bounds of the reactions according to the experimental data\n",
    "    for interval_key, interval_model_time in intervals.items():\n",
    "        if interval_model_time in name:  # Match model name with interval\n",
    "            for rep_key, rep_suffix in replicates.items():\n",
    "                if rep_suffix in name:  # Match replicate with model name\n",
    "                    \n",
    "                     with model as modified_model:\n",
    "\n",
    "                        # Open the bounds for the biomass reaction \n",
    "                        modified_model.reactions.biomass_cho_s.bounds = (0,1000)\n",
    "                         \n",
    "                        # Create a copy of the modified_model before making changes\n",
    "                        pre_modification_model = modified_model.copy()\n",
    " \n",
    "                        for rxn in modified_model.boundary:\n",
    "                            if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                rxn.bounds = (-1000, 1000)\n",
    "                            elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                rxn.bounds = (-0.001, 1000)\n",
    "                            elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                                rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "\n",
    "                        print(f'Calculating Growth Rate for ZeLa Condition:{rep_key,interval_key}')\n",
    "                        exp_gr = uptsec_zela['exp_growth_rate'][(rep_key,interval_key)]\n",
    "                        print(f'Experimental growth rate is: {exp_gr}')\n",
    "                        for reaction in modified_model.reactions:\n",
    "                            if reaction.id == 'EX_etoh_e': #Model creates infeasible solutions when secreting etoh\n",
    "                                continue\n",
    "                            elif reaction.id == 'ATPM': # Add ATP Maintenance Cost\n",
    "                                reaction.lower_bound = 8.5\n",
    "                            for r,v in uptsec_zela.items():\n",
    "                                if reaction.id == r:\n",
    "                                    reaction.upper_bound = 1000\n",
    "                                    reaction.lower_bound = v[(rep_key,interval_key)]\n",
    "                                    \n",
    "                        try:                         \n",
    "                            pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            pfba_atp = pfba_solution.fluxes['ATPM']\n",
    "                            print(f'Simulated ATP Maintencance is: {pfba_atp}')\n",
    "                            print(f'Simulated growth rate is: {pfba_obj}')\n",
    "                            \n",
    "                        except Infeasible:\n",
    "                            print(f'Infeasible solution for replicate {rep_key} interval {interval_key}')\n",
    "                            print('Reverting to the original model and retrying pFBA...')\n",
    "\n",
    "                            for rxn in pre_modification_model.reactions:\n",
    "                                if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                    rxn.bounds = (-1000, 1000)\n",
    "                                elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                    rxn.bounds = (-0.01, 1000)\n",
    "                                elif rxn.id == 'ATPM': # Add ATP Maintenance Cost\n",
    "                                    rxn.lower_bound = 0\n",
    "                                  \n",
    "                            pfba_solution = cobra.flux_analysis.pfba(pre_modification_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            pfba_atp = pfba_solution.fluxes['ATPM']\n",
    "                            print(f'Simulated ATP Maintencance is: {pfba_atp}')\n",
    "                            print(f'Simulated growth rate with original model is: {pfba_obj}')\n",
    "                            \n",
    "                        results.append([rep_suffix, interval_key, exp_gr, pfba_obj])\n",
    "\n",
    "                        \n",
    "# Creating a DataFrame\n",
    "df_zela = pd.DataFrame(results, columns=['Hue', 'Category', 'X Axis', 'Y Axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define markers for categories\n",
    "markers = {\n",
    "    'P0 to P2': 'p',  # Pentagons\n",
    "    'P2 to P4': 'o',  # Circles\n",
    "    'P4 to P6': 's',  # Squares\n",
    "    'P6 to P8': '^',  # Triangles\n",
    "}\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each group with different colors for hue and different shapes for categories\n",
    "hues = df_zela['Hue'].unique()\n",
    "colors = plt.get_cmap('Set2')  # Improved color palette\n",
    "\n",
    "for hue_idx, hue in enumerate(hues):\n",
    "    hue_subset = df_zela[df_zela['Hue'] == hue]\n",
    "    for category, marker in markers.items():\n",
    "        subset = hue_subset[hue_subset['Category'] == category]\n",
    "        plt.scatter(subset['X Axis'], subset['Y Axis'], \n",
    "                    label=f'{hue} - {category}', \n",
    "                    marker=marker, \n",
    "                    s=120,  # Slightly larger size for better visibility\n",
    "                    color=colors(hue_idx),\n",
    "                    edgecolor='black',  # Adding edge color for better separation\n",
    "                    alpha=0.7)  # Adding transparency to reduce clutter\n",
    "\n",
    "# Add the identity line y = x\n",
    "plt.plot([0.00, 0.055], [0.00, 0.055], color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "x_data = df_zela['X Axis']\n",
    "y_data = df_zela['Y Axis']\n",
    "# Calculate R²\n",
    "r2 = r2_score(y_data, x_data)\n",
    "# Calculate Pearson's r\n",
    "r, p_value = pearsonr(x_data, y_data)\n",
    "\n",
    "# Annotate R² and Pearson's r\n",
    "plt.text(0.005, max(df_zela['Y Axis'].max(), df_zela['X Axis'].max()) * 0.75, \n",
    "         f'R² = {r2:.3f}\\nPearson\\'s r = {r:.3f}', \n",
    "         fontsize=14, \n",
    "         bbox=dict(facecolor='white', alpha=0.6, edgecolor='gray'))\n",
    "\n",
    "# Set evenly distributed ticks from 0.01 to 0.08\n",
    "ticks = [i / 100.0 for i in range(1, 11)]\n",
    "plt.xticks(ticks=ticks, labels=[f'{i/100.0:.2f}' for i in range(1, 11)], fontsize=12)\n",
    "plt.yticks(ticks=ticks, labels=[f'{i/100.0:.2f}' for i in range(1, 11)], fontsize=12)\n",
    "\n",
    "# Set limits for both axes\n",
    "plt.xlim(0.0, 0.055)\n",
    "plt.ylim(0.0, 0.055)\n",
    "\n",
    "# Setting the same scale for both axes\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Adding labels and title with improved font size\n",
    "plt.xlabel('Experimental Growth Rate', fontsize=16)\n",
    "plt.ylabel('Simulated Growth Rate', fontsize=16)\n",
    "plt.title('pFBA Simulations CHO-ZeLa', fontsize=20)\n",
    "\n",
    "# Adding a light grid for better readability\n",
    "plt.grid(visible=True, linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "\n",
    "# Adjusting legend for better placement and readability\n",
    "plt.legend(title='Batch - Timepoint', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11, title_fontsize=13)\n",
    "\n",
    "# Show plot with improved layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Analyses/growth_rate_pred/bar_plot_growth_rate_zela.png', dpi=300)  # Save with higher resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Upatke/Secretion Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ZeLa Cells\n",
    "\n",
    "for reaction, values in uptsec_zela.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Organize data for plotting\n",
    "    batches = sorted(set(k[0] for k in values.keys()))\n",
    "    timepoints = ['P0 to P2', 'P2 to P4', 'P4 to P6', 'P6 to P8', 'P8 to P12', 'P12 to P14']\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data = [values.get((batch, tp), 0) for tp in timepoints]\n",
    "        plt.plot(timepoints, batch_data, marker='o', label=batch)\n",
    "    \n",
    "    plt.title(f'Changes in {reaction} over time')\n",
    "    plt.xlabel('Timepoints')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WT Cells\n",
    "\n",
    "for reaction, values in uptsec_wt.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Organize data for plotting\n",
    "    batches = sorted(set(k[0] for k in values.keys()))\n",
    "    timepoints = ['P0 to P2', 'P2 to P4', 'P4 to P6', 'P6 to P8', 'P8 to P12', 'P12 to P14']\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_data = [values.get((batch, tp), 0) for tp in timepoints]\n",
    "        plt.plot(timepoints, batch_data, marker='o', label=batch)\n",
    "    \n",
    "    plt.title(f'Changes in {reaction} over time')\n",
    "    plt.xlabel('Timepoints')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Flux Sampling Analysis <a id='fsa'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cobra\n",
    "from cobra import Reaction\n",
    "from cobra.io import load_json_model, read_sbml_model, load_matlab_model\n",
    "from cobra.flux_analysis.loopless import add_loopless\n",
    "from cobra.exceptions import Infeasible\n",
    "from cobra.sampling import ACHRSampler\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing context-specific models models\n",
    "model_directory = '../Data/Context_specific_models/'\n",
    "\n",
    "# Model Names\n",
    "wt_model_names = ['WT_P4_Bio141', 'WT_P4_Bio142', 'WT_P4_Bio143',\n",
    "                  'WT_P6_Bio141', 'WT_P6_Bio142', 'WT_P6_Bio143']\n",
    "\n",
    "zela_model_names = ['ZeLa_P4_Bio144', 'ZeLa_P4_Bio145', 'ZeLa_P4_Bio146', 'ZeLa_P4_Bio147', 'ZeLa_P4_Bio148',\n",
    "                    'ZeLa_P6_Bio144', 'ZeLa_P6_Bio146', 'ZeLa_P6_Bio147', 'ZeLa_P6_Bio148']\n",
    "\n",
    "# Dictionaries to store the loaded models\n",
    "wt_models = {}\n",
    "zela_models = {}\n",
    "\n",
    "# List all .mat files in the directory\n",
    "model_files = [f for f in os.listdir(model_directory) if f.endswith('.mat')]\n",
    "\n",
    "# Iterate through the .mat files and load only the models that match predefined strings\n",
    "for model_file in model_files:\n",
    "    # Full path to the model file\n",
    "    model_path = os.path.join(model_directory, model_file)\n",
    "    \n",
    "    # Check if the file corresponds to a WT model\n",
    "    for model_name in wt_model_names:\n",
    "        if model_name in model_file:\n",
    "            wt_models[model_name] = load_matlab_model(model_path)\n",
    "            wt_models[model_name].id = model_name\n",
    "            print(f\"Loaded WT model: {model_name}\")\n",
    "            break  # Stop checking other names since this file is already processed\n",
    "    \n",
    "    # Check if the file corresponds to a ZeLa model\n",
    "    for model_name in zela_model_names:\n",
    "        if model_name in model_file:\n",
    "            zela_models[model_name] = load_matlab_model(model_path)\n",
    "            zela_models[model_name].id = model_name\n",
    "            print(f\"Loaded ZeLa model: {model_name}\")\n",
    "            break  # Stop checking other names since this file is already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uptake and secretion rate dict\n",
    "\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_wt_dict.pkl', 'rb') as file:\n",
    "    uptsec_wt = pickle.load(file)\n",
    "\n",
    "with open('../Data/Uptake_Secretion_Rates/uptake_secretion_raw_zela_dict.pkl', 'rb') as file:\n",
    "    uptsec_zela = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATP generating reactions loop\n",
    "\n",
    "atp_loop_reactions = [\n",
    " 'SCP22x','TMNDNCCOAtx','OCCOAtx','r0391','BiGGRxn67','r2247','r2246','r2245','r2317','HMR_0293',\n",
    " 'HMR_7741','r1453','r1393','NICRNS','GapFill-R08726','RE2915M','HMR_3288','HMR_1325','RE2439C',\n",
    " 'r1450','RE3477C','AAPSAS','r0698','3HDH260p','HMR_3272','ACOAD183n3m','GapFill-R01463','r1468',\n",
    " 'r0655','r0603','r0541','HMR_1329','GapFill-R03599','AKGDm','r0556','OIVD1m','OIVD3m','2OXOADOXm',\n",
    " 'r0386','r0451','GLYCLm','MACACI'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check. Make sure all the exchange reactions in the experimental data are present in the model and that the models ar growing\n",
    "\n",
    "for name,model in zela_models.items():\n",
    "    print('---------------------------------------')\n",
    "    print(f'------------{model.id}------------')\n",
    "    print('')\n",
    "    print(f'Objective:{model.objective}')\n",
    "    print('')\n",
    "    print(f'GR1: {model.slim_optimize()}')\n",
    "    exchanges = list(uptsec_wt.keys())\n",
    "    \n",
    "    model_ex = [r.id for r in model.boundary]\n",
    "    model_reactions = [r.id for r in model.reactions]\n",
    "\n",
    "    # Remove ATP loop reactions\n",
    "    for atp_reaction in atp_loop_reactions:\n",
    "        if atp_reaction in model_reactions:\n",
    "            # Knock out the reaction temporarily\n",
    "            reaction = model.reactions.get_by_id(atp_reaction)\n",
    "            original_bounds = reaction.bounds  # Store the original bounds to revert later\n",
    "            reaction.knock_out()\n",
    "            \n",
    "            # Test if the model remains feasible after knocking out the reaction\n",
    "            new_gr = model.slim_optimize()\n",
    "            print(f'GR2 (After knocking out {atp_reaction}): {new_gr}')\n",
    "            \n",
    "            # Check if the model's growth rate is infeasible or close to zero\n",
    "            if math.isnan(new_gr) or new_gr < 1e-6:\n",
    "                print(f'{atp_reaction} causes infeasibility. Reverting knockout.')\n",
    "                reaction.bounds = (0,1000)  # Set reaction to irreversible\n",
    "\n",
    "                new_gr_3 = model.slim_optimize() # Optimize model with irreversible reaction\n",
    "                print(f'GR3 (After making reaction irreversible {atp_reaction}): {new_gr_3}')\n",
    "\n",
    "                # Check if the model's growth rate 3 is infeasible or close to zero\n",
    "                if math.isnan(new_gr_3) or new_gr_3 < 1e-6:\n",
    "                    print(f'{atp_reaction} in its irreversible format causes infeasibility.')\n",
    "                    reaction.bounds = original_bounds\n",
    "                else:\n",
    "                    print(f'{atp_reaction} successfully transformed to reversible without infeasibility.')\n",
    "                    \n",
    "            else:\n",
    "                print(f'{atp_reaction} successfully knocked out without infeasibility.')\n",
    "\n",
    "                \n",
    "    print('')\n",
    "    print('Missing reactions from exp data:')\n",
    "    for ex in exchanges:\n",
    "        if ex not in model_ex:\n",
    "            print(ex)\n",
    "    print('---------------------------------------')\n",
    "    print('---------------------------------------')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "\n",
    "for name,model in wt_models.items():\n",
    "    print('---------------------------------------')\n",
    "    print(f'------------{model.id}------------')\n",
    "    print('')\n",
    "    print(f'Objective:{model.objective}')\n",
    "    print('---------------------------------------')\n",
    "    print(f'GR: {model.slim_optimize()}')\n",
    "    exchanges = list(uptsec_wt.keys())\n",
    "    model_ex = [r.id for r in model.boundary]\n",
    "    model_reactions = [r.id for r in model.reactions]\n",
    "\n",
    "    # Remove ATP loop reactions\n",
    "    for atp_reaction in atp_loop_reactions:\n",
    "        if atp_reaction in model_reactions:\n",
    "            # Knock out the reaction temporarily\n",
    "            reaction = model.reactions.get_by_id(atp_reaction)\n",
    "            original_bounds = reaction.bounds  # Store the original bounds to revert later\n",
    "            reaction.knock_out()\n",
    "            \n",
    "            # Test if the model remains feasible after knocking out the reaction\n",
    "            new_gr = model.slim_optimize()\n",
    "            print(f'GR2 (After knocking out {atp_reaction}): {new_gr}')\n",
    "            \n",
    "            # Check if the model's growth rate is infeasible or close to zero\n",
    "            if math.isnan(new_gr) or new_gr < 1e-6:\n",
    "                print(f'{atp_reaction} causes infeasibility. Reverting knockout.')\n",
    "                reaction.bounds = (0,1000)  # Set reaction to irreversible\n",
    "\n",
    "                new_gr_3 = model.slim_optimize() # Optimize model with irreversible reaction\n",
    "                print(f'GR3 (After making reaction irreversible {atp_reaction}): {new_gr_3}')\n",
    "\n",
    "                # Check if the model's growth rate 3 is infeasible or close to zero\n",
    "                if math.isnan(new_gr_3) or new_gr_3 < 1e-6:\n",
    "                    print(f'{atp_reaction} in its irreversible format causes infeasibility.')\n",
    "                    reaction.bounds = original_bounds\n",
    "                else:\n",
    "                    print(f'{atp_reaction} successfully transformed to reversible without infeasibility.')\n",
    "                    \n",
    "            else:\n",
    "                print(f'{atp_reaction} successfully knocked out without infeasibility.')\n",
    "    \n",
    "    for ex in exchanges:\n",
    "        if ex not in model_ex:\n",
    "            print(ex)\n",
    "    print('---------------------------------------')\n",
    "    print('---------------------------------------')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flux Sampling Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WT\n",
    "\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# Adjust the lower bound values according to the experimental growth rates in order to indetify bottlenecks\n",
    "intervals = {'P2 to P4':'P4', 'P4 to P6':'P6'}\n",
    "replicates = {'U1':'Bio141', 'U2':'Bio142', 'U3':'Bio143'}\n",
    "\n",
    "for name,model in wt_models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    # Set lower bounds of the reactions according to the experimental data\n",
    "    for interval_key, interval_model_time in intervals.items():\n",
    "        if interval_model_time in name:  # Match model name with interval\n",
    "            for rep_key, rep_suffix in replicates.items():\n",
    "                if rep_suffix in name:  # Match replicate with model name\n",
    "\n",
    "                     with model as modified_model:\n",
    "                         \n",
    "                        for rxn in modified_model.boundary:\n",
    "                            # Keep boundaries open for essential metabolites\n",
    "                            if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                rxn.bounds = (-1000, 1000)\n",
    "                            elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                rxn.bounds = (-0.001, 1000)\n",
    "                            elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                                rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "                        \n",
    "                        print(f'Calculating Growth Rate for WT Condition:{rep_key,interval_key}')\n",
    "                        exp_gr = uptsec_wt['exp_growth_rate'][(rep_key,interval_key)]\n",
    "                        print(f'Experimental growth rate is: {exp_gr}')\n",
    "                        for reaction in modified_model.reactions:\n",
    "                            if reaction.id == 'EX_etoh_e': #Model creates infeasible solutions when secreting etoh\n",
    "                                continue\n",
    "                            for r,v in uptsec_wt.items():\n",
    "                                if reaction.id == r:\n",
    "                                    reaction.upper_bound = 1000\n",
    "                                    reaction.lower_bound = v[(rep_key,interval_key)]\n",
    "                                    \n",
    "                        try:\n",
    "                            pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            print(f'Simulated growth rate is: {pfba_obj}')\n",
    "            \n",
    "                            # If pFBA succeeds, proceed to sampling\n",
    "                            pfba_model_to_sample = modified_model\n",
    "\n",
    "                            # Perform flux sampling using ACHRSampler\n",
    "                            print(\"Performing flux sampling...\")\n",
    "                            achr_sampler = ACHRSampler(pfba_model_to_sample)\n",
    "                            samples = 10000 \n",
    "                            sampled_fluxes = achr_sampler.sample(samples)\n",
    "                            \n",
    "                            # Validate and save the sampled fluxes\n",
    "                            if 'v' not in achr_sampler.validate(sampled_fluxes).flatten():\n",
    "                                print(f\"Error in sampling for {rep_key}, {interval_key}\")\n",
    "                            else:\n",
    "                                f_name = f\"../Analyses/flux_sampling/Sampling_{name}_{samples}.pkl\"\n",
    "                                with open(f_name, 'wb') as file:\n",
    "                                    pickle.dump(sampled_fluxes, file)\n",
    "                            \n",
    "                        except Infeasible:\n",
    "                            print(f'Infeasible solution for replicate {rep_key} interval {interval_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeLa\n",
    "\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# Adjust the lower bound values according to the experimental growth rates in order to indetify bottlenecks\n",
    "intervals = {'P2 to P4':'P4', 'P4 to P6':'P6'}\n",
    "replicates = {'U4':'Bio144', 'U5':'Bio145', 'U6':'Bio146', 'U7':'Bio147', 'U8':'Bio148'}\n",
    "\n",
    "for name,model in zela_models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    # Set lower bounds of the reactions according to the experimental data\n",
    "    for interval_key, interval_model_time in intervals.items():\n",
    "        if interval_model_time in name:  # Match model name with interval\n",
    "            for rep_key, rep_suffix in replicates.items():\n",
    "                if rep_suffix in name:  # Match replicate with model name\n",
    "                    \n",
    "                     with model as modified_model:\n",
    "                         \n",
    "                        for rxn in modified_model.boundary:\n",
    "                            # Keep boundaries open for essential metabolites\n",
    "                            if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                rxn.bounds = (-1000, 1000)\n",
    "                            elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                rxn.bounds = (-0.001, 1000)\n",
    "                            elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                                rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "                        \n",
    "                        print(f'Calculating Growth Rate for WT Condition:{rep_key,interval_key}')\n",
    "                        exp_gr = uptsec_zela['exp_growth_rate'][(rep_key,interval_key)]\n",
    "                        print(f'Experimental growth rate is: {exp_gr}')\n",
    "                        for reaction in modified_model.reactions:\n",
    "                            if reaction.id == 'EX_etoh_e': #Model creates infeasible solutions when secreting etoh\n",
    "                                continue\n",
    "                            for r,v in uptsec_zela.items():\n",
    "                                if reaction.id == r:\n",
    "                                    reaction.upper_bound = 1000\n",
    "                                    reaction.lower_bound = v[(rep_key,interval_key)]\n",
    "                                    \n",
    "                        try:\n",
    "                            pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            print(f'Simulated growth rate is: {pfba_obj}')\n",
    "            \n",
    "                            # If pFBA succeeds, proceed to sampling\n",
    "                            pfba_model_to_sample = modified_model\n",
    "                            \n",
    "                            # Perform flux sampling using ACHRSampler\n",
    "                            print(\"Performing flux sampling...\")\n",
    "                            achr_sampler = ACHRSampler(pfba_model_to_sample)\n",
    "                            samples = 10000 \n",
    "                            sampled_fluxes = achr_sampler.sample(samples)\n",
    "                            \n",
    "                            # Validate and save the sampled fluxes\n",
    "                            if 'v' not in achr_sampler.validate(sampled_fluxes).flatten():\n",
    "                                print(f\"Error in sampling for {rep_key}, {interval_key}\")\n",
    "                            else:\n",
    "                                f_name = f\"../Analyses/flux_sampling/Sampling_{name}_{samples}.pkl\"\n",
    "                                with open(f_name, 'wb') as file:\n",
    "                                    pickle.dump(sampled_fluxes, file)\n",
    "                            \n",
    "                        except Infeasible:\n",
    "                            print(f'Infeasible solution for replicate {rep_key} interval {interval_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsb_sampling(model, n_samples, steps_per_point):\n",
    "    current_solution = model.optimize()\n",
    "    if current_solution.status != \"optimal\":\n",
    "        raise ValueError(\"Initial optimization did not yield an optimal solution.\")\n",
    "\n",
    "    current_fluxes = np.array([rxn.flux for rxn in model.reactions])\n",
    "    n_rxns = len(model.reactions)\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        direction = np.random.randn(n_rxns)\n",
    "        direction /= np.linalg.norm(direction)\n",
    "\n",
    "        step_size = random.uniform(-1, 1)  # Allow step size to be negative or positive\n",
    "        proposed_fluxes = current_fluxes + step_size * direction\n",
    "\n",
    "        for i, rxn in enumerate(model.reactions):\n",
    "            rxn.lower_bound = min(proposed_fluxes[i], rxn.upper_bound)\n",
    "            rxn.upper_bound = max(proposed_fluxes[i], rxn.lower_bound)\n",
    "\n",
    "        solution = model.optimize()\n",
    "        if solution.status == \"optimal\":\n",
    "            samples.append({rxn.id: solution.fluxes[rxn.id] for rxn in model.reactions})\n",
    "            current_fluxes = np.array([rxn.flux for rxn in model.reactions])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WT\n",
    "\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# Adjust the lower bound values according to the experimental growth rates in order to indetify bottlenecks\n",
    "intervals = {'P2 to P4':'P4', 'P4 to P6':'P6'}\n",
    "replicates = {'U1':'Bio141', 'U2':'Bio142', 'U3':'Bio143'}\n",
    "\n",
    "for name,model in wt_models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    # Set lower bounds of the reactions according to the experimental data\n",
    "    for interval_key, interval_model_time in intervals.items():\n",
    "        if interval_model_time in name:  # Match model name with interval\n",
    "            for rep_key, rep_suffix in replicates.items():\n",
    "                if rep_suffix in name:  # Match replicate with model name\n",
    "\n",
    "                     with model as modified_model:\n",
    "                         \n",
    "                        for rxn in modified_model.boundary:\n",
    "                            # Keep boundaries open for essential metabolites\n",
    "                            if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                rxn.bounds = (-1000, 1000)\n",
    "                            elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                rxn.bounds = (-0.001, 1000)\n",
    "                            elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                                rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "                        \n",
    "                        print(f'Calculating Growth Rate for WT Condition:{rep_key,interval_key}')\n",
    "                        exp_gr = uptsec_wt['exp_growth_rate'][(rep_key,interval_key)]\n",
    "                        print(f'Experimental growth rate is: {exp_gr}')\n",
    "                        for reaction in modified_model.reactions:\n",
    "                            if reaction.id == 'EX_etoh_e': #Model creates infeasible solutions when secreting etoh\n",
    "                                continue\n",
    "                            for r,v in uptsec_wt.items():\n",
    "                                if reaction.id == r:\n",
    "                                    reaction.upper_bound = 1000\n",
    "                                    reaction.lower_bound = v[(rep_key,interval_key)]\n",
    "\n",
    "                        try:\n",
    "                            pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            print(f'Simulated growth rate is: {pfba_obj}')\n",
    "\n",
    "                            # If pFBA succeeds, proceed to sampling\n",
    "                            pfba_model_to_sample = modified_model\n",
    "\n",
    "                            # **Adjust reaction bounds to limit flux values before sampling**\n",
    "                            for reaction in pfba_model_to_sample.reactions:\n",
    "                                reaction.upper_bound = min(reaction.upper_bound, 1000)\n",
    "                                reaction.lower_bound = max(reaction.lower_bound, -1000)\n",
    "\n",
    "                            # Perform flux sampling using ADSB\n",
    "                            print(\"Performing ADSB flux sampling...\")\n",
    "                            adsb_samples = adsb_sampling(pfba_model_to_sample, n_samples=100, steps_per_point=10)\n",
    "\n",
    "                            # Convert samples to DataFrame for filtering\n",
    "                            sampled_fluxes_df = pd.DataFrame(adsb_samples)\n",
    "\n",
    "                            # Filter out samples where any flux value exceeds 100 or is less than -100\n",
    "                            filtered_sampled_fluxes = sampled_fluxes_df[(sampled_fluxes_df.abs() <= 100).all(axis=1)]\n",
    "                            num_filtered_samples = len(filtered_sampled_fluxes)\n",
    "                            print(f\"{num_filtered_samples} samples remain after filtering out samples with flux values exceeding 100 or less than -100.\")\n",
    "\n",
    "                            if num_filtered_samples == 0:\n",
    "                                print(\"No samples remain after filtering. Consider adjusting your threshold or generating more samples.\")\n",
    "                            else:\n",
    "                                # Identify reactions with flux values exceeding the threshold\n",
    "                                for index, row in sampled_fluxes_df.iterrows():\n",
    "                                    exceeding_fluxes = row[(row.abs() > 100)].to_dict()\n",
    "                                    if exceeding_fluxes:\n",
    "                                        print(f\"Sample {index} has reactions exceeding threshold: {exceeding_fluxes}\")\n",
    "\n",
    "                                # Save the filtered sampled fluxes\n",
    "                                f_name = f\"../Analyses/flux_sampling/Sampling_{name}_filtered.pkl\"\n",
    "                                with open(f_name, 'wb') as file:\n",
    "                                    pickle.dump(filtered_sampled_fluxes, file)\n",
    "                                print(f\"Filtered flux sampling data saved to {f_name}.\")\n",
    "\n",
    "                                # Optional: Print statistics of the filtered data\n",
    "                                print(\"Flux value statistics after filtering:\")\n",
    "                                print(filtered_sampled_fluxes.describe())\n",
    "\n",
    "                        except Infeasible:\n",
    "                            print(f'Infeasible solution for replicate {rep_key} interval {interval_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WT\n",
    "\n",
    "objective = 'biomass_cho_s'\n",
    "\n",
    "# Adjust the lower bound values according to the experimental growth rates in order to identify bottlenecks\n",
    "intervals = {'P2 to P4': 'P4', 'P4 to P6': 'P6'}\n",
    "replicates = {'U1': 'Bio141', 'U2': 'Bio142', 'U3': 'Bio143'}\n",
    "\n",
    "for name, model in wt_models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    # Set lower bounds of the reactions according to the experimental data\n",
    "    for interval_key, interval_model_time in intervals.items():\n",
    "        if interval_model_time in name:  # Match model name with interval\n",
    "            for rep_key, rep_suffix in replicates.items():\n",
    "                if rep_suffix in name:  # Match replicate with model name\n",
    "\n",
    "                    with model as modified_model:\n",
    "\n",
    "                        for rxn in modified_model.boundary:\n",
    "                            # Keep boundaries open for essential metabolites\n",
    "                            if rxn.id in ['EX_h2o_e', 'EX_h_e', 'EX_o2_e', 'EX_hco3_e', 'EX_so4_e', 'EX_pi_e']:\n",
    "                                rxn.bounds = (-1000, 1000)\n",
    "                            elif rxn.id in ['SK_Asn_X_Ser_Thr_r', 'SK_Tyr_ggn_c', 'SK_Ser_Thr_g', 'SK_pre_prot_r']:\n",
    "                                rxn.bounds = (-0.001, 1000)\n",
    "                            elif rxn.id.startswith((\"EX_\", \"SK_\", \"DM_\")):\n",
    "                                rxn.bounds = (0, 1000)  # Close uptake rates for others\n",
    "\n",
    "                        print(f'Calculating Growth Rate for WT Condition: {rep_key}, {interval_key}')\n",
    "                        exp_gr = uptsec_wt['exp_growth_rate'][(rep_key, interval_key)]\n",
    "                        print(f'Experimental growth rate is: {exp_gr}')\n",
    "                        for reaction in modified_model.reactions:\n",
    "                            if reaction.id == 'EX_etoh_e':  # Model creates infeasible solutions when secreting etoh\n",
    "                                continue\n",
    "                            for r, v in uptsec_wt.items():\n",
    "                                if reaction.id == r:\n",
    "                                    reaction.upper_bound = 1000\n",
    "                                    reaction.lower_bound = v[(rep_key, interval_key)]\n",
    "\n",
    "                        try:\n",
    "                            pfba_solution = cobra.flux_analysis.pfba(modified_model)\n",
    "                            pfba_obj = pfba_solution.fluxes[objective]\n",
    "                            print(f'Simulated growth rate is: {pfba_obj}')\n",
    "\n",
    "                            # If pFBA succeeds, proceed to sampling\n",
    "                            pfba_model_to_sample = modified_model\n",
    "\n",
    "                            # **Adjust reaction bounds to limit flux values before sampling**\n",
    "                            for reaction in pfba_model_to_sample.reactions:\n",
    "                                reaction.upper_bound = min(reaction.upper_bound, 1000)\n",
    "                                reaction.lower_bound = max(reaction.lower_bound, -1000)\n",
    "\n",
    "                            # Perform flux sampling using ADSB\n",
    "                            print(\"Performing ADSB flux sampling...\")\n",
    "                            adsb_samples = adsb_sampling(pfba_model_to_sample, n_samples=100, steps_per_point=0.0001)\n",
    "\n",
    "                            # Convert samples to DataFrame\n",
    "                            sampled_fluxes_df = pd.DataFrame(adsb_samples)\n",
    "                            # Save the sampled fluxes\n",
    "                            f_name = f\"../Analyses/flux_sampling/Sampling_{name}_ADSB_100.pkl\"\n",
    "                            with open(f_name, 'wb') as file:\n",
    "                                pickle.dump(sampled_fluxes_df, file)\n",
    "                            print(f\"Flux sampling data saved to {f_name}.\")\n",
    "\n",
    "                        except Infeasible:\n",
    "                            print(f'Infeasible solution for replicate {rep_key} interval {interval_key}')\n",
    "\n",
    "# Load the saved sampled fluxes and visualize\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved sampled fluxes\n",
    "file_name = \"../Analyses/flux_sampling/Sampling_WT_P6_Bio141_ADSB_100.pkl\"\n",
    "with open(file_name, 'rb') as file:\n",
    "    loaded_fluxes_df = pickle.load(file)\n",
    "\n",
    "# Visualization of the loaded sampled fluxes\n",
    "if not loaded_fluxes_df.empty:\n",
    "    num_columns = len(loaded_fluxes_df.columns)\n",
    "    nrows = 6\n",
    "    ncols = 6\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 12))\n",
    "\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, column in enumerate(loaded_fluxes_df.columns[:nrows * ncols]):\n",
    "        ax = axes[i]\n",
    "        sns.kdeplot(loaded_fluxes_df[column], ax=ax, color='skyblue', fill=True, label='WT', zorder=1, common_norm=False)\n",
    "        ax.set_title(f'{column}', fontsize=10)\n",
    "        ax.set_xlabel('Flux Value', fontsize=8)\n",
    "        ax.set_ylabel('Density', fontsize=8)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_fluxes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Option #1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from cobra.exceptions import Infeasible\n",
    "\n",
    "def adsb_sampling(model, n_samples=100, steps_per_point=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Adaptive Directions Sampling on a Box (ADSB)\n",
    "\n",
    "    Uses ADSB to generate a uniform random sample from the loopless flux solution space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : cobra.Model\n",
    "        The metabolic model to sample from.\n",
    "    n_samples : int\n",
    "        Total number of samples.\n",
    "    steps_per_point : int\n",
    "        Thinning or number of steps per effective point.\n",
    "    verbose : bool\n",
    "        Verbosity flag.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    samples : pandas.DataFrame\n",
    "        DataFrame with sampled fluxes (numReactions x n_samples)\n",
    "    \"\"\"\n",
    "    # Import necessary modules\n",
    "    import cobra\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    # Get the number of reactions\n",
    "    numReactions = len(model.reactions)\n",
    "\n",
    "    # Get lower and upper bounds\n",
    "    lb = np.array([rxn.lower_bound for rxn in model.reactions])\n",
    "    ub = np.array([rxn.upper_bound for rxn in model.reactions])\n",
    "\n",
    "    # Initial feasible point using pFBA\n",
    "    try:\n",
    "        solution = cobra.flux_analysis.pfba(model)\n",
    "        x0 = solution.fluxes.values\n",
    "    except Infeasible:\n",
    "        raise Exception(\"Model is infeasible and cannot generate an initial point.\")\n",
    "\n",
    "    # Parameters\n",
    "    numChains = 1  # You can adjust the number of chains if needed\n",
    "    pointsPerChain = n_samples\n",
    "    numSamples = n_samples\n",
    "    nTimes = steps_per_point\n",
    "    ptarget = 0.99\n",
    "    bTol = 1e-8\n",
    "    uTol = 1e-8\n",
    "\n",
    "    # Function to keep points within bounds\n",
    "    def keep_within_bounds(point):\n",
    "        return np.minimum(np.maximum(point, lb), ub)\n",
    "\n",
    "    # Function to check feasibility (placeholder, needs proper implementation)\n",
    "    def is_feasible(point):\n",
    "        return True  # Replace with actual feasibility check if loopless sampling is needed\n",
    "\n",
    "    # Initialize sampling variables\n",
    "    nDim = pointsPerChain\n",
    "    points = np.zeros((numReactions, nDim, numChains))\n",
    "    points[:, 0, :] = x0.reshape(-1, 1)\n",
    "    udir = np.zeros((numReactions, numChains))  # Corrected initialization\n",
    "    currPoint = np.zeros((numReactions, numChains))\n",
    "    nextPoint = np.zeros((numReactions, numChains))\n",
    "\n",
    "    indexes = np.tile(np.arange(nDim), (numChains, 1))\n",
    "    Lcord = np.zeros(numChains)\n",
    "    steps = np.zeros(numChains)\n",
    "    posStep = np.zeros(numChains)\n",
    "    negStep = np.zeros(numChains)\n",
    "\n",
    "    # Start clock and sampler\n",
    "    iterSample = 0\n",
    "    t0 = time.time()\n",
    "    nSteps = int(-np.log(1 - ptarget) * nTimes * nDim)\n",
    "    nStepsArray = np.round(np.linspace(0, nSteps, num=11)).astype(int)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------------------------')\n",
    "        print('%Prog \\t Time \\t Time left')\n",
    "        print('---------------------------')\n",
    "\n",
    "    idx_prev = indexes.copy()  # Initialize idx_prev\n",
    "\n",
    "    while iterSample <= nSteps:\n",
    "        iterSample += 1\n",
    "\n",
    "        # Print progress information\n",
    "        if verbose and iterSample in nStepsArray and iterSample > 1:\n",
    "            timeElapsed = (time.time() - t0) / 60\n",
    "            timePerStep = timeElapsed / iterSample\n",
    "            timeLeft = (nSteps - iterSample) * timePerStep\n",
    "            progress = round(100 * iterSample / nSteps)\n",
    "            print(f'{progress}%\\t{timeElapsed:.2f}\\t{timeLeft:.2f}')\n",
    "\n",
    "        # Pre-allocate variables\n",
    "        flags = np.ones(numChains, dtype=bool)\n",
    "\n",
    "        # Sample feasible direction\n",
    "        while np.any(flags):\n",
    "            idx_next = indexes.copy()\n",
    "\n",
    "            # Figure out the true max & min step sizes\n",
    "            for ix in np.where(flags)[0]:\n",
    "                # Update points based on the previous iteration\n",
    "                if iterSample > 1:\n",
    "                    points[:, idx_prev[ix, -1], ix] = nextPoint[:, ix]\n",
    "\n",
    "                # Sample three indexes without replacement\n",
    "                idx1 = np.random.randint(nDim)\n",
    "                idx_next[ix, [idx1, -1]] = idx_next[ix, [-1, idx1]]\n",
    "                idx2 = np.random.randint(nDim - 1)\n",
    "                idx_next[ix, [idx2, -2]] = idx_next[ix, [-2, idx2]]\n",
    "                idx3 = np.random.randint(nDim - 2)\n",
    "                idx_next[ix, [idx3, -3]] = idx_next[ix, [-3, idx3]]\n",
    "\n",
    "                # Define current point and direction of movement\n",
    "                currPoint[:, ix] = points[:, idx_next[ix, -1], ix]\n",
    "                udir[:, ix] = points[:, idx_next[ix, -3], ix] - points[:, idx_next[ix, -2], ix]\n",
    "\n",
    "                if np.all(udir[:, ix] == 0):\n",
    "                    continue\n",
    "\n",
    "                # Normalize udir\n",
    "                norm_udir = norm(udir[:, ix])\n",
    "                if norm_udir == 0:\n",
    "                    continue\n",
    "                udir[:, ix] /= norm_udir\n",
    "\n",
    "                # Figure out positive and negative directions, and maximum distance to boundaries\n",
    "                posDir = udir[:, ix] > uTol\n",
    "                negDir = udir[:, ix] < -uTol\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    posStepTemp = (ub - currPoint[:, ix]) / udir[:, ix]\n",
    "                    negStepTemp = (lb - currPoint[:, ix]) / udir[:, ix]\n",
    "\n",
    "                posValues = np.concatenate([posStepTemp[posDir], negStepTemp[negDir]])\n",
    "                negValues = np.concatenate([negStepTemp[posDir], posStepTemp[negDir]])\n",
    "\n",
    "                posStep[ix] = np.min(posValues) if posValues.size > 0 else np.inf\n",
    "                negStep[ix] = np.max(negValues) if negValues.size > 0 else -np.inf\n",
    "\n",
    "            # Draw new direction if too close to the boundaries\n",
    "            Lcord[flags] = posStep[flags] - negStep[flags]\n",
    "            flags[flags] = (Lcord[flags] < bTol) | (posStep[flags] < 0) | (negStep[flags] > 0)\n",
    "\n",
    "        # Sample from the shrinking interval as in slice sampling\n",
    "        flags = np.ones(numChains, dtype=bool)\n",
    "        while np.any(flags):\n",
    "            # Perform a random step\n",
    "            steps[flags] = negStep[flags] + Lcord[flags] * np.random.rand(np.sum(flags))\n",
    "            nextPoint[:, flags] = currPoint[:, flags] + udir[:, flags] * steps[flags]\n",
    "            nextPoint[:, flags] = keep_within_bounds(nextPoint[:, flags])\n",
    "\n",
    "            # Feasibility check (if loopless sampling is required)\n",
    "            if is_feasible(nextPoint[:, flags]):\n",
    "                break\n",
    "            else:\n",
    "                # Shrink hypercube if necessary\n",
    "                posSign = steps[flags] > 0\n",
    "                posStep[flags] = np.where(posSign, steps[flags], posStep[flags])\n",
    "                negStep[flags] = np.where(~posSign, steps[flags], negStep[flags])\n",
    "                Lcord[flags] = posStep[flags] - negStep[flags]\n",
    "                flags[flags] = Lcord[flags] > bTol\n",
    "\n",
    "        # Reset position of the next point to the current point for zero-length steps\n",
    "        fixed_point = Lcord < bTol\n",
    "        nextPoint[:, fixed_point] = currPoint[:, fixed_point]\n",
    "\n",
    "        # Save indexes to change in the next iteration\n",
    "        idx_prev = idx_next.copy()\n",
    "\n",
    "        # Store the sample if needed\n",
    "        if iterSample % steps_per_point == 0:\n",
    "            sample_idx = iterSample // steps_per_point - 1\n",
    "            if sample_idx < n_samples:\n",
    "                points[:, sample_idx, :] = nextPoint\n",
    "\n",
    "    samplingTime = (time.time() - t0) / 60  # Time in minutes\n",
    "\n",
    "    # Reshape the samples to return as DataFrame\n",
    "    samples = pd.DataFrame(points[:, :n_samples, 0], index=[rxn.id for rxn in model.reactions])\n",
    "\n",
    "    return samples.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Option #2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from cobra.exceptions import Infeasible\n",
    "\n",
    "def adsb_sampling(model, n_samples=100, steps_per_point=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Adaptive Directions Sampling on a Box (ADSB)\n",
    "    \n",
    "    Uses ADSB to generate a uniform random sample from the loopless flux solution space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : cobra.Model\n",
    "        The metabolic model to sample from.\n",
    "    n_samples : int\n",
    "        Total number of samples.\n",
    "    steps_per_point : int\n",
    "        Thinning or number of steps per effective point.\n",
    "    verbose : bool\n",
    "        Verbosity flag.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    samples : pandas.DataFrame\n",
    "        DataFrame with sampled fluxes (numReactions x n_samples)\n",
    "    \"\"\"\n",
    "    # Import necessary modules\n",
    "    import cobra\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    # Get the number of reactions\n",
    "    numReactions = len(model.reactions)\n",
    "\n",
    "    # Get lower and upper bounds\n",
    "    lb = np.array([rxn.lower_bound for rxn in model.reactions])\n",
    "    ub = np.array([rxn.upper_bound for rxn in model.reactions])\n",
    "\n",
    "    # Initial feasible point using pFBA\n",
    "    try:\n",
    "        solution = cobra.flux_analysis.pfba(model)\n",
    "        x0 = solution.fluxes.values\n",
    "    except Infeasible:\n",
    "        raise Exception(\"Model is infeasible and cannot generate an initial point.\")\n",
    "\n",
    "    # Parameters\n",
    "    numChains = 1  # You can adjust the number of chains if needed\n",
    "    numSamples = n_samples\n",
    "    nTimes = steps_per_point\n",
    "    ptarget = 0.99\n",
    "    bTol = 1e-8\n",
    "    uTol = 1e-8\n",
    "\n",
    "    # Precompute the number of steps to achieve the desired target\n",
    "    nSteps = int(-np.log(1 - ptarget) * nTimes * numReactions)\n",
    "\n",
    "    # Initialize sampling variables\n",
    "    currPoint = x0.copy()\n",
    "    samples = np.zeros((numSamples, numReactions))\n",
    "    samples[0, :] = x0\n",
    "\n",
    "    # Start clock\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Precompute random directions and steps\n",
    "    # This reduces the number of random number generations inside loops\n",
    "    total_iterations = nSteps\n",
    "    random_directions = np.random.randn(total_iterations, numReactions)\n",
    "    random_steps = np.random.rand(total_iterations)\n",
    "\n",
    "    # Loop over the number of steps\n",
    "    sample_idx = 1  # Start from 1 since the initial point is already stored\n",
    "    for iterSample in range(total_iterations):\n",
    "        # Generate a random direction and normalize it\n",
    "        udir = random_directions[iterSample]\n",
    "        norm_udir = norm(udir)\n",
    "        if norm_udir == 0:\n",
    "            continue\n",
    "        udir /= norm_udir\n",
    "\n",
    "        # Compute maximum positive and negative steps\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            posStepTemp = (ub - currPoint) / udir\n",
    "            negStepTemp = (lb - currPoint) / udir\n",
    "\n",
    "        posStepTemp[udir <= uTol] = np.inf\n",
    "        negStepTemp[udir >= -uTol] = -np.inf\n",
    "\n",
    "        posStep = np.min(posStepTemp)\n",
    "        negStep = np.max(negStepTemp)\n",
    "\n",
    "        Lcord = posStep - negStep\n",
    "\n",
    "        if Lcord < bTol or posStep < 0 or negStep > 0:\n",
    "            continue  # Skip if the step size is too small or invalid\n",
    "\n",
    "        # Perform a random step within the allowable range\n",
    "        step_size = negStep + Lcord * random_steps[iterSample]\n",
    "        nextPoint = currPoint + udir * step_size\n",
    "        nextPoint = np.minimum(np.maximum(nextPoint, lb), ub)  # Ensure within bounds\n",
    "\n",
    "        # Feasibility check (if loopless sampling is required)\n",
    "        # Since is_feasible always returns True, we skip the check\n",
    "        currPoint = nextPoint\n",
    "\n",
    "        # Store the sample every 'steps_per_point' iterations\n",
    "        if (iterSample + 1) % steps_per_point == 0:\n",
    "            if sample_idx < numSamples:\n",
    "                samples[sample_idx, :] = currPoint\n",
    "                sample_idx += 1\n",
    "            else:\n",
    "                break  # Collected required number of samples\n",
    "\n",
    "        # Optionally print progress\n",
    "        if verbose and (iterSample + 1) % (total_iterations // 10) == 0:\n",
    "            timeElapsed = (time.time() - t0) / 60\n",
    "            progress = 100 * (iterSample + 1) / total_iterations\n",
    "            print(f'{progress:.0f}% completed. Time elapsed: {timeElapsed:.2f} minutes.')\n",
    "\n",
    "    # Convert samples to DataFrame\n",
    "    samples_df = pd.DataFrame(samples[:sample_idx, :], columns=[rxn.id for rxn in model.reactions])\n",
    "\n",
    "    return samples_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsb_samples = adsb_sampling(pfba_model_to_sample, n_samples=1000, steps_per_point=5)\n",
    "print(adsb_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform flux sampling using ADSB\n",
    "print(\"Performing ADSB flux sampling...\")\n",
    "adsb_samples = adsb_sampling(pfba_model_to_sample, n_samples=100, steps_per_point=1)\n",
    "\n",
    "# Convert samples to DataFrame\n",
    "sampled_fluxes_df = adsb_samples\n",
    "\n",
    "# Save the sampled fluxes\n",
    "f_name = f\"../Analyses/flux_sampling/Sampling_{name}_ADSB_{len(sampled_fluxes_df)}.pkl\"\n",
    "with open(f_name, 'wb') as file:\n",
    "    pickle.dump(sampled_fluxes_df, file)\n",
    "print(f\"Flux sampling data saved to {f_name}.\")\n",
    "\n",
    "# Optional: Print statistics of the data\n",
    "if not sampled_fluxes_df.empty:\n",
    "    print(\"Flux value statistics:\")\n",
    "    print(sampled_fluxes_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pathway Enrichment Analysis <a id='pea'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1\n",
    "# Use the fluxes from the growth rate calculations to filter reactions with fluxes\n",
    "# Define a cut off for the flux ??? # Plot to visualization of the flux distribution\n",
    "# Run Flux Enrichment Analysis on those reactions to see pathways enriched when optimized by biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2\n",
    "# Transcriptomic data from cell batches / biorreactiors\n",
    "# Overlay this into the recons and extract the reactions associated to the genes\n",
    "# Extract a reaction vector / \n",
    "# Run Flux Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def flux_enrichment_analysis(generic_model, rxn_list, attribute='subsystem'):\n",
    "    \n",
    "    # Check if the attribute exists in the first reaction as a proxy for all\n",
    "    if not hasattr(model.reactions[0], attribute):\n",
    "        raise ValueError(f'Attribute {attribute} not found in model reactions')\n",
    "\n",
    "    # Extract attribute information for all reactions\n",
    "    attribute_values = [getattr(rxn, attribute, 'None') for rxn in model.reactions]\n",
    "    unique_attributes = set(attribute_values)\n",
    "    \n",
    "    # Count occurrences in the model and in the reaction set\n",
    "    model_counts = {attr: attribute_values.count(attr) for attr in unique_attributes}\n",
    "    rxn_set_counts = {attr: 0 for attr in unique_attributes}\n",
    "    for rxn in rxn_list:\n",
    "        rxn_attr = getattr(model.reactions.get_by_id(rxn), attribute, 'None')\n",
    "        rxn_set_counts[rxn_attr] += 1\n",
    "\n",
    "    # Calculate p-values using hypergeometric test\n",
    "    M = len(model.reactions)  # Total number of reactions\n",
    "    n = len(rxn_list)  # Size of reaction set\n",
    "    p_values = []\n",
    "    for attr in unique_attributes:\n",
    "        N = model_counts[attr]  # Total reactions in group\n",
    "        x = rxn_set_counts[attr]  # Reactions in group and in set\n",
    "        p_value = hypergeom.sf(x-1, M, N, n)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    # Adjust p-values for multiple testing\n",
    "    _, adj_p_values, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "\n",
    "    # Compile results\n",
    "    results = pd.DataFrame({\n",
    "        'Group': list(unique_attributes),\n",
    "        'P-value': p_values,\n",
    "        'Adjusted P-value': adj_p_values,\n",
    "        'Enriched set size': [rxn_set_counts[attr] for attr in unique_attributes],\n",
    "        'Total set size': [model_counts[attr] for attr in unique_attributes],\n",
    "    }).sort_values(by='Adjusted P-value')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: FEA on reactions active during growth rate optimization\n",
    "Use the fluxes from the growth rate calculations to filter reactions with fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a vector of active reactions for each one of the conditions\n",
    "\n",
    "active_reactions = []\n",
    "\n",
    "for sol in pfba_solutions_fluxes:\n",
    "    rxns_fluxes = []\n",
    "    for n,f in sol['fluxes'].items():\n",
    "        if f != 0:\n",
    "            rxns_fluxes.append(n)\n",
    "    \n",
    "    active_reactions.append({\n",
    "        \"batch\": sol['model'],\n",
    "        \"condition\": sol['condition'],\n",
    "        \"flux_vector\": rxns_fluxes\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generic model\n",
    "model = load_json_model('iCHO3595_unblocked.json')\n",
    "\n",
    "# Add atp demand reaction\n",
    "DM_atp = Reaction('DM_atp_c')\n",
    "model.add_reactions([DM_atp])\n",
    "model.reactions.DM_atp_c.build_reaction_from_string('atp_c -->')\n",
    "model.reactions.DM_atp_c.bounds = (0.01, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate FEA results for each one of the vectors generated for each condition\n",
    "\n",
    "fea_results = []\n",
    "\n",
    "# Load generic model\n",
    "#model = load_json_model('iCHO3595_unblocked.json')\n",
    "\n",
    "for fluxes in active_reactions:\n",
    "    results = flux_enrichment_analysis(model, fluxes['flux_vector'], 'subsystem')\n",
    "    fea_results.append({\n",
    "        \"Batch\": fluxes['batch'],\n",
    "        \"Condition\": fluxes['condition'],\n",
    "        \"Results\": results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform p-values to -log10\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for result in fea_results:\n",
    "    batch = result['Batch']\n",
    "    temp_df = result['Results'][['Group', 'P-value']].copy()\n",
    "    temp_df.columns = ['Group', f'P-value_{batch}']\n",
    "    if all_data.empty:\n",
    "        all_data = temp_df\n",
    "    else:\n",
    "        all_data = pd.merge(all_data, temp_df, on='Group', how='outer')\n",
    "\n",
    "# Remove groups with a 0 value in all conditions (assuming a '0' value indicates non-significance)\n",
    "significant_filter = (all_data.drop(columns='Group') < 0.05).any(axis=1)\n",
    "filtered_data = all_data[significant_filter]\n",
    "filtered_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Transform the p-values\n",
    "for col in filtered_data.columns:\n",
    "    if col.startswith('P-value'):\n",
    "        filtered_data[f'-log10_{col}'] = -np.log10(filtered_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Extract relevant columns for heatmap\n",
    "heatmap_data = filtered_data[[col for col in filtered_data.columns if col.startswith('-log10')]]\n",
    "\n",
    "# Clean column names\n",
    "heatmap_data.columns = heatmap_data.columns.str.replace(r\"-log10_P-value_\", \"\", regex=True)\n",
    "\n",
    "# Normalize the data (z-score)\n",
    "normalized_data = heatmap_data.apply(zscore, axis=1)\n",
    "\n",
    "# Prepare heatmap data\n",
    "normalized_data.index = filtered_data['Group']\n",
    "\n",
    "# Reorder columns: Group P2, P4, P6, and P8 together\n",
    "columns_order = []\n",
    "for phase in ['P2', 'P4', 'P6', 'P8']:\n",
    "    columns_order.extend([col for col in normalized_data.columns if phase in col])\n",
    "\n",
    "# Reorder the DataFrame\n",
    "normalized_data = normalized_data[columns_order]\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(normalized_data, annot=True, cmap=\"viridis\", cbar_kws={'label': 'Z-score of -log10(P-value)'}, vmin=-1.5, vmax=1.5)\n",
    "plt.title('Heatmap of Z-score Normalized -log10(P-values) Across Conditions', fontsize=20)\n",
    "plt.xlabel('Conditions', fontsize=15)\n",
    "plt.ylabel('Groups', fontsize=15)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../Simulations/flux_enrichment_analysis/FEA_heatmap_all.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the columns based on the presence of \"P2 to P4\" or \"P4 to P6\"\n",
    "subset_df = heatmap_data.filter(regex='_P4_|_P6_')\n",
    "\n",
    "# Normalize the data (z-score)\n",
    "normalized_data = subset_df.apply(zscore, axis=1)\n",
    "\n",
    "# Prepare heatmap data\n",
    "normalized_data.index = filtered_data['Group']\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(normalized_data, annot=True, cmap=\"viridis\", cbar_kws={'label': 'Z-score of -log10(P-value)'}, vmin=-1.5, vmax=1.5)\n",
    "plt.title('Heatmap of Z-score Normalized -log10(P-values) Across Conditions', fontsize=20)\n",
    "plt.xlabel('Conditions', fontsize=15)\n",
    "plt.ylabel('Groups', fontsize=15)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../Simulations/flux_enrichment_analysis/FEA_heatmap_p2_p6.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
